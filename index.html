<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-14T00:00:00Z">2024-11-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">57</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian Optimization Approach to Machine Translation Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking a list of candidates from a machine translation system with an
external scoring model and returning the highest-scoring candidate remains a
simple and effective method for improving the overall output quality.
Translation scoring models continue to grow in size, with the best models being
comparable to generation models. Thus, reranking can add substantial
computational cost to the translation pipeline. In this work, we pose reranking
as a Bayesian optimization (BayesOpt) problem. By strategically selecting
candidates to score based on a balance of exploration and exploitation, we show
that it is possible to find top-scoring candidates when scoring only a fraction
of the candidate list. For instance, our method achieves the same CometKiwi
score using only 70 scoring evaluations compared a baseline system using 180.
We present a multi-fidelity setting for BayesOpt, where the candidates are
first scored with a cheaper but noisier proxy scoring model, which further
improves the cost-performance tradeoff when using smaller but well-trained
distilled proxy scorers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1: Preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span> Hallucination Reasoning with Zero-shot Knowledge Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongmin Lee, Hsiang Hsu, Chun-Fu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM hallucination, where LLMs occasionally generate unfaithful text, poses
significant challenges for their practical applications. Most existing
detection methods rely on external knowledge, LLM fine-tuning, or
hallucination-labeled datasets, and they do not distinguish between different
types of hallucinations, which are crucial for improving detection performance.
We introduce a new task, Hallucination Reasoning, which classifies
LLM-generated text into one of three categories: aligned, misaligned, and
fabricated. Our novel zero-shot method assesses whether LLM has enough
knowledge about a given prompt and text. Our experiments conducted on new
datasets demonstrate the effectiveness of our method in hallucination reasoning
and underscore its importance for enhancing detection performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Squeezed Attention: Accelerating Long Context Length <span class="highlight-title">LLM</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Large Language Model (LLM) applications require long input prompts
to perform complex downstream tasks like document analysis and code generation.
For these long context length applications, the length of the input prompt
poses a significant challenge in terms of inference efficiency since the
inference costs increase linearly with sequence length. However, for many of
these applications, much of the context in the prompt is fixed across different
user inputs, thereby providing the opportunity to perform offline optimizations
to process user inputs quickly, as they are received. In this work, we propose
Squeezed Attention as a mechanism to accelerate LLM applications where a large
portion of the input prompt is fixed. We first leverage K-means clustering
offline to group the keys for the fixed context based on semantic similarity
and represent each cluster with a single centroid value. During inference, we
compare query tokens from the user input with the centroids to predict which of
the keys from the fixed context are semantically relevant and need to be loaded
during inference. We then compute exact attention using only these important
keys from the fixed context, thereby reducing bandwidth and computational
costs. We also extend our method to use a hierarchical centroid lookup to
identify important keys, which can reduce the complexity of attention from
linear to logarithmic with respect to the context length. We implement
optimized Triton kernels for centroid comparison and sparse FlashAttention with
important keys, achieving more than 4x speedups during both the prefill and
generation phases for long-context inference. Furthermore, we have extensively
evaluated our method on various long-context benchmarks including LongBench,
where it achieves a 3x reduction in KV cache budget without accuracy loss and
up to an 8x reduction with <0.5 point accuracy gap for various models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Decoding via Latent Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, Jack Lanchantin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During language model decoding, it is known that using higher temperature
sampling gives more creative responses, while lower temperatures are more
factually accurate. However, such models are commonly applied to general
instruction following, which involves both creative and fact seeking tasks,
using a single fixed temperature across all examples and tokens. In this work,
we introduce Adaptive Decoding, a layer added to the model to select the
sampling temperature dynamically at inference time, at either the token or
example level, in order to optimize performance. To learn its parameters we
introduce Latent Preference Optimization (LPO) a general approach to train
discrete latent variables such as choices of temperature. Our method
outperforms all fixed decoding temperatures across a range of tasks that
require different temperatures, including UltraFeedback, Creative Story
Writing, and GSM8K.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Limits of Language Generation: Trade-Offs Between Hallucination
  and Mode Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specifying all desirable properties of a language model is challenging, but
certain requirements seem essential. Given samples from an unknown language,
the trained model should produce valid strings not seen in training and be
expressive enough to capture the language's full richness. Otherwise,
outputting invalid strings constitutes "hallucination," and failing to capture
the full range leads to "mode collapse." We ask if a language model can meet
both requirements.
  We investigate this within a statistical language generation setting building
on Gold and Angluin. Here, the model receives random samples from a
distribution over an unknown language K, which belongs to a possibly infinite
collection of languages. The goal is to generate unseen strings from K. We say
the model generates from K with consistency and breadth if, as training size
increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in
language generation are possible. We answer this negatively: for a large class
of language models, including next-token prediction models, this is impossible
for most collections of candidate languages. This contrasts with [KM24]'s
result, showing consistent generation without breadth is possible for any
countable collection of languages. Our finding highlights that generation with
breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples
needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is
achievable for any countable collection of languages when negative examples
(strings outside K) are available alongside positive ones. This suggests that
post-training feedback, which encodes negative examples, can be crucial in
reducing hallucinations while limiting mode collapse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract shortened to fit arXiv limit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTR: Precision-Driven Tool Recommendation for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Gao, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By augmenting Large Language Models (LLMs) with external tools, their
capacity to solve complex problems has been significantly enhanced. However,
despite ongoing advancements in the parsing capabilities of LLMs, incorporating
all available tools simultaneously in the prompt remains impractical due to the
vast number of external tools. Consequently, it is essential to provide LLMs
with a precise set of tools tailored to the specific task, considering both
quantity and quality. Current tool retrieval methods primarily focus on
refining the ranking list of tools and directly packaging a fixed number of
top-ranked tools as the tool set. However, these approaches often fail to equip
LLMs with the optimal set of tools prior to execution, since the optimal number
of tools for different tasks could be different, resulting in inefficiencies
such as redundant or unsuitable tools, which impede immediate access to the
most relevant tools. This paper addresses the challenge of recommending precise
toolsets for LLMs. We introduce the problem of tool recommendation, define its
scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.
PTR captures an initial, concise set of tools by leveraging historical tool
bundle usage and dynamically adjusts the tool set by performing tool matching,
culminating in a multi-view-based tool addition. Additionally, we present a new
dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness
of tool recommendation for LLMs. We further validate our design choices through
comprehensive experiments, demonstrating promising accuracy across two open
benchmarks and our RecTools dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Moral Foundations Weibo Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Cao, Miaoyan Hu, Jiahan Wei, Baha Ihnaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moral sentiments expressed in natural language significantly influence both
online and offline environments, shaping behavioral styles and interaction
patterns, including social media selfpresentation, cyberbullying, adherence to
social norms, and ethical decision-making. To effectively measure moral
sentiments in natural language processing texts, it is crucial to utilize
large, annotated datasets that provide nuanced understanding for accurate
analysis and modeltraining. However, existing corpora, while valuable, often
face linguistic limitations. To address this gap in the Chinese language
domain,we introduce the Moral Foundation Weibo Corpus. This corpus consists of
25,671 Chinese comments on Weibo, encompassing six diverse topic areas. Each
comment is manually annotated by at least three systematically trained
annotators based on ten moral categories derived from a grounded theory of
morality. To assess annotator reliability, we present the kappa testresults, a
gold standard for measuring consistency. Additionally, we apply several the
latest large language models to supplement the manual annotations, conducting
analytical experiments to compare their performance and report baseline results
for moral sentiment classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the
  AutoNuggetizer Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on "refactoring" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA-Mesh: Unifying 3D Mesh Generation with <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores expanding the capabilities of large language models (LLMs)
pretrained on text to generate 3D meshes within a unified model. This offers
key advantages of (1) leveraging spatial knowledge already embedded in LLMs,
derived from textual sources like 3D tutorials, and (2) enabling conversational
3D generation and mesh understanding. A primary challenge is effectively
tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.
To address this, we introduce LLaMA-Mesh, a novel approach that represents the
vertex coordinates and face definitions of 3D meshes as plain text, allowing
direct integration with LLMs without expanding the vocabulary. We construct a
supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate
3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs
as required, and (3) understand and interpret 3D meshes. Our work is the first
to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge
for 3D mesh generation in a text-based format, effectively unifying the 3D and
text modalities. LLaMA-Mesh achieves mesh generation quality on par with models
trained from scratch while maintaining strong text generation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BabyLM Challenge: Exploring the Effect of Variation Sets on Language
  Model Training Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akari Haga, Akiyo Fukatsu, Miyu Oba, Arianna Bisazza, Yohei Oseki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current large language models have achieved a remarkable success, their
data efficiency remains a challenge to overcome. Recently it has been suggested
that child-directed speech (CDS) can improve training data efficiency of modern
language models based on Transformer neural networks. However, it is not yet
understood which specific properties of CDS are effective for training these
models. In the context of the BabyLM Challenge, we focus on Variation Sets
(VSs), sets of consecutive utterances expressing a similar intent with slightly
different words and structures, which are ubiquitous in CDS. To assess the
impact of VSs on training data efficiency, we augment CDS data with different
proportions of artificial VSs and use these datasets to train an
auto-regressive model, GPT-2. We find that the best proportion of VSs depends
on the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of
VSs, but EWOK scores do not. Additionally, the results vary depending on
multiple factors such as the number of epochs and the order of utterance
presentation. Taken together, these findings suggest that VSs can have a
beneficial influence on language models, while leaving room for further
investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper accepted BabyLM challenge 2024 at CONLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Piecing It All Together: Verifying Multi-Hop Multimodal Claims 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Aman Rangapur, Xiongxiao Xu, Yueqing Liang, Haroon Gharwi, Carl Yang, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing claim verification datasets often do not require systems to perform
complex reasoning or effectively interpret multimodal evidence. To address
this, we introduce a new task: multi-hop multimodal claim verification. This
task challenges models to reason over multiple pieces of evidence from diverse
sources, including text, images, and tables, and determine whether the combined
multimodal evidence supports or refutes a given claim. To study this task, we
construct MMCV, a large-scale dataset comprising 16k multi-hop claims paired
with multimodal evidence, generated and refined using large language models,
with additional input from human feedback. We show that MMCV is challenging
even for the latest state-of-the-art multimodal large language models,
especially as the number of reasoning hops increases. Additionally, we
establish a human performance benchmark on a subset of MMCV. We hope this
dataset and its evaluation task will encourage future research in multimodal
multi-hop claim verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practical Guide to Fine-tuning <span class="highlight-title">Language Model</span>s with Limited Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Márton Szép, Daniel Rueckert, Rüdiger von Eisenhart-Rothe, Florian Hinterwimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing pre-trained Large Language Models (LLMs) has become the de facto
standard in Natural Language Processing (NLP) despite their extensive data
requirements. Motivated by the recent surge in research focused on training
LLMs with limited data, particularly in low-resource domains and languages,
this paper surveys recent transfer learning approaches to optimize model
performance in downstream tasks where data is scarce. We first address initial
and continued pre-training strategies to better leverage prior knowledge in
unseen domains and languages. We then examine how to maximize the utility of
limited data during fine-tuning and few-shot learning. The final section takes
a task-specific perspective, reviewing models and methods suited for different
levels of data scarcity. Our goal is to provide practitioners with practical
guidelines for overcoming the challenges posed by constrained data while also
highlighting promising directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication Compression for Tensor Parallel <span class="highlight-title">LLM</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Hansen-Palmus, Michael Truong-Le, Oliver Hausdörfer, Alok Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have pushed the frontier of artificial
intelligence but are comprised of hundreds of billions of parameters and
operations. For faster inference latency, LLMs are deployed on multiple
hardware accelerators through various Model Parallelism strategies. Our paper
looks into the details on one such strategy - Tensor Parallel - and proposes to
reduce latency by compressing inter-accelerator communication. We leverage fine
grained quantization techniques to compress selected activations by 3.5 - 4.5x.
Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with
negligible model performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Use of Readability Metrics in Legal Text: A Systematic Literature
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Han, Aaron Ceross, Jeroen H. M. Bergmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the text in legal documents can be challenging due to their
complex structure and the inclusion of domain-specific jargon. Laws and
regulations are often crafted in such a manner that engagement with them
requires formal training, potentially leading to vastly different
interpretations of the same texts. Linguistic complexity is an important
contributor to the difficulties experienced by readers. Simplifying texts could
enhance comprehension across a broader audience, not just among trained
professionals. Various metrics have been developed to measure document
readability. Therefore, we adopted a systematic review approach to examine the
linguistic and readability metrics currently employed for legal and regulatory
texts. A total of 3566 initial papers were screened, with 34 relevant studies
found and further assessed. Our primary objective was to identify which current
metrics were applied for evaluating readability within the legal field. Sixteen
different metrics were identified, with the Flesch-Kincaid Grade Level being
the most frequently used method. The majority of studies (73.5%) were found in
the domain of "informed consent forms". From the analysis, it is clear that not
all legal domains are well represented in terms of readability metrics and that
there is a further need to develop more consensus on which metrics should be
applied for legal documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyuan Zhang, Ruihui Wang, Bo Xia, Yuan Sun, Xiaobing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in high-resource languages but face
notable challenges in low-resource languages like Mongolian. This paper
addresses these challenges by categorizing capabilities into language abilities
(syntax and semantics) and cognitive abilities (knowledge and reasoning). To
systematically evaluate these areas, we developed MM-Eval, a specialized
dataset based on Modern Mongolian Language Textbook I and enriched with WebQSP
and MGSM datasets.
  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,
Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models
performed better on syntactic tasks than semantic tasks, highlighting a gap in
deeper language understanding; and 2) knowledge tasks showed a moderate
decline, suggesting that models can transfer general knowledge from
high-resource to low-resource contexts.
  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,
and 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in
low-resource languages like Mongolian. The dataset is available at
https://github.com/joenahm/MM-Eval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Tasks with Fuzzy Time Requirements from Natural Language
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Sucker, Michael Neubauer, Dominik Henrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language allows robot programming to be accessible to everyone.
However, the inherent fuzziness in natural language poses challenges for
inflexible, traditional robot systems. We focus on instructions with fuzzy time
requirements (e.g., "start in a few minutes"). Building on previous robotics
research, we introduce fuzzy skills. These define an execution by the robot
with so-called satisfaction functions representing vague execution time
requirements. Such functions express a user's satisfaction over potential
starting times for skill execution. When the robot handles multiple fuzzy
skills, the satisfaction function provides a temporal tolerance window for
execution, thus, enabling optimal scheduling based on satisfaction. We
generalized such functions based on individual user expectations with a user
study. The participants rated their satisfaction with an instruction's
execution at various times. Our investigations reveal that trapezoidal
functions best approximate the users' satisfaction. Additionally, the results
suggest that users are more lenient if the execution is specified further into
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, to be published in 2024 IEEE International
  Conference on Robotic Computing (IRC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Everyone deserves their voice to be heard: Analyzing Predictive Gender
  Bias in ASR Models Applied to Dutch Speech Data <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rik Raes, Saskia Lensink, Mykola Pechenizkiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that state-of-the-art (SotA) Automatic Speech
Recognition (ASR) systems, such as Whisper, often exhibit predictive biases
that disproportionately affect various demographic groups. This study focuses
on identifying the performance disparities of Whisper models on Dutch speech
data from the Common Voice dataset and the Dutch National Public Broadcasting
organisation. We analyzed the word error rate, character error rate and a
BERT-based semantic similarity across gender groups. We used the moral
framework of Weerts et al. (2022) to assess quality of service harms and
fairness, and to provide a nuanced discussion on the implications of these
biases, particularly for automatic subtitling. Our findings reveal substantial
disparities in word error rate (WER) among gender groups across all model
sizes, with bias identified through statistical testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECML PKDD 2024, 4th Workshop on Bias and Fairness in AI
  (BIAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Unseen Domain Fake News Detection via Causal Propagation
  Substructures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of fake news on social media poses significant threats to
individuals and society. Text-based and graph-based models have been employed
for fake news detection by analysing news content and propagation networks,
showing promising results in specific scenarios. However, these data-driven
models heavily rely on pre-existing in-distribution data for training, limiting
their performance when confronted with fake news from emerging or previously
unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news
is a challenging yet critical task. In this paper, we introduce the Causal
Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to
enhance zero-shot fake news detection by extracting causal substructures from
propagation graphs using in-distribution data and generalising this approach to
OOD data. The model employs a graph neural network based mask generation
process to identify dominant nodes and edges within the propagation graph,
using these substructures for fake news detection. Additionally, the
performance of CSDA is further improved through contrastive learning in
few-shot scenarios, where a limited amount of OOD data is available for
training. Extensive experiments on public social media datasets demonstrate
that CSDA effectively handles OOD fake news detection, achieving a 7 to 16
percents accuracy improvement over other state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Parameterization of Lightweight <span class="highlight-title">Transformer</span> for On-Device Speech
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixing Zhang, Zhongren Dong, Weixiang Xu, Jing Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing implementation of machine learning models on edge or
Internet-of-Things (IoT) devices, deploying advanced models on
resource-constrained IoT devices remains challenging. Transformer models, a
currently dominant neural architecture, have achieved great success in broad
domains but their complexity hinders its deployment on IoT devices with limited
computation capability and storage size. Although many model compression
approaches have been explored, they often suffer from notorious performance
degradation. To address this issue, we introduce a new method, namely
Transformer Re-parameterization, to boost the performance of lightweight
Transformer models. It consists of two processes: the High-Rank Factorization
(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)
process in the inference stage. In the former process, we insert an additional
linear layer before the Feed-Forward Network (FFN) of the lightweight
Transformer. It is supposed that the inserted HRF layers can enhance the model
learning capability. In the later process, the auxiliary HRF layer will be
merged together with the following FFN layer into one linear layer and thus
recover the original structure of the lightweight model. To examine the
effectiveness of the proposed method, we evaluate it on three widely used
Transformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer
networks, in the application of speech emotion recognition on the IEMOCAP, M3ED
and DAIC-WOZ datasets. Experimental results show that our proposed method
consistently improves the performance of lightweight Transformers, even making
them comparable to large models. The proposed re-parameterization approach
enables advanced Transformer models to be deployed on resource-constrained IoT
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveThru: a Document Extraction Platform and Benchmark <span class="highlight-title">Dataset</span>s for
  Indonesian Local Language Archives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MohammadRifqi Farhansyah, Muhammad Zuhdi Fikri Johari, Afinzaki Amiral, Ayu Purwarianti, Kumara Ari Yuana, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indonesia is one of the most diverse countries linguistically. However,
despite this linguistic diversity, Indonesian languages remain underrepresented
in Natural Language Processing (NLP) research and technologies. In the past two
years, several efforts have been conducted to construct NLP resources for
Indonesian languages. However, most of these efforts have been focused on
creating manual resources thus difficult to scale to more languages. Although
many Indonesian languages do not have a web presence, locally there are
resources that document these languages well in printed forms such as books,
magazines, and newspapers. Digitizing these existing resources will enable
scaling of Indonesian language resource construction to many more languages. In
this paper, we propose an alternative method of creating datasets by digitizing
documents, which have not previously been used to build digital language
resources in Indonesia. DriveThru is a platform for extracting document content
utilizing Optical Character Recognition (OCR) techniques in its system to
provide language resource building with less manual effort and cost. This paper
also studies the utility of current state-of-the-art LLM for post-OCR
correction to show the capability of increasing the character accuracy rate
(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DTELS: Towards Dynamic Granularity of <span class="highlight-title">Timeline</span> <span class="highlight-title">Summarization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Zhang, Tong Zhou, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, <span class="highlight-author">Jun Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of online news has posed significant challenges in
tracking the continuous development of news topics. Traditional timeline
summarization constructs a chronological summary of the events but often lacks
the flexibility to meet the diverse granularity needs. To overcome this
limitation, we introduce a new paradigm, Dynamic-granularity TimELine
Summarization, (DTELS), which aims to construct adaptive timelines based on
user instructions or requirements. This paper establishes a comprehensive
benchmark for DTLES that includes: (1) an evaluation framework grounded in
journalistic standards to assess the timeline quality across four dimensions:
Informativeness, Granular Consistency, Factuality, and Coherence; (2) a
large-scale, multi-source dataset with multiple granularity timeline
annotations based on a consensus process to facilitate authority; (3) extensive
experiments and analysis with two proposed solutions based on Large Language
Models (LLMs) and existing state-of-the-art TLS methods. The experimental
results demonstrate the effectiveness of LLM-based solutions. However, even the
most advanced LLMs struggle to consistently generate timelines that are both
informative and granularly consistent, highlighting the challenges of the DTELS
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> StreamAdapter: Efficient Test Time Adaptation from Contextual Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilxat Muhtar, Yelong Shen, Yaming Yang, Xiaodong Liu, Yadong Lu, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Xueliang Zhang, Jianfeng Gao, Weizhu Chen, <span class="highlight-author">Qi Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) allows large language models (LLMs) to adapt to new
tasks directly from the given demonstrations without requiring gradient
updates. While recent advances have expanded context windows to accommodate
more demonstrations, this approach increases inference costs without
necessarily improving performance. To mitigate these issues, We propose
StreamAdapter, a novel approach that directly updates model parameters from
context at test time, eliminating the need for explicit in-context
demonstrations. StreamAdapter employs context mapping and weight absorption
mechanisms to dynamically transform ICL demonstrations into parameter updates
with minimal additional parameters. By reducing reliance on numerous in-context
examples, StreamAdapter significantly reduce inference costs and allows for
efficient inference with constant time complexity, regardless of demonstration
count. Extensive experiments across diverse tasks and model architectures
demonstrate that StreamAdapter achieves comparable or superior adaptation
capability to ICL while requiring significantly fewer demonstrations. The
superior task adaptation and context encoding capabilities of StreamAdapter on
both language understanding and generation tasks provides a new perspective for
adapting LLMs at test time using context, allowing for more efficient
adaptation across scenarios and more cost-effective inference
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 Pages, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Consistency in Multimodal <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Senyu Li, Ning Shi, Bradley Hauer, Zijun Wu, Grzegorz Kondrak, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in multimodal methodologies have marked the beginning of
an exciting era for models adept at processing diverse data types, encompassing
text, audio, and visual content. Models like GPT-4V, which merge computer
vision with advanced language processing, exhibit extraordinary proficiency in
handling intricate tasks that require a simultaneous understanding of both
textual and visual information. Prior research efforts have meticulously
evaluated the efficacy of these Vision Large Language Models (VLLMs) in various
domains, including object detection, image captioning, and other related
fields. However, existing analyses have often suffered from limitations,
primarily centering on the isolated evaluation of each modality's performance
while neglecting to explore their intricate cross-modal interactions.
Specifically, the question of whether these models achieve the same level of
accuracy when confronted with identical task instances across different
modalities remains unanswered. In this study, we take the initiative to delve
into the interaction and comparison among these modalities of interest by
introducing a novel concept termed cross-modal consistency. Furthermore, we
propose a quantitative evaluation framework founded on this concept. Our
experimental findings, drawn from a curated collection of parallel
vision-language datasets developed by us, unveil a pronounced inconsistency
between the vision and language modalities within GPT-4V, despite its portrayal
as a unified multimodal model. Our research yields insights into the
appropriate utilization of such models and hints at potential avenues for
enhancing their design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreak Attacks and Defenses against Multimodal <span class="highlight-title">Generative</span> Models: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuannan Liu, Xing Cui, Peipei Li, Zekun Li, Huaibo Huang, Shuhan Xia, Miaoxuan Zhang, Yueying Zou, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of multimodal foundation models has led to significant
advancements in cross-modal understanding and generation across diverse
modalities, including text, images, audio, and video. However, these models
remain susceptible to jailbreak attacks, which can bypass built-in safety
mechanisms and induce the production of potentially harmful content.
Consequently, understanding the methods of jailbreak attacks and existing
defense mechanisms is essential to ensure the safe deployment of multimodal
generative models in real-world scenarios, particularly in security-sensitive
applications. To provide comprehensive insight into this topic, this survey
reviews jailbreak and defense in multimodal generative models. First, given the
generalized lifecycle of multimodal jailbreak, we systematically explore
attacks and corresponding defense strategies across four levels: input,
encoder, generator, and output. Based on this analysis, we present a detailed
taxonomy of attack methods, defense mechanisms, and evaluation frameworks
specific to multimodal generative models. Additionally, we cover a wide range
of input-output configurations, including modalities such as Any-to-Text,
Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight
current research challenges and propose potential directions for future
research.The open-source repository corresponding to this work can be found at
https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form
  Text through a Benchmark <span class="highlight-title">Dataset</span> in Biomedicine <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Seo, Jongwon Lim, Dongjun Jang, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DAHL, a benchmark dataset and automated evaluation system
designed to assess hallucination in long-form text generation, specifically
within the biomedical domain. Our benchmark dataset, meticulously curated from
biomedical research papers, consists of 8,573 questions across 29 categories.
DAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)
by deconstructing responses into atomic units, each representing a single piece
of information. The accuracy of these responses is averaged to produce the DAHL
Score, offering a more in-depth evaluation of hallucinations compared to
previous methods that rely on multiple-choice tasks. We conduct experiments
with 8 different models, finding that larger models tend to hallucinate less;
however, beyond a model size of 7 to 8 billion parameters, further scaling does
not significantly improve factual accuracy. The DAHL Score holds potential as
an efficient alternative to human-annotated preference labels, being able to be
expanded to other specialized domains. We release the dataset and code in
public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024/FEVER</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Financial Domain Adaptation of <span class="highlight-title">Language Model</span>s via Model
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Tanabe, Masanori Hirano, Kazuki Matoya, Kentaro Imajo, Hiroki Sakaji, Itsuki Noda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The domain adaptation of language models, including large language models
(LLMs), has become increasingly important as the use of such models continues
to expand. This study demonstrates the effectiveness of Composition to Augment
Language Models (CALM) in adapting to the financial domain. CALM is a model to
extend the capabilities of existing models by introducing cross-attention
between two LLMs with different functions. In our experiments, we developed a
CALM to enhance the financial performance of an LLM with strong response
capabilities by leveraging a financial-specialized LLM. Notably, the CALM was
trained using a financial dataset different from the one used to train the
financial-specialized LLM, confirming CALM's ability to adapt to various
datasets. The models were evaluated through quantitative Japanese financial
benchmarks and qualitative response comparisons, demonstrating that CALM
enables superior responses with higher scores than the original models and
baselines. Additionally, comparative experiments on connection points revealed
that connecting the middle layers of the models is most effective in
facilitating adaptation to the financial domain. These findings confirm that
CALM is a practical approach for adapting LLMs to the financial domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate<span class="highlight-title">GPT</span>: Unleashing <span class="highlight-title">GPT</span>-3.5 Turbo to Combat Hate Speech on X 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of social media platforms like Twitter and Facebook has
enabled people of all ages to share their thoughts and experiences, leading to
an immense accumulation of user-generated content. However, alongside the
benefits, these platforms also face the challenge of managing hate speech and
offensive content, which can undermine rational discourse and threaten
democratic values. As a result, there is a growing need for automated methods
to detect and mitigate such content, especially given the complexity of
conversations that may require contextual analysis across multiple languages,
including code-mixed languages like Hinglish, German-English, and Bangla. We
participated in the English task where we have to classify English tweets into
two categories namely Hate and Offensive and Non Hate-Offensive. In this work,
we experiment with state-of-the-art large language models like GPT-3.5 Turbo
via prompting to classify tweets into Hate and Offensive or Non Hate-Offensive.
In this study, we evaluate the performance of a classification model using
Macro-F1 scores across three distinct runs. The Macro-F1 score, which balances
precision and recall across all classes, is used as the primary metric for
model evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and
0.754 for run 3, indicating a high level of performance with minimal variance
among the runs. The results suggest that the model consistently performs well
in terms of precision and recall, with run 1 showing the highest performance.
These findings highlight the robustness and reliability of the model across
different runs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Hate Speech and Offensive Content
  Identification in English and Indo-Aryan Languages (HASOC)). arXiv admin
  note: text overlap with arXiv:2411.05039, arXiv:2411.06946</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive and Practical Evaluation of Retrieval-Augmented Generation
  Systems for Medical Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Trung Ngo, Chien Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Unstructured Text Enhanced Open-domain Dialogue System: A Systematic
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Ma, Mingda Li, Weinan Zhang, Jiapeng Li, <span class="highlight-author">Ting Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating external knowledge into dialogue generation has been proven to
benefit the performance of an open-domain Dialogue System (DS), such as
generating informative or stylized responses, controlling conversation topics.
In this article, we study the open-domain DS that uses unstructured text as
external knowledge sources (\textbf{U}nstructured \textbf{T}ext
\textbf{E}nhanced \textbf{D}ialogue \textbf{S}ystem, \textbf{UTEDS}). The
existence of unstructured text entails distinctions between UTEDS and
traditional data-driven DS and we aim to analyze these differences. We first
give the definition of the UTEDS related concepts, then summarize the recently
released datasets and models. We categorize UTEDS into Retrieval and Generative
models and introduce them from the perspective of model components. The
retrieval models consist of Fusion, Matching, and Ranking modules, while the
generative models comprise Dialogue and Knowledge Encoding, Knowledge
Selection, and Response Generation modules. We further summarize the evaluation
methods utilized in UTEDS and analyze the current models' performance. At last,
we discuss the future development trends of UTEDS, hoping to inspire new
research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 3 Figures, 11 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DROJ: A <span class="highlight-title">Prompt</span>-Driven Attack against <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyang Hu, Boran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Due to their training on
internet-sourced datasets, LLMs can sometimes generate objectionable content,
necessitating extensive alignment with human feedback to avoid such outputs.
Despite massive alignment efforts, LLMs remain susceptible to adversarial
jailbreak attacks, which usually are manipulated prompts designed to circumvent
safety mechanisms and elicit harmful responses. Here, we introduce a novel
approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which
optimizes jailbreak prompts at the embedding level to shift the hidden
representations of harmful queries towards directions that are more likely to
elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat
model show that DROJ achieves a 100\% keyword-based Attack Success Rate (ASR),
effectively preventing direct refusals. However, the model occasionally
produces repetitive and non-informative responses. To mitigate this, we
introduce a helpfulness system prompt that enhances the utility of the model's
responses. Our code is available at
https://github.com/Leon-Leyang/LLM-Safeguard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent
  Evaluation of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) showcase varied
multilingual capabilities across tasks like translation, code generation, and
reasoning. Previous assessments often limited their scope to fundamental
natural language processing (NLP) or isolated capability-specific tasks. To
alleviate this drawback, we aim to present a comprehensive multilingual
multitask benchmark. First, we present a pipeline for selecting available and
reasonable benchmarks from massive ones, addressing the oversight in previous
work regarding the utility of these benchmarks, i.e., their ability to
differentiate between models being evaluated. Leveraging this pipeline, we
introduce P-MMEval, a large-scale benchmark covering effective fundamental and
capability-specialized datasets. Furthermore, P-MMEval delivers consistent
language coverage across various datasets and provides parallel samples.
Finally, we conduct extensive experiments on representative multilingual model
series to compare performances across models, analyze dataset effectiveness,
examine prompt impacts on model performances, and explore the relationship
between multilingual performances and factors such as tasks, model sizes, and
languages. These insights offer valuable guidance for future research. The
dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Help for Optimizing Low-Skilled Users' Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Gu, Wichayaporn Wongkamjan, Jordan Lee Boyd-Graber, Jonathan K. Kummerfeld, Denis Peskoff, Jonathan May
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AIs can beat humans in game environments; however, how helpful those agents
are to human remains understudied. We augment CICERO, a natural language agent
that demonstrates superhuman performance in Diplomacy, to generate both move
and message advice based on player intentions. A dozen Diplomacy games with
novice and experienced players, with varying advice settings, show that some of
the generated advice is beneficial. It helps novices compete with experienced
players and in some instances even surpass them. The mere presence of advice
can be advantageous, even if players do not follow it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantitative Assessment of Intersectional Empathetic Bias and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Formanek, Ondrej Sotolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing amount of literature critiques the current operationalizations of
empathy based on loose definitions of the construct. Such definitions
negatively affect dataset quality, model robustness, and evaluation
reliability. We propose an empathy evaluation framework that operationalizes
empathy close to its psychological origins. The framework measures the variance
in responses of LLMs to prompts using existing metrics for empathy and
emotional valence. The variance is introduced through the controlled generation
of the prompts by varying social biases affecting context understanding, thus
impacting empathetic understanding. The control over generation ensures high
theoretical validity of the constructs in the prompt dataset. Also, it makes
high-quality translation, especially into languages that currently have
little-to-no way of evaluating empathy or bias, such as the Slavonic family,
more manageable. Using chosen LLMs and various prompt types, we demonstrate the
empathy evaluation with the framework, including multiple-choice answers and
free generation. The variance in our initial evaluation sample is small and we
were unable to measure convincing differences between the empathetic
understanding in contexts given by different social groups. However, the
results are promising because the models showed significant alterations their
reasoning chains needed to capture the relatively subtle changes in the
prompts. This provides the basis for future research into the construction of
the evaluation sample and statistical methods for measuring the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verifiable by Design: Aligning <span class="highlight-title">Language Model</span>s to Quote from
  <span class="highlight-title">Pre-Train</span>ing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To trust the fluent generations of large language models (LLMs), humans must
be able to verify their correctness against trusted, external sources. Recent
efforts, such as providing citations via retrieved documents or post-hoc
provenance, enhance verifiability but provide no guarantees on their
correctness. To address these limitations, we tackle the verifiability goal
with a different philosophy: trivializing the verification process by
developing models that quote verbatim statements from trusted sources in their
pre-training data. We propose Quote-Tuning, which demonstrates the feasibility
of aligning models to quote. The core of Quote-Tuning is a fast membership
inference function that efficiently verifies text against trusted corpora. We
leverage this tool to design a reward function to quantify quotes in model
responses, and curate datasets for preference learning. Experiments show that
Quote-Tuning significantly increases verbatim quotes from high-quality
documents by up to 130% relative to base models while maintaining response
quality. Quote-Tuning is applicable in different tasks, generalizes to
out-of-domain data and diverse model families, and provides additional benefits
to truthfulness. Our method not only serves as a hassle-free method to increase
quoting but also opens up avenues for improving LLM trustworthiness through
better verifiability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoDefense: Multi-<span class="highlight-title">Agent</span> <span class="highlight-title">LLM</span> Defense against Jailbreak Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive pre-training in moral alignment to prevent generating
harmful information, large language models (LLMs) remain vulnerable to
jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense
framework that filters harmful responses from LLMs. With the response-filtering
mechanism, our framework is robust against different jailbreak attack prompts,
and can be used to defend different victim models. AutoDefense assigns
different roles to LLM agents and employs them to complete the defense task
collaboratively. The division in tasks enhances the overall
instruction-following of LLMs and enables the integration of other defense
components as tools. With AutoDefense, small open-source LMs can serve as
agents and defend larger models against jailbreak attacks. Our experiments show
that AutoDefense can effectively defense against different jailbreak attacks,
while maintaining the performance at normal user request. For example, we
reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using
LLaMA-2-13b with a 3-agent system. Our code and data are publicly available at
https://github.com/XHMY/AutoDefense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRSD: Rethinking Similarity and Diversity for Retrieval in Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Gao, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector retrieval algorithms are essential for semantic queries within the
rapidly evolving landscape of Large Language Models (LLMs). The ability to
retrieve vectors that satisfy both similarity and diversity criteria
substantially enhances the performance of LLMs. Although Maximal Marginal
Relevance (MMR) is widely employed in retrieval scenarios requiring relevance
and diversity, variations in the parameter $\lambda$ lead to fluctuations that
complicate the optimization trajectory in vector spaces. This obscures the
direction of improvement and highlights the lack of a robust theoretical
analysis regarding similarity and diversity constraints in retrieval processes.
To address these challenges, this paper introduces a novel approach that
characterizes both constraints through the relationship between the sum vector
and the query vector. The proximity of these vectors ensures the similarity
constraint, while requiring individual vectors within the sum vector to diverge
in their alignment with the query vector satisfies the diversity constraint. We
first formulate a new combinatorial optimization problem, selecting k vectors
from a candidate set such that their sum vector maximally aligns with the query
vector, and demonstrate that this problem is NP-complete. This result
underscores the inherent difficulty of simultaneously achieving similarity and
diversity in vector retrieval, thereby providing a theoretical foundation for
future research. Subsequently, we present the heuristic algorithm Vectors
Retrieval with Similarity and Diversity, VRSD, which features a clear
optimization objective and eliminates the need for preset parameters. VRSD also
achieves a modest reduction in time complexity compared to MMR. Empirical
validation confirms that VRSD significantly outperforms MMR across various
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value Residual Learning For Alleviating Attention Concentration In
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers can capture long-range dependencies using self-attention,
allowing tokens to attend to all others directly. However, stacking multiple
attention layers leads to attention concentration. One natural way to address
this issue is to use cross-layer attention, allowing information from earlier
layers to be directly accessible to later layers. However, this approach is
computationally expensive. To address this problem, we propose Transformer with
residual value (ResFormer) which approximates cross-layer attention through
adding a residual connection from the values of the the first layer to all
subsequent layers. Based on this method, one variant is the Transformer with
single layer value (SVFormer), where all layers share the same value embedding
from first layer, reducing the $KV$ cache by nearly 50\%. Comprehensive
empirical evidence demonstrates that ResFormer mitigates attention
concentration problem in deeper layers and enhances representation across most
layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in
training error as well as downstream tasks. Further visualization results
suggest that Resformer alleviates attention sinks through avoiding value-state
drains. SVFormer trains significantly faster than the vanilla Transformer and
performs better than other methods like GQA and CLA, with performance
influenced by sequence length and cumulative learning rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Bases in Support of <span class="highlight-title">Large Language Model</span>s for Processing Web
  News 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Zhang, Nabin Pakka, Nian-Feng Tzeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have received considerable interest in wide
applications lately. During pre-training via massive datasets, such a model
implicitly memorizes the factual knowledge of trained datasets in its hidden
parameters. However, knowledge held implicitly in parameters often makes its
use by downstream applications ineffective due to the lack of common-sense
reasoning. In this article, we introduce a general framework that permits to
build knowledge bases with an aid of LLMs, tailored for processing Web news.
The framework applies a rule-based News Information Extractor (NewsIE) to news
items for extracting their relational tuples, referred to as knowledge bases,
which are then graph-convoluted with the implicit knowledge facts of news items
obtained by LLMs, for their classification. It involves two lightweight
components: 1) NewsIE: for extracting the structural information of every news
item, in the form of relational tuples; 2) BERTGraph: for graph convoluting the
implicit knowledge facts with relational tuples extracted by NewsIE. We have
evaluated our framework under different news-related datasets for news category
classification, with promising experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Arabic Multi-Label Emotion Classification using Stacked
  Embeddings and Hybrid Loss Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Azeem Aslam, Wang Jun, Nisar Ahmed, Muhammad Imran Zaman, Li Yanan, Hu Hongfei, Wang Shiyu, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-label emotion classification, particularly for low-resource
languages like Arabic, the challenges of class imbalance and label correlation
hinder model performance, especially in accurately predicting minority
emotions. To address these issues, this study proposes a novel approach that
combines stacked embeddings, meta-learning, and a hybrid loss function to
enhance multi-label emotion classification for the Arabic language. The study
extracts contextual embeddings from three fine-tuned language
models-ArabicBERT, MarBERT, and AraBERT-which are then stacked to form enriched
embeddings. A meta-learner is trained on these stacked embeddings, and the
resulting concatenated representations are provided as input to a Bi-LSTM
model, followed by a fully connected neural network for multi-label
classification. To further improve performance, a hybrid loss function is
introduced, incorporating class weighting, label correlation matrix, and
contrastive learning, effectively addressing class imbalances and improving the
handling of label correlations. Extensive experiments validate the proposed
model's performance across key metrics such as Precision, Recall, F1-Score,
Jaccard Accuracy, and Hamming Loss. The class-wise performance analysis
demonstrates the hybrid loss function's ability to significantly reduce
disparities between majority and minority classes, resulting in a more balanced
emotion classification. An ablation study highlights the contribution of each
component, showing the superiority of the model compared to baseline approaches
and other loss functions. This study not only advances multi-label emotion
classification for Arabic but also presents a generalizable framework that can
be adapted to other languages and domains, providing a significant step forward
in addressing the challenges of low-resource emotion classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is submitted in Scientific Reports and is currently under
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">LLM</span>s Recognize Toxicity? A Structured Investigation Framework and
  Toxicity Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06900v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06900v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of developing Large Language Models (LLMs) that adhere to
societal standards, it is imperative to detect the toxicity in the generated
text. The majority of existing toxicity metrics rely on encoder models trained
on specific toxicity datasets, which are susceptible to out-of-distribution
(OOD) problems and depend on the dataset's definition of toxicity. In this
paper, we introduce a robust metric grounded on LLMs to flexibly measure
toxicity according to the given definition. We first analyze the toxicity
factors, followed by an examination of the intrinsic toxic attributes of LLMs
to ascertain their suitability as evaluators. Finally, we evaluate the
performance of our metric with detailed analysis. Our empirical results
demonstrate outstanding performance in measuring toxicity within verified
factors, improving on conventional metrics by 12 points in the F1 score. Our
findings also indicate that upstream toxicity significantly influences
downstream metrics, suggesting that LLMs are unsuitable for toxicity
evaluations within unverified factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 page long</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIMER-IT: Zero-Shot NER on Italian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to Named Entity Recognition (NER) frame the task into
a BIO sequence labeling problem. Although these systems often excel in the
downstream task at hand, they require extensive annotated data and struggle to
generalize to out-of-distribution input domains and unseen entity types. On the
contrary, Large Language Models (LLMs) have demonstrated strong zero-shot
capabilities. While several works address Zero-Shot NER in English, little has
been done in other languages. In this paper, we define an evaluation framework
for Zero-Shot NER, applying it to the Italian language. Furthermore, we
introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning
approach for zero-shot NER leveraging prompts enriched with definition and
guidelines. Comparisons with other state-of-the-art models, demonstrate the
superiority of SLIMER-IT on never-seen-before entity tags.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> IRCAN: Mitigating Knowledge Conflicts in <span class="highlight-title">LLM</span> Generation via Identifying
  and Reweighting Context-Aware Neurons <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, <span class="highlight-author">Deyi Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely acknowledged that large language models (LLMs) encode a vast
reservoir of knowledge after being trained on mass data. Recent studies
disclose knowledge conflicts in LLM generation, wherein outdated or incorrect
parametric knowledge (i.e., encoded knowledge) contradicts new knowledge
provided in the context. To mitigate such knowledge conflicts, we propose a
novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to
capitalize on neurons that are crucial in processing contextual cues.
Specifically, IRCAN first identifies neurons that significantly contribute to
context processing, utilizing a context-aware attribution score derived from
integrated gradients. Subsequently, the identified context-aware neurons are
strengthened via reweighting. In doing so, we steer LLMs to generate
context-sensitive outputs with respect to the new knowledge provided in the
context. Extensive experiments conducted across a variety of models and tasks
demonstrate that IRCAN not only achieves remarkable improvements in handling
knowledge conflicts but also offers a scalable, plug-and-play solution that can
be integrated seamlessly with existing models. Our codes are released at
https://github.com/danshi777/IRCAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Large Language Model</span>s Truly Grasp Mathematics? An Empirical
  Exploration From Cognitive Psychology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cognitive mechanism by which Large Language Models (LLMs) solve
mathematical problems remains a widely debated and unresolved issue. Currently,
there is little interpretable experimental evidence that connects LLMs'
problem-solving with human cognitive psychology.To determine if LLMs possess
human-like mathematical reasoning, we modified the problems used in the human
Cognitive Reflection Test (CRT). Our results show that, even with the use of
Chains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model
(noted for its reasoning capabilities), have a high error rate when solving
these modified CRT problems. Specifically, the average accuracy rate dropped by
up to 50% compared to the original questions.Further analysis of LLMs'
incorrect answers suggests that they primarily rely on pattern matching from
their training data, which aligns more with human intuition (System 1 thinking)
rather than with human-like reasoning (System 2 thinking). This finding
challenges the belief that LLMs have genuine mathematical reasoning abilities
comparable to humans. As a result, this work may adjust overly optimistic views
on LLMs' progress towards artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Potential of Multimodal <span class="highlight-title">LLM</span> with Knowledge-Intensive
  Multimodal ASR <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Wang, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have made
significant progress in integrating information across various modalities, yet
real-world applications in educational and scientific domains remain
challenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,
which focuses on transcribing scientific conference videos by leveraging visual
information from slides to enhance the accuracy of technical terminologies.
Realized that traditional metrics like WER fall short in assessing performance
accurately, prompting the proposal of severity-aware WER (SWER) that considers
the content type and severity of ASR errors. We propose the Scientific Vision
Augmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to
improve transcript quality through post-editing. Evaluations of
state-of-the-art MLLMs, including GPT-4o, show a 45% improvement over
speech-only baselines, highlighting the importance of multimodal information
integration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Small <span class="highlight-title">Language Model</span>s Learn, Unlearn, and Retain Noise Patterns? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small Language Models (SLMs) are generally considered more compact versions
of large language models (LLMs). This study investigates the ability of SLMs
with parameters between 1 and 3 billion to learn, retain, and subsequently
eliminate different types of noise present in the data. Four pre-trained SLMs
were utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The
models were instruction-tuned on noise-free data and tested using in-context
examples to determine if they could learn noise through examples. Subsequently,
noise patterns were introduced in instruction tuning to evaluate the noise
learning, unlearning, and retention capabilities of the models. Olmo, the
smallest model, was highly sensitive to noise, quickly adapting to noisy
patterns. Phi2 resisted learning character-level and transliteration noise,
likely due to its carefully curated, structured, and high-quality pretraining
data. Gemma excelled with transliteration noise, likely benefiting from its
multilingual pretraining. The findings can be used to develop robust training
strategies for SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Language Model</span>s Encode the Value of Numbers Linearly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwei Zhu, Damai Dai, Zhifang Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive competence in various
tasks, but their internal mechanisms on mathematical problems are still
under-explored. In this paper, we study a fundamental question: how language
models encode the value of numbers, a basic element in math. To study the
question, we construct a synthetic dataset comprising addition problems and
utilize linear probes to read out input numbers from the hidden states.
Experimental results support the existence of encoded number values in LLMs on
different layers, and these values can be extracted via linear probes. Further
experiments show that LLMs store their calculation results in a similar manner,
and we can intervene the output via simple vector additions, proving the causal
connection between encoded numbers and language model outputs. Our research
provides evidence that LLMs encode the value of numbers linearly, offering
insights for better exploring, designing, and utilizing numeric information in
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are available at
  https://github.com/solitaryzero/NumProbe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On Context Utilization in <span class="highlight-title">Summarization</span> with <span class="highlight-title">Large Language Model</span>s <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10570v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10570v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Ravaut, <span class="highlight-author">Aixin Sun</span>, Nancy F. Chen, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in abstractive summarization tasks,
delivering fluent and pertinent summaries. Recent advancements have extended
their capabilities to handle long-input contexts, exceeding 100k tokens.
However, in question answering, language models exhibit uneven utilization of
their input context. They tend to favor the initial and final segments,
resulting in a U-shaped performance pattern concerning where the answer is
located within the input. This bias raises concerns, particularly in
summarization where crucial content may be dispersed throughout the source
document(s). Besides, in summarization, mapping facts from the source to the
summary is not trivial as salient content is usually re-phrased. In this paper,
we conduct the first comprehensive study on context utilization and position
bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5
evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on
the which we benchmark two alternative inference methods to alleviate position
bias: hierarchical summarization and incremental summarization. Our code and
data can be found here: https://github.com/ntunlp/MiddleSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024. 9 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span>s for Power Scheduling: A User-Centric Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Mongaillard, Samson Lasaulce, Othman Hicheur, Chao Zhang, Lina Bariah, Vineeth S. Varma, Hang Zou, Qiyang Zhao, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While traditional optimization and scheduling schemes are designed to meet
fixed, predefined system requirements, future systems are moving toward
user-driven approaches and personalized services, aiming to achieve high
quality-of-experience (QoE) and flexibility. This challenge is particularly
pronounced in wireless and digitalized energy networks, where users'
requirements have largely not been taken into consideration due to the lack of
a common language between users and machines. The emergence of powerful large
language models (LLMs) marks a radical departure from traditional
system-centric methods into more advanced user-centric approaches by providing
a natural communication interface between users and devices. In this paper, for
the first time, we introduce a novel architecture for resource scheduling
problems by constructing three LLM agents to convert an arbitrary user's voice
request (VRQ) into a resource allocation vector. Specifically, we design an LLM
intent recognition agent to translate the request into an optimization problem
(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To
evaluate system performance, we construct a database of typical VRQs in the
context of electric vehicle (EV) charging. As a proof of concept, we primarily
use Llama 3 8B. Through testing with different prompt engineering scenarios,
the obtained results demonstrate the efficiency of the proposed architecture.
The conducted performance analysis allows key insights to be extracted. For
instance, having a larger set of candidate OPs to model the real-world problem
might degrade the final performance because of a higher recognition/OP
classification noise level. All results and codes are open source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised <span class="highlight-title">Summarization</span> Re-ranking <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09593v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09593v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Ravaut, Shafiq Joty, Nancy Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of task-specific pre-training objectives, abstractive
summarization models like PEGASUS offer appealing zero-shot performance on
downstream summarization tasks. However, the performance of such unsupervised
models still lags significantly behind their supervised counterparts. Similarly
to the supervised setup, we notice a very high variance in quality among
summary candidates from these models while only one candidate is kept as the
summary output. In this paper, we propose to re-rank summary candidates in an
unsupervised manner, aiming to close the performance gap between unsupervised
and supervised models. Our approach improves the unsupervised PEGASUS by up to
7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted
summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%
from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on
a dataset, evaluating on another).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, 10 tables, 23 appendix pages, ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Objective and Unbiased Decision Assessments with <span class="highlight-title">LLM</span>-Enhanced
  Hierarchical Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Kwan Hui Lim, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How objective and unbiased are we while making decisions? This work
investigates cognitive bias identification in high-stake decision making
process by human experts, questioning its effectiveness in real-world settings,
such as candidates assessments for university admission. We begin with a
statistical analysis assessing correlations among different decision points
among in the current process, which discovers discrepancies that imply
cognitive bias and inconsistency in decisions. This motivates our exploration
of bias-aware AI-augmented workflow that surpass human judgment. We propose
BGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,
Gated Residual Connections and Multi-Head Attention. Using it as a backbone
model, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,
which simulate real-world decision-making. In our experiments, both the
proposed model and the agentic workflow significantly improves on both human
judgment and alternative models, validated with real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code is available at: https://github.com/junhua/bgm-han</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterization of Political Polarized Users Attacked by Language
  Toxicity on Twitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the dynamics of language toxicity on social media is important
for us to investigate the propagation of misinformation and the development of
echo chambers for political scenarios such as U.S. presidential elections.
Recent research has used large-scale data to investigate the dynamics across
social media platforms. However, research on the toxicity dynamics is not
enough. This study aims to provide a first exploration of the potential
language toxicity flow among Left, Right and Center users. Specifically, we aim
to examine whether Left users were easier to be attacked by language toxicity.
In this study, more than 500M Twitter posts were examined. It was discovered
that Left users received much more toxic replies than Right and Center users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Problematic Tokens: Tokenizer Bias in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Yang, Zhiqiang Wang, Yanbin Lin, Zunduo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,
have shown exceptional performance, especially in languages with abundant
resources like English, thanks to extensive datasets that ensure robust
training. Conversely, these models exhibit limitations when processing
under-resourced languages such as Chinese and Korean, where issues including
hallucinatory responses remain prevalent. This paper traces the roots of these
disparities to the tokenization process inherent to these models. Specifically,
it explores how the tokenizers vocabulary, often used to speed up the
tokenization process and reduce tokens but constructed independently of the
actual model training data, inadequately represents non-English languages. This
misrepresentation results in the propagation of under-trained or untrained
tokens, which perpetuate biases and pose serious concerns related to data
security and ethical standards. We aim to dissect the tokenization mechanics of
GPT-4o, illustrating how its simplified token-handling methods amplify these
risks and offer strategic solutions to mitigate associated security and ethical
issues. Through this study, we emphasize the critical need to rethink
tokenization frameworks to foster more equitable and secure AI technologies.
The code and data are available at https://github.com/yeyimilk/LLMGPT4o
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11th IEEE Special session on Privacy and Security of Big Data (PSBD
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> From Instance Training to Instruction Learning: Task Adapters Generation
  from Instructions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Yanchao Hao, Shengping Liu, Kang Liu, <span class="highlight-author">Jun Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have acquired the ability to solve general tasks
by utilizing instruction finetuning (IFT). However, IFT still relies heavily on
instance training of extensive task data, which greatly limits the adaptability
of LLMs to real-world scenarios where labeled task instances are scarce and
broader task generalization becomes paramount. Contrary to LLMs, humans acquire
skills and complete tasks not merely through repeated practice but also by
understanding and following instructional guidelines. This paper is dedicated
to simulating human learning to address the shortcomings of instance training,
focusing on instruction learning to enhance cross-task generalization. Within
this context, we introduce Task Adapters Generation from Instructions (TAGI),
which automatically constructs the task-specific model in a parameter
generation manner based on the given task instructions without retraining for
unseen tasks. Specifically, we utilize knowledge distillation to enhance the
consistency between TAGI developed through Learning with Instruction and
task-specific models developed through Training with Instance, by aligning the
labels, output logits, and adapter parameters between them. TAGI is endowed
with cross-task generalization capabilities through a two-stage training
process that includes hypernetwork pretraining and finetuning. We evaluate TAGI
on the Super-Natural Instructions and P3 datasets. The experimental results
demonstrate that TAGI can match or even outperform traditional meta-trained
models and other hypernetwork models, while significantly reducing
computational requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Rewarding with <span class="highlight-title">Prompt</span> Optimization Enables Tuning-free
  Self-Alignment of <span class="highlight-title">Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Models (LLMs) traditionally relies on costly training
and human preference annotations. Self-alignment seeks to reduce these expenses
by enabling models to align themselves. To further lower costs and achieve
alignment without any expensive tuning or annotations, we introduce a new
tuning-free approach for self-alignment, Dynamic Rewarding with Prompt
Optimization (DRPO). Our approach leverages a search-based optimization
framework that allows LLMs to iteratively self-improve and craft the optimal
alignment instructions, all without additional training or human intervention.
The core of DRPO is a dynamic rewarding mechanism, which identifies and
rectifies model-specific alignment weaknesses, allowing LLMs to adapt
efficiently to diverse alignment challenges. Empirical evaluations on eight
recent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly
enhances alignment performance, with base models outperforming their
SFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by
DRPO surpass those curated by human experts, further validating the
effectiveness of our approach. Our findings highlight the great potential of
current LLMs to achieve adaptive self-alignment through inference-time
optimization, complementing tuning-based alignment methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Heads Matter: A Head-Level KV Cache Compression Method with
  Integrated Retrieval and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Key-Value (KV) caching is a common technique to enhance the computational
efficiency of Large Language Models (LLMs), but its memory overhead grows
rapidly with input length. Prior work has shown that not all tokens are equally
important for text generation, proposing layer-level KV cache compression to
selectively retain key information. Recognizing the distinct roles of attention
heads in generation, we propose HeadKV, a head-level KV cache compression
method, and HeadKV-R2, which leverages a novel contextual reasoning ability
estimation for compression. Our approach operates at the level of individual
heads, estimating their importance for contextual QA tasks that require both
retrieval and reasoning capabilities. Extensive experiments across diverse
benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,
Mistral-7B-Instruct), and long-context abilities tests demonstrate that our
head-level KV cache compression significantly outperforms strong baselines,
particularly in low-resource settings (KV size = 64 & 128). Notably, our method
retains just 1.5% of the KV cache while achieving 97% of the performance of the
full KV cache on the contextual question answering benchmark.Codes are
available at https://github.com/FYYFU/HeadKV
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>2CLIP: Powerful <span class="highlight-title">Language Model</span> Unlocks Richer Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the
  AutoNuggetizer Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on "refactoring" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARM: Unlocking the Future of Recommendation Systems through Memory
  Augmentation and Scalable Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling-law has guided the language model designing for past years, however,
it is worth noting that the scaling laws of NLP cannot be directly applied to
RecSys due to the following reasons: (1) The amount of training samples and
model parameters is typically not the bottleneck for the model. Our
recommendation system can generate over 50 billion user samples daily, and such
a massive amount of training data can easily allow our model parameters to
exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the
stability and robustness of the recommendation system, it is essential to
control computational complexity FLOPs carefully. Considering the above
differences with LLM, we can draw a conclusion that: for a RecSys model,
compared to model parameters, the computational complexity FLOPs is a more
expensive factor that requires careful control. In this paper, we propose our
milestone work, MARM (Memory Augmented Recommendation Model), which explores a
new cache scaling-laws successfully.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-assisted Explicit and Implicit Multi-interest Learning Framework for
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Qiao, Chen Gao, Yong Li, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-interest modeling in current recommender systems (RS) is mainly based
on user behavioral data, capturing user interest preferences from multiple
dimensions. However, since behavioral data is implicit and often highly sparse,
it is challenging to understand users' complex and diverse interests. Recent
studies have shown that the rich semantic information in the text can
effectively supplement the deficiencies of behavioral data. Despite this, it is
still difficult for small models to directly extract semantic features
associated with users' deep interests. That is, how to effectively align
semantics with behavioral information to form a more comprehensive and accurate
understanding of user interests has become a critical research problem.To
address this, we propose an LLM-assisted explicit and implicit multi-interest
learning framework (named EIMF) to model user interests on two levels: behavior
and semantics. The framework consists of two parts: Implicit Behavioral
Interest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The
traditional multi-interest RS model in IBIM can learn users' implicit
behavioral interests from interactions with items. In ESIM, we first adopt a
clustering algorithm to select typical samples and design a prompting strategy
on LLM to obtain explicit semantic interests. Furthermore, in the training
phase, the semantic interests of typical samples can enhance the representation
learning of behavioral interests based on the multi-task learning on semantic
prediction and modality alignment. Therefore, in the inference stage, accurate
recommendations can be achieved with only the user's behavioral data. Extensive
experiments on real-world datasets demonstrate the effectiveness of the
proposed EIMF framework, which effectively and efficiently combines small
models with LLM to improve the accuracy of multi-interest modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing multiple <span class="highlight-title">LLM</span>s for Information Retrieval: A case study on Deep
  Learning methodologies in Biodiversity publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vamsi Krishna Kommineni, Birgitta König-Ries, Sheeba Samuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) techniques are increasingly applied in scientific studies
across various domains to address complex research questions. However, the
methodological details of these DL models are often hidden in the unstructured
text. As a result, critical information about how these models are designed,
trained, and evaluated is challenging to access and comprehend. To address this
issue, in this work, we use five different open-source Large Language Models
(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,
and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)
approach to extract and process DL methodological details from scientific
publications automatically. We built a voting classifier from the outputs of
five LLMs to accurately report DL methodological information. We tested our
approach using biodiversity publications, building upon our previous research.
To validate our pipeline, we employed two datasets of DL-related biodiversity
publications: a curated set of 100 publications from our prior work and a set
of 364 publications from the Ecological Informatics journal. Our results
demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of
DL methodological information, achieving an accuracy of 69.5% (417 out of 600
comparisons) based solely on textual content from publications. This
performance was assessed against human annotators who had access to code,
figures, tables, and other supplementary information. Although demonstrated in
biodiversity, our methodology is not limited to this field; it can be applied
across other scientific domains where detailed methodological reporting is
essential for advancing knowledge and ensuring reproducibility. This study
presents a scalable and reliable approach for automating information
extraction, facilitating better reproducibility and knowledge transfer across
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive and Practical Evaluation of Retrieval-Augmented Generation
  Systems for Medical Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Trung Ngo, Chien Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeBaTeR: Denoising Bipartite <span class="highlight-title">Temporal</span> Graph for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu He, Jose Sepulveda, Mostafa Rahmani, Alyssa Woo, Fei Wang, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the difficulty of acquiring large-scale explicit user feedback,
implicit feedback (e.g., clicks or other interactions) is widely applied as an
alternative source of data, where user-item interactions can be modeled as a
bipartite graph. Due to the noisy and biased nature of implicit real-world
user-item interactions, identifying and rectifying noisy interactions are vital
to enhance model performance and robustness. Previous works on purifying
user-item interactions in collaborative filtering mainly focus on mining the
correlation between user/item embeddings and noisy interactions, neglecting the
benefit of temporal patterns in determining noisy interactions. Time
information, while enhancing the model utility, also bears its natural
advantage in helping to determine noisy edges, e.g., if someone usually watches
horror movies at night and talk shows in the morning, a record of watching a
horror movie in the morning is more likely to be noisy interaction. Armed with
this observation, we introduce a simple yet effective mechanism for generating
time-aware user/item embeddings and propose two strategies for denoising
bipartite temporal graph in recommender systems (DeBaTeR): the first is through
reweighting the adjacency matrix (DeBaTeR-A), where a reliability score is
defined to reweight the edges through both soft assignment and hard assignment;
the second is through reweighting the loss function (DeBaTeR-L), where weights
are generated to reweight user-item samples in the losses. Extensive
experiments have been conducted to demonstrate the efficacy of our methods and
illustrate how time information indeed helps identifying noisy edges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRSD: Rethinking Similarity and Diversity for Retrieval in Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Gao, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector retrieval algorithms are essential for semantic queries within the
rapidly evolving landscape of Large Language Models (LLMs). The ability to
retrieve vectors that satisfy both similarity and diversity criteria
substantially enhances the performance of LLMs. Although Maximal Marginal
Relevance (MMR) is widely employed in retrieval scenarios requiring relevance
and diversity, variations in the parameter $\lambda$ lead to fluctuations that
complicate the optimization trajectory in vector spaces. This obscures the
direction of improvement and highlights the lack of a robust theoretical
analysis regarding similarity and diversity constraints in retrieval processes.
To address these challenges, this paper introduces a novel approach that
characterizes both constraints through the relationship between the sum vector
and the query vector. The proximity of these vectors ensures the similarity
constraint, while requiring individual vectors within the sum vector to diverge
in their alignment with the query vector satisfies the diversity constraint. We
first formulate a new combinatorial optimization problem, selecting k vectors
from a candidate set such that their sum vector maximally aligns with the query
vector, and demonstrate that this problem is NP-complete. This result
underscores the inherent difficulty of simultaneously achieving similarity and
diversity in vector retrieval, thereby providing a theoretical foundation for
future research. Subsequently, we present the heuristic algorithm Vectors
Retrieval with Similarity and Diversity, VRSD, which features a clear
optimization objective and eliminates the need for preset parameters. VRSD also
achieves a modest reduction in time complexity compared to MMR. Empirical
validation confirms that VRSD significantly outperforms MMR across various
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIMER-IT: Zero-Shot NER on Italian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to Named Entity Recognition (NER) frame the task into
a BIO sequence labeling problem. Although these systems often excel in the
downstream task at hand, they require extensive annotated data and struggle to
generalize to out-of-distribution input domains and unseen entity types. On the
contrary, Large Language Models (LLMs) have demonstrated strong zero-shot
capabilities. While several works address Zero-Shot NER in English, little has
been done in other languages. In this paper, we define an evaluation framework
for Zero-Shot NER, applying it to the Italian language. Furthermore, we
introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning
approach for zero-shot NER leveraging prompts enriched with definition and
guidelines. Comparisons with other state-of-the-art models, demonstrate the
superiority of SLIMER-IT on never-seen-before entity tags.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial
  Keyword Queries <span class="chip">VLDB</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yin, Shanshan Feng, Shang Liu, Gao Cong, Yew Soon Ong, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of spatio-textual data, Top-k KNN spatial keyword
queries (TkQs), which return a list of objects based on a ranking function that
considers both spatial and textual relevance, have found many real-life
applications. To efficiently handle TkQs, many indexes have been developed, but
the effectiveness of TkQ is limited. To improve effectiveness, several deep
learning models have recently been proposed, but they suffer severe efficiency
issues and there are no efficient indexes specifically designed to accelerate
the top-k search process for these deep learning models. To tackle these
issues, we consider embedding based spatial keyword queries, which capture the
semantic meaning of query keywords and object descriptions in two separate
embeddings to evaluate textual relevance. Although various models can be used
to generate these embeddings, no indexes have been specifically designed for
such queries. To fill this gap, we propose LIST, a novel machine learning based
Approximate Nearest Neighbor Search index that Learns to Index the
Spatio-Textual data. LIST utilizes a new learning-to-cluster technique to group
relevant queries and objects together while separating irrelevant queries and
objects. There are two key challenges in building an effective and efficient
index, i.e., the absence of high-quality labels and the unbalanced clustering
results. We develop a novel pseudo-label generation technique to address the
two challenges. Additionally, we introduce a learning based spatial relevance
model that can integrates with various text relevance models to form a
lightweight yet effective relevance for reranking objects retrieved by LIST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by VLDB Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LabelCraft: Empowering Short Video Recommendations with Automated Label
  Crafting <span class="chip">WSDM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Bai, Yang Zhang, Jing Lu, Jianxin Chang, Xiaoxue Zang, Yanan Niu, Yang Song, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short video recommendations often face limitations due to the quality of user
feedback, which may not accurately depict user interests. To tackle this
challenge, a new task has emerged: generating more dependable labels from
original feedback. Existing label generation methods rely on manual rules,
demanding substantial human effort and potentially misaligning with the desired
objectives of the platform. To transcend these constraints, we introduce
LabelCraft, a novel automated label generation method explicitly optimizing
pivotal operational metrics for platform success. By formulating label
generation as a higher-level optimization problem above recommender model
optimization, LabelCraft introduces a trainable labeling model for automatic
label mechanism modeling. Through meta-learning techniques, LabelCraft
effectively addresses the bi-level optimization hurdle posed by the recommender
and labeling models, enabling the automatic acquisition of intricate label
generation mechanisms. Extensive experiments on real-world datasets corroborate
LabelCraft's excellence across varied operational metrics, encompassing usage
time, user engagement, and retention. Codes are available at
https://github.com/baiyimeng/LabelCraft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WSDM'24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">116</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Surprising Effectiveness of Attention Transfer for Vision
  <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander C. Li, Yuandong Tian, Beidi Chen, Deepak Pathak, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional wisdom suggests that pre-training Vision Transformers (ViT)
improves downstream performance by learning useful representations. Is this
actually true? We investigate this question and find that the features and
representations learned during pre-training are not essential. Surprisingly,
using only the attention patterns from pre-training (i.e., guiding how
information flows between tokens) is sufficient for models to learn high
quality features from scratch and achieve comparable downstream performance. We
show this by introducing a simple method called attention transfer, where only
the attention patterns from a pre-trained teacher ViT are transferred to a
student, either by copying or distilling the attention maps. Since attention
transfer lets the student learn its own features, ensembling it with a
fine-tuned teacher also further improves accuracy on ImageNet. We
systematically study various aspects of our findings on the sufficiency of
attention maps, including distribution shift settings where they underperform
fine-tuning. We hope our exploration provides a better understanding of what
pre-training accomplishes and leads to a useful alternative to the standard
practice of fine-tuning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code:
  https://github.com/alexlioralexli/attention-transfer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional regression for the Nonlinear Single-Variable Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yantao Wu, Mauro Maggioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several statistical models for regression of a function $F$ on $\mathbb{R}^d$
without the statistical and computational curse of dimensionality exist, for
example by imposing and exploiting geometric assumptions on the distribution of
the data (e.g. that its support is low-dimensional), or strong smoothness
assumptions on $F$, or a special structure $F$. Among the latter, compositional
models assume $F=f\circ g$ with $g$ mapping to $\mathbb{R}^r$ with $r\ll d$,
have been studied, and include classical single- and multi-index models and
recent works on neural networks. While the case where $g$ is linear is rather
well-understood, much less is known when $g$ is nonlinear, and in particular
for which $g$'s the curse of dimensionality in estimating $F$, or both $f$ and
$g$, may be circumvented. In this paper, we consider a model
$F(X):=f(\Pi_\gamma X) $ where $\Pi_\gamma:\mathbb{R}^d\to[0,\rm{len}_\gamma]$
is the closest-point projection onto the parameter of a regular curve $\gamma:
[0,\rm{len}_\gamma]\to\mathbb{R}^d$ and $f:[0,\rm{len}_\gamma]\to\mathbb{R}^1$.
The input data $X$ is not low-dimensional, far from $\gamma$, conditioned on
$\Pi_\gamma(X)$ being well-defined. The distribution of the data, $\gamma$ and
$f$ are unknown. This model is a natural nonlinear generalization of the
single-index model, which corresponds to $\gamma$ being a line. We propose a
nonparametric estimator, based on conditional regression, and show that under
suitable assumptions, the strongest of which being that $f$ is coarsely
monotone, it can achieve the $one$-$dimensional$ optimal min-max rate for
non-parametric regression, up to the level of noise in the observations, and be
constructed in time $\mathcal{O}(d^2n\log n)$. All the constants in the
learning bounds, in the minimal number of samples required for our bounds to
hold, and in the computational complexity are at most low-order polynomials in
$d$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Classification of Open-Source ML Models and <span class="highlight-title">Dataset</span>s for
  Software Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra González, Xavier Franch, David Lo, Silverio Martínez-Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Open-Source Pre-Trained Models (PTMs) and datasets provide
extensive resources for various Machine Learning (ML) tasks, yet these
resources lack a classification tailored to Software Engineering (SE) needs.
Aims: We apply an SE-oriented classification to PTMs and datasets on a popular
open-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs
over time. Method: We conducted a repository mining study. We started with a
systematically gathered database of PTMs and datasets from the HF API. Our
selection was refined by analyzing model and dataset cards and metadata, such
as tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are
replicable, with a publicly accessible replication package. Results: The most
common SE task among PTMs and datasets is code generation, with a primary focus
on software development and limited attention to software management. Popular
PTMs and datasets mainly target software development. Among ML tasks, text
generation is the most common in SE PTMs and datasets. There has been a marked
increase in PTMs for SE since 2023 Q2. Conclusions: This study underscores the
need for broader task coverage to enhance the integration of ML within SE
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralDEM - Real-time Simulation of Industrial Particulate Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Alkin, Tobias Kronlachner, Samuele Papa, Stefan Pirker, Thomas Lichtenegger, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in computing power have made it possible to numerically simulate
large-scale fluid-mechanical and/or particulate systems, many of which are
integral to core industrial processes. Among the different numerical methods
available, the discrete element method (DEM) provides one of the most accurate
representations of a wide range of physical systems involving granular and
discontinuous materials. Consequently, DEM has become a widely accepted
approach for tackling engineering problems connected to granular flows and
powder mechanics. Additionally, DEM can be integrated with grid-based
computational fluid dynamics (CFD) methods, enabling the simulation of chemical
processes taking place, e.g., in fluidized beds. However, DEM is
computationally intensive because of the intrinsic multiscale nature of
particulate systems, restricting simulation duration or number of particles.
Towards this end, NeuralDEM presents an end-to-end approach to replace slow
numerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEM
is capable of picturing long-term transport processes across different regimes
using macroscopic observables without any reference to microscopic model
parameters. First, NeuralDEM treats the Lagrangian discretization of DEM as an
underlying continuous field, while simultaneously modeling macroscopic behavior
directly as additional auxiliary fields. Second, NeuralDEM introduces
multi-branch neural operators scalable to real-time modeling of
industrially-sized scenarios - from slow and pseudo-steady to fast and
transient. Such scenarios have previously posed insurmountable challenges for
deep learning models. Notably, NeuralDEM faithfully models coupled CFD-DEM
fluidized bed reactors of 160k CFD cells and 500k DEM particles for
trajectories of 28s. NeuralDEM will open many new doors to advanced engineering
and much faster process cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://nx-ai.github.io/NeuralDEM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable
  Medical Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahan Bhatt, Nandan Vaghela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Med-Bot, an AI-powered chatbot designed to provide
users with accurate and reliable medical information. Utilizing advanced
libraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,
Med-Bot is built to handle the complexities of natural language understanding
in a healthcare context. The integration of llamaassisted data processing and
AutoGPT-Q provides enhanced performance in processing and responding to queries
based on PDFs of medical literature, ensuring that users receive precise and
trustworthy information. This research details the methodologies employed in
developing Med-Bot and evaluates its effectiveness in disseminating healthcare
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical
  chatbot, Context-based interaction, Llama-assisted data processing,
  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine
  learning in healthcare, Conversational AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How do Machine Learning Models Change? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Castaño, Rafael Cabañas, Antonio Salmerón, David Lo, Silverio Martínez-Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Machine Learning (ML) models and their open-source
implementations has transformed Artificial Intelligence research and
applications. Platforms like Hugging Face (HF) enable the development, sharing,
and deployment of these models, fostering an evolving ecosystem. While previous
studies have examined aspects of models hosted on platforms like HF, a
comprehensive longitudinal study of how these models change remains
underexplored. This study addresses this gap by utilizing both repository
mining and longitudinal analysis methods to examine over 200,000 commits and
1,200 releases from over 50,000 models on HF. We replicate and extend an ML
change taxonomy for classifying commits and utilize Bayesian networks to
uncover patterns in commit and release activities over time. Our findings
indicate that commit activities align with established data science
methodologies, such as CRISP-DM, emphasizing iterative refinement and
continuous improvement. Additionally, release patterns tend to consolidate
significant updates, particularly in documentation, distinguishing between
granular changes and milestone-based releases. Furthermore, projects with
higher popularity prioritize infrastructure enhancements early in their
lifecycle, and those with intensive collaboration practices exhibit improved
documentation standards. These and other insights enhance the understanding of
model changes on community platforms and provide valuable guidance for best
practices in model maintenance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Operators Can Play Dynamic Stackelberg Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Alvarez, Ibrahim Ekren, Anastasis Kratsios, Xuwei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Stackelberg games are a broad class of two-player games in which the
leader acts first, and the follower chooses a response strategy to the leader's
strategy. Unfortunately, only stylized Stackelberg games are explicitly
solvable since the follower's best-response operator (as a function of the
control of the leader) is typically analytically intractable. This paper
addresses this issue by showing that the \textit{follower's best-response
operator} can be approximately implemented by an \textit{attention-based neural
operator}, uniformly on compact subsets of adapted open-loop controls for the
leader. We further show that the value of the Stackelberg game where the
follower uses the approximate best-response operator approximates the value of
the original Stackelberg game. Our main result is obtained using our universal
approximation theorem for attention-based neural operators between spaces of
square-integrable adapted stochastic processes, as well as stability results
for a general class of Stackelberg games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Limits of Language Generation: Trade-Offs Between Hallucination
  and Mode Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specifying all desirable properties of a language model is challenging, but
certain requirements seem essential. Given samples from an unknown language,
the trained model should produce valid strings not seen in training and be
expressive enough to capture the language's full richness. Otherwise,
outputting invalid strings constitutes "hallucination," and failing to capture
the full range leads to "mode collapse." We ask if a language model can meet
both requirements.
  We investigate this within a statistical language generation setting building
on Gold and Angluin. Here, the model receives random samples from a
distribution over an unknown language K, which belongs to a possibly infinite
collection of languages. The goal is to generate unseen strings from K. We say
the model generates from K with consistency and breadth if, as training size
increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in
language generation are possible. We answer this negatively: for a large class
of language models, including next-token prediction models, this is impossible
for most collections of candidate languages. This contrasts with [KM24]'s
result, showing consistent generation without breadth is possible for any
countable collection of languages. Our finding highlights that generation with
breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples
needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is
achievable for any countable collection of languages when negative examples
(strings outside K) are available alongside positive ones. This suggests that
post-training feedback, which encodes negative examples, can be crucial in
reducing hallucinations while limiting mode collapse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract shortened to fit arXiv limit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCCE: Missingness-aware Causal Concept Explainer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Gao, Guanhua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal concept effect estimation is gaining increasing interest in the field
of interpretable machine learning. This general approach explains the behaviors
of machine learning models by estimating the causal effect of
human-understandable concepts, which represent high-level knowledge more
comprehensibly than raw inputs like tokens. However, existing causal concept
effect explanation methods assume complete observation of all concepts involved
within the dataset, which can fail in practice due to incomplete annotations or
missing concept data. We theoretically demonstrate that unobserved concepts can
bias the estimation of the causal effects of observed concepts. To address this
limitation, we introduce the Missingness-aware Causal Concept Explainer (MCCE),
a novel framework specifically designed to estimate causal concept effects when
not all concepts are observable. Our framework learns to account for residual
bias resulting from missing concepts and utilizes a linear predictor to model
the relationships between these concepts and the outputs of black-box machine
learning models. It can offer explanations on both local and global levels. We
conduct validations using a real-world dataset, demonstrating that MCCE
achieves promising performance compared to state-of-the-art explanation methods
in causal concept effect estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Uncertainty Quantification of Factual Estimand of
  Efficacy from Before-and-After Treatment Repeated Measures Randomized
  Controlled Trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingya Wang, Yang Han, Yushi Liu, Szu-Yu Tang, Jason C. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal estimand for comparing a new treatment $Rx$ with a control $C$ is
the $\textit{counterfactual}$ efficacy $Rx:C$, the expected differential
outcome between $Rx$ and $C$ if each patient were given $\textit{both}$. While
counterfactual $\textit{point estimation}$ from $\textit{factual}$ Randomized
Controlled Trials (RCTs) has been available, this article shows
$\textit{counterfactual}$ uncertainty quantification (CUQ), quantifying
uncertainty for factual point estimates but in a counterfactual setting, is
surprisingly achievable. We achieve CUQ whose variability is typically smaller
than factual UQ, by creating a new statistical modeling principle called ETZ
which is applicable to RCTs with $\textit{Before-and-After}$ treatment Repeated
Measures, common in many therapeutic areas.
  We urge caution when estimate of the unobservable true condition of a patient
before treatment has measurement error, because that violation of standard
regression assumption can cause attenuation in estimating treatment effects.
Fortunately, we prove that, for traditional medicine in general, and for
targeted therapy with efficacy defined as averaged over the population,
counterfactual point estimation is unbiased. However, for targeted therapy,
both Real Human and Digital Twins approaches should respect this limitation,
lest predicted treatment effect in $\textit{subgroups}$ will have bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local deployment of large-scale music AI models on commodity hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Zhou, Charlie Ruan, Zihe Zhao, Tianqi Chen, Chris Donahue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the MIDInfinite, a web application capable of generating symbolic
music using a large-scale generative AI model locally on commodity hardware.
Creating this demo involved porting the Anticipatory Music Transformer, a large
language model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine
Learning Compilation (MLC) framework. Once the model is ported, MLC facilitates
inference on a variety of runtimes including C++, mobile, and the browser. We
envision that MLC has the potential to bridge the gap between the landscape of
increasingly capable music AI models and technology more familiar to music
software developers. As a proof of concept, we build a web application that
allows users to generate endless streams of multi-instrumental MIDI in the
browser, either from scratch or conditioned on a prompt. On commodity hardware
(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster
than real-time playback for 72.9% of generations, and increases to 86.3% with 2
seconds of upfront buffering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MICCAI-CDMRI 2023 QuantConn Challenge Findings on Achieving Robust
  Quantitative Connectivity through Harmonized Preprocessing of <span class="highlight-title">Diffusion</span> MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nancy R. Newlin, Kurt Schilling, Serge Koudoro, Bramsh Qamar Chandio, Praitayini Kanakaraj, Daniel Moyer, Claire E. Kelly, Sila Genc, Jian Chen, Joseph Yuan-Mou Yang, Ye Wu, Yifei He, Jiawei Zhang, Qingrun Zeng, Fan Zhang, Nagesh Adluru, Vishwesh Nath, Sudhir Pathak, Walter Schneider, Anurag Gade, Yogesh Rathi, Tom Hendriks, Anna Vilanova, Maxime Chamberland, Tomasz Pieciak, Dominika Ciupek, Antonio Tristán Vega, Santiago Aja-Fernández, Maciej Malawski, Gani Ouedraogo, Julia Machnio, Christian Ewert, Paul M. Thompson, Neda Jahanshad, Eleftherios Garyfallidis, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White matter alterations are increasingly implicated in neurological diseases
and their progression. International-scale studies use diffusion-weighted
magnetic resonance imaging (DW-MRI) to qualitatively identify changes in white
matter microstructure and connectivity. Yet, quantitative analysis of DW-MRI
data is hindered by inconsistencies stemming from varying acquisition
protocols. There is a pressing need to harmonize the preprocessing of DW-MRI
datasets to ensure the derivation of robust quantitative diffusion metrics
across acquisitions. In the MICCAI-CDMRI 2023 QuantConn challenge, participants
were provided raw data from the same individuals collected on the same scanner
but with two different acquisitions and tasked with preprocessing the DW-MRI to
minimize acquisition differences while retaining biological variation.
Submissions are evaluated on the reproducibility and comparability of
cross-acquisition bundle-wise microstructure measures, bundle shape features,
and connectomics. The key innovations of the QuantConn challenge are that (1)
we assess bundles and tractography in the context of harmonization for the
first time, (2) we assess connectomics in the context of harmonization for the
first time, and (3) we have 10x additional subjects over prior harmonization
challenge, MUSHAC and 100x over SuperMUDI. We find that bundle surface area,
fractional anisotropy, connectome assortativity, betweenness centrality, edge
count, modularity, nodal strength, and participation coefficient measures are
most biased by acquisition and that machine learning voxel-wise correction,
RISH mapping, and NeSH methods effectively reduce these biases. In addition,
microstructure measures AD, MD, RD, bundle length, connectome density,
efficiency, and path length are least biased by these acquisition differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024/019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Moral Foundations Weibo Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Cao, Miaoyan Hu, Jiahan Wei, Baha Ihnaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moral sentiments expressed in natural language significantly influence both
online and offline environments, shaping behavioral styles and interaction
patterns, including social media selfpresentation, cyberbullying, adherence to
social norms, and ethical decision-making. To effectively measure moral
sentiments in natural language processing texts, it is crucial to utilize
large, annotated datasets that provide nuanced understanding for accurate
analysis and modeltraining. However, existing corpora, while valuable, often
face linguistic limitations. To address this gap in the Chinese language
domain,we introduce the Moral Foundation Weibo Corpus. This corpus consists of
25,671 Chinese comments on Weibo, encompassing six diverse topic areas. Each
comment is manually annotated by at least three systematically trained
annotators based on ten moral categories derived from a grounded theory of
morality. To assess annotator reliability, we present the kappa testresults, a
gold standard for measuring consistency. Additionally, we apply several the
latest large language models to supplement the manual annotations, conducting
analytical experiments to compare their performance and report baseline results
for moral sentiment classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latency Optimization in LEO Satellite Communications with Hybrid Beam
  Pattern and Interference Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Zhang, Ye Hu, Minchae Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of low Earth orbit (LEO) satellite communication
systems has significantly enhanced global connectivity, offering high-capacity,
low-latency services crucial for next-generation applications. However, the
dense configuration of LEO constellations poses challenges in resource
allocation optimization and interference management, complicating coexistence
with other communication systems. To address these limitations, this paper
proposes a novel framework for optimizing the beam scheduling and resource
allocation in multi-beam LEO systems. To satisfy the uneven terrestrial traffic
demand, a hybrid beam pattern is employed to enhance the downlink quality of
service and minimize the transmission latency from LEO satellites to ground
user terminals. Additionally, a dynamic co-channel interference (CCI) control
mechanism is developed to mitigate inter-beam interference within the LEO
constellation and limit cross-system interference affecting protected users
from other networks. The problem of user-beam-frequency allocation with power
optimization is formulated as a mixed-integer dynamic programming model and
solved using a low-complexity neural network-based graph generation algorithm.
Simulation results show that the proposed approach outperforms the baseline
methods of full frequency reuse and single-channel transmission, and highlights
the potential for further performance improvement with multi-user
transmissions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA-Mesh: Unifying 3D Mesh Generation with <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores expanding the capabilities of large language models (LLMs)
pretrained on text to generate 3D meshes within a unified model. This offers
key advantages of (1) leveraging spatial knowledge already embedded in LLMs,
derived from textual sources like 3D tutorials, and (2) enabling conversational
3D generation and mesh understanding. A primary challenge is effectively
tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.
To address this, we introduce LLaMA-Mesh, a novel approach that represents the
vertex coordinates and face definitions of 3D meshes as plain text, allowing
direct integration with LLMs without expanding the vocabulary. We construct a
supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate
3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs
as required, and (3) understand and interpret 3D meshes. Our work is the first
to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge
for 3D mesh generation in a text-based format, effectively unifying the 3D and
text modalities. LLaMA-Mesh achieves mesh generation quality on par with models
trained from scratch while maintaining strong text generation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expert Study on Interpretable Machine Learning Models with Missing Data <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Stempfle, Arthur James, Julie Josse, Tobias Gauss, Fredrik D. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inherently interpretable machine learning (IML) models provide valuable
insights for clinical decision-making but face challenges when features have
missing values. Classical solutions like imputation or excluding incomplete
records are often unsuitable in applications where values are missing at test
time. In this work, we conducted a survey with 71 clinicians from 29 trauma
centers across France, including 20 complete responses to study the interaction
between medical professionals and IML applied to data with missing values. This
provided valuable insights into how missing data is interpreted in clinical
machine learning. We used the prediction of hemorrhagic shock as a concrete
example to gauge the willingness and readiness of the participants to adopt IML
models from three classes of methods. Our findings show that, while clinicians
value interpretability and are familiar with common IML methods, classical
imputation techniques often misalign with their intuition, and that models that
natively handle missing values are preferred. These results emphasize the need
to integrate clinical intuition into future IML models for better
human-computer interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Deviation Learning for Visual Anomaly Detection with Data
  Contamination <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindya Sundar Das, Guansong Pang, Monowar Bhuyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual anomaly detection targets to detect images that notably differ from
normal pattern, and it has found extensive application in identifying defective
parts within the manufacturing industry. These anomaly detection paradigms
predominantly focus on training detection models using only clean, unlabeled
normal samples, assuming an absence of contamination; a condition often unmet
in real-world scenarios. The performance of these methods significantly depends
on the quality of the data and usually decreases when exposed to noise. We
introduce a systematic adaptive method that employs deviation learning to
compute anomaly scores end-to-end while addressing data contamination by
assigning relative importance to the weights of individual instances. In this
approach, the anomaly scores for normal instances are designed to approximate
scalar scores obtained from the known prior distribution. Meanwhile, anomaly
scores for anomaly examples are adjusted to exhibit statistically significant
deviations from these reference scores. Our approach incorporates a constrained
optimization problem within the deviation learning framework to update instance
weights, resolving this problem for each mini-batch. Comprehensive experiments
on the MVTec and VisA benchmark datasets indicate that our proposed method
surpasses competing techniques and exhibits both stability and robustness in
the presence of data contamination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equation-informed data-driven identification of flow budgets and
  dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataliya Sevryugina, Serena Costanzo, Steve de Bruyn Kops, Colm-cille Caulfield, Iraj Mortazavi, Taraneh Sayadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational Fluid Dynamics (CFD) is an indispensable method of fluid
modelling in engineering applications, reducing the need for physical
prototypes and testing for tasks such as design optimisation and performance
analysis. Depending on the complexity of the system under consideration, models
ranging from low to high fidelity can be used for prediction, allowing
significant speed-up. However, the choice of model requires information about
the actual dynamics of the flow regime. Correctly identifying the
regions/clusters of flow that share the same dynamics has been a challenging
research topic to date. In this study, we propose a novel hybrid approach to
flow clustering. It consists of characterising each sample point of the system
with equation-based features, i.e. features are budgets that represent the
contribution of each term from the original governing equation to the local
dynamics at each sample point. This was achieved by applying the Sparse
Identification of Nonlinear Dynamical systems (SINDy) method pointwise to time
evolution data. The method proceeds with equation-based clustering using the
Girvan-Newman algorithm. This allows the detection of communities that share
the same physical dynamics. The algorithm is implemented in both Eulerian and
Lagrangian frameworks. In the Lagrangian, i.e. dynamic approach, the clustering
is performed on the trajectory of each point, allowing the change of clusters
to be represented also in time. The performance of the algorithm is first
tested on a flow around a cylinder. The construction of the dynamic clusters in
this test case clearly shows the evolution of the wake from the steady state
solution through the transient to the oscillatory solution. Dynamic clustering
was then successfully tested on turbulent flow data. Two distinct and
well-defined clusters were identified and their temporal evolution was
reconstructed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing the Unseen: Detecting Hidden Backdoors in Black-Box Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Xuan Huang, Jia-Wei Chen, Zhi-Peng Zhang, Chia-Mu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual prompting (VP) is a new technique that adapts well-trained frozen
models for source domain tasks to target domain tasks. This study examines VP's
benefits for black-box model-level backdoor detection. The visual prompt in VP
maps class subspaces between source and target domains. We identify a
misalignment, termed class subspace inconsistency, between clean and poisoned
datasets. Based on this, we introduce \textsc{BProm}, a black-box model-level
detection method to identify backdoors in suspicious models, if any.
\textsc{BProm} leverages the low classification accuracy of prompted models
when backdoors are present. Extensive experiments confirm \textsc{BProm}'s
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practical Guide to Fine-tuning <span class="highlight-title">Language Model</span>s with Limited Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Márton Szép, Daniel Rueckert, Rüdiger von Eisenhart-Rothe, Florian Hinterwimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing pre-trained Large Language Models (LLMs) has become the de facto
standard in Natural Language Processing (NLP) despite their extensive data
requirements. Motivated by the recent surge in research focused on training
LLMs with limited data, particularly in low-resource domains and languages,
this paper surveys recent transfer learning approaches to optimize model
performance in downstream tasks where data is scarce. We first address initial
and continued pre-training strategies to better leverage prior knowledge in
unseen domains and languages. We then examine how to maximize the utility of
limited data during fine-tuning and few-shot learning. The final section takes
a task-specific perspective, reviewing models and methods suited for different
levels of data scarcity. Our goal is to provide practitioners with practical
guidelines for overcoming the challenges posed by constrained data while also
highlighting promising directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Randomized Truthful Auctions with Learning <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gagan Aggarwal, Anupam Gupta, Andres Perlroth, Grigoris Velegkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a setting where agents use no-regret learning algorithms to
participate in repeated auctions. \citet{kolumbus2022auctions} showed, rather
surprisingly, that when bidders participate in second-price auctions using
no-regret bidding algorithms, no matter how large the number of interactions
$T$ is, the runner-up bidder may not converge to bidding truthfully. Our first
result shows that this holds for \emph{general deterministic} truthful
auctions. We also show that the ratio of the learning rates of the bidders can
\emph{qualitatively} affect the convergence of the bidders. Next, we consider
the problem of revenue maximization in this environment. In the setting with
fully rational bidders, \citet{myerson1981optimal} showed that revenue can be
maximized by using a second-price auction with reserves.We show that, in stark
contrast, in our setting with learning bidders, \emph{randomized} auctions can
have strictly better revenue guarantees than second-price auctions with
reserves, when $T$ is large enough. Finally, we study revenue maximization in
the non-asymptotic regime. We define a notion of {\em auctioneer regret}
comparing the revenue generated to the revenue of a second price auction with
truthful bids. When the auctioneer has to use the same auction throughout the
interaction, we show an (almost) tight regret bound of $\smash{\widetilde
\Theta(T^{3/4})}.$ If the auctioneer can change auctions during the
interaction, but in a way that is oblivious to the bids, we show an (almost)
tight bound of $\smash{\widetilde \Theta(\sqrt{T})}.$
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAN-Based Architecture for Low-dose Computed Tomography Imaging
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunuo Wang, Ningning Yang, Jialin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have surfaced as a revolutionary
element within the domain of low-dose computed tomography (LDCT) imaging,
providing an advanced resolution to the enduring issue of reconciling radiation
exposure with image quality. This comprehensive review synthesizes the rapid
advancements in GAN-based LDCT denoising techniques, examining the evolution
from foundational architectures to state-of-the-art models incorporating
advanced features such as anatomical priors, perceptual loss functions, and
innovative regularization strategies. We critically analyze various GAN
architectures, including conditional GANs (cGANs), CycleGANs, and
Super-Resolution GANs (SRGANs), elucidating their unique strengths and
limitations in the context of LDCT denoising. The evaluation provides both
qualitative and quantitative results related to the improvements in performance
in benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.
After highlighting the positive results, we discuss some of the challenges
preventing a wider clinical use, including the interpretability of the images
generated by GANs, synthetic artifacts, and the need for clinically relevant
metrics. The review concludes by highlighting the essential significance of
GAN-based methodologies in the progression of precision medicine via tailored
LDCT denoising models, underlining the transformative possibilities presented
by artificial intelligence within contemporary radiological practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication Compression for Tensor Parallel <span class="highlight-title">LLM</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Hansen-Palmus, Michael Truong-Le, Oliver Hausdörfer, Alok Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have pushed the frontier of artificial
intelligence but are comprised of hundreds of billions of parameters and
operations. For faster inference latency, LLMs are deployed on multiple
hardware accelerators through various Model Parallelism strategies. Our paper
looks into the details on one such strategy - Tensor Parallel - and proposes to
reduce latency by compressing inter-accelerator communication. We leverage fine
grained quantization techniques to compress selected activations by 3.5 - 4.5x.
Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with
negligible model performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Golden Noise for <span class="highlight-title">Diffusion</span> Models: A Learning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, Zeke Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion model is a popular paradigm that synthesizes
personalized images by providing a text prompt and a random Gaussian noise.
While people observe that some noises are ``golden noises'' that can achieve
better text-image alignment and higher human preference than others, we still
lack a machine learning framework to obtain those golden noises. To learn
golden noises for diffusion sampling, we mainly make three contributions in
this paper. First, we identify a new concept termed the \textit{noise prompt},
which aims at turning a random Gaussian noise into a golden noise by adding a
small desirable perturbation derived from the text prompt. Following the
concept, we first formulate the \textit{noise prompt learning} framework that
systematically learns ``prompted'' golden noise associated with a text prompt
for diffusion models. Second, we design a noise prompt data collection pipeline
and collect a large-scale \textit{noise prompt dataset}~(NPD) that contains
100k pairs of random noises and golden noises with the associated text prompts.
With the prepared NPD as the training dataset, we trained a small \textit{noise
prompt network}~(NPNet) that can directly learn to transform a random noise
into a golden noise. The learned golden noise perturbation can be considered as
a kind of prompt for noise, as it is rich in semantic information and tailored
to the given text prompt. Third, our extensive experiments demonstrate the
impressive effectiveness and generalization of NPNet on improving the quality
of synthesized images across various diffusion models, including SDXL,
DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and
efficient controller that acts as a plug-and-play module with very limited
additional inference and computational costs, as it just provides a golden
noise instead of a random noise without accessing the original pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developement of Reinforcement Learning based Optimisation Method for
  Side-Sill Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Borse, Rutwik Gulakala, Marcus Stoffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimisation for crashworthiness is a critical part of the vehicle
development process. Due to stringent regulations and increasing market
demands, multiple factors must be considered within a limited timeframe.
However, for optimal crashworthiness design, multiobjective optimisation is
necessary, and for complex parts, multiple design parameters must be evaluated.
This crashworthiness analysis requires computationally intensive finite element
simulations. This challenge leads to the need for inverse multi-parameter
multi-objective optimisation. This challenge leads to the need for
multi-parameter, multi-objective inverse optimisation. This article
investigates a machine learning-based method for this type of optimisation,
focusing on the design optimisation of a multi-cell side sill to improve
crashworthiness results. Furthermore, the optimiser is coupled with an FE
solver to achieve improved results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Bayesian <span class="highlight-title">Generative</span> Modeling for Compressive Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Böck, Sadaf Syed, Wolfgang Utschick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the fundamental linear inverse problem in compressive
sensing (CS) by introducing a new type of regularizing generative prior. Our
proposed method utilizes ideas from classical dictionary-based CS and, in
particular, sparse Bayesian learning (SBL), to integrate a strong
regularization towards sparse solutions. At the same time, by leveraging the
notion of conditional Gaussianity, it also incorporates the adaptability from
generative models to training data. However, unlike most state-of-the-art
generative models, it is able to learn from a few compressed and noisy data
samples and requires no optimization algorithm for solving the inverse problem.
Additionally, similar to Dirichlet prior networks, our model parameterizes a
conjugate prior enabling its application for uncertainty quantification. We
support our approach theoretically through the concept of variational inference
and validate it empirically using different types of compressible signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What makes a good BIM design: quantitative linking between design
  behavior and quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Rui Ni, Peng Pan, Jia-Rui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Architecture Engineering & Construction (AEC) industry, how design
behaviors impact design quality remains unclear. This study proposes a novel
approach, which, for the first time, identifies and quantitatively describes
the relationship between design behaviors and quality of design based on
Building Information Modeling (BIM). Real-time collection and log mining are
integrated to collect raw data of design behaviors. Feature engineering and
various machine learning models are then utilized for quantitative modeling and
interpretation. Results confirm an existing quantifiable relationship which can
be learned by various models. The best-performing model using Extremely Random
Trees achieved an R2 value of 0.88 on the test set. Behavioral features related
to designer's skill level and changes of design intentions are identified to
have significant impacts on design quality. These findings deepen our
understanding of the design process and help forming BIM designs with better
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks and Differential Equations: A hybrid approach for
  data assimilation of fluid flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Quattromini, M. A. Bucci, S. Cherubini, O. Semeraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel hybrid approach that combines Graph Neural
Networks (GNNs) with Reynolds-Averaged Navier Stokes (RANS) equations to
enhance the accuracy of mean flow reconstruction across a range of fluid
dynamics applications. Traditional purely data-driven Neural Networks (NNs)
models, often struggle maintaining physical consistency. Moreover, they
typically require large datasets to achieve reliable performances. The GNN
framework, which naturally handles unstructured data such as complex geometries
in Computational Fluid Dynamics (CFD), is here integrated with RANS equations
as a physical baseline model. The methodology leverages the adjoint method,
enabling the use of RANS-derived gradients as optimization terms in the GNN
training process. This ensures that the learned model adheres to the governing
physics, maintaining physical consistency while improving the prediction
accuracy. We test our approach on multiple CFD scenarios, including cases
involving generalization with respect to the Reynolds number, sparse
measurements, denoising and inpainting of missing portions of the mean flow.
The results demonstrate significant improvements in the accuracy of the
reconstructed mean flow compared to purely data-driven models, using limited
amounts of data in the training dataset. The key strengths of this study are
the integration of physical laws into the training process of the GNN, and the
ability to achieve high-accuracy predictions with a limited amount of data,
making this approach particularly valuable for applications in fluid dynamics
where data is often scarce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResidualDroppath: Enhancing Feature Reuse over Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual connections are one of the most important components in neural
network architectures for mitigating the vanishing gradient problem and
facilitating the training of much deeper networks. One possible explanation for
how residual connections aid deeper network training is by promoting feature
reuse. However, we identify and analyze the limitations of feature reuse with
vanilla residual connections. To address these limitations, we propose
modifications in training methods. Specifically, we provide an additional
opportunity for the model to learn feature reuse with residual connections
through two types of iterations during training. The first type of iteration
involves using droppath, which enforces feature reuse by randomly dropping a
subset of layers. The second type of iteration focuses on training the dropped
parts of the model while freezing the undropped parts. As a result, the dropped
parts learn in a way that encourages feature reuse, as the model relies on the
undropped parts with feature reuse in mind. Overall, we demonstrated
performance improvements in models with residual connections for image
classification in certain cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Renal Cell Carcinoma subtyping: learning from multi-resolution
  localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Mohamad, Francesco Ponzio, Santa Di Cataldo, Damien Ambrosetti, Xavier Descombes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renal Cell Carcinoma is typically asymptomatic at the early stages for many
patients. This leads to a late diagnosis of the tumor, where the curability
likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high,
with respect to its incidence rate. To increase the survival chance, a fast and
correct categorization of the tumor subtype is paramount. Nowadays,
computerized methods, based on artificial intelligence, represent an
interesting opportunity to improve the productivity and the objectivity of the
microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their
exploitation is hampered by the paucity of annotated dataset, essential for a
proficient training of supervised machine learning technologies. This study
sets out to investigate a novel self supervised training strategy for machine
learning diagnostic tools, based on the multi-resolution nature of the
histological samples. We aim at reducing the need of annotated dataset, without
significantly reducing the accuracy of the tool. We demonstrate the
classification capability of our tool on a whole slide imaging dataset for
Renal Cancer subtyping, and we compare our solution with several
state-of-the-art classification counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Machine Learning for Single-Shot Measurement of Free Electron
  Laser Pulse Power <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Till Korten, Vladimir Rybnikov, Mathias Vogt, Juliane Roensch-Schulenburg, Peter Steinbach, Najmeh Mirian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electron beam accelerators are essential in many scientific and technological
fields. Their operation relies heavily on the stability and precision of the
electron beam. Traditional diagnostic techniques encounter difficulties in
addressing the complex and dynamic nature of electron beams. Particularly in
the context of free-electron lasers (FELs), it is fundamentally impossible to
measure the lasing-on and lasingoff electron power profiles for a single
electron bunch. This is a crucial hurdle in the exact reconstruction of the
photon pulse profile. To overcome this hurdle, we developed a machine learning
model that predicts the temporal power profile of the electron bunch in the
lasing-off regime using machine parameters that can be obtained when lasing is
on. The model was statistically validated and showed superior predictions
compared to the state-of-the-art batch calibrations. The work we present here
is a critical element for a virtual pulse reconstruction diagnostic (VPRD) tool
designed to reconstruct the power profile of individual photon pulses without
requiring repeated measurements in the lasing-off regime. This promises to
significantly enhance the diagnostic capabilities in FELs at large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, Machine Learning and the Physical Sciences
  Workshop, NeurIPS 2024 https://neurips.cc/virtual/2024/100009</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caravan MultiMet: Extending Caravan with Multiple Weather Nowcasts and
  <span class="highlight-title">Forecast</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Shalev, Frederik Kratzert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Caravan large-sample hydrology dataset (Kratzert et al., 2023) was
created to standardize and harmonize streamflow data from various regional
datasets, combined with globally available meteorological forcing and catchment
attributes. This community-driven project also allows researchers to
conveniently extend the dataset for additional basins, as done 6 times to date
(see https://github.com/kratzert/Caravan/discussions/10). We present a novel
extension to Caravan, focusing on enriching the meteorological forcing data.
Our extension adds three precipitation nowcast products (CPC, IMERG v07 Early,
and CHIRPS) and three weather forecast products (ECMWF IFS HRES, GraphCast, and
CHIRPS-GEFS) to the existing ERA5-Land reanalysis data. The inclusion of
diverse data sources, particularly weather forecasts, enables more robust
evaluation and benchmarking of hydrological models, especially for real-time
forecasting scenarios. To the best of our knowledge, this extension makes
Caravan the first large-sample hydrology dataset to incorporate weather
forecast data, significantly enhancing its capabilities and fostering
advancements in hydrological research, benchmarking, and real-time hydrologic
forecasting. The data is publicly available under a CC-BY-4.0 license on Zenodo
in two parts (https://zenodo.org/records/14161235,
https://zenodo.org/records/14161281) and on Google Cloud Platform (GCP) - see
more under the Data Availability chapter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Tailed Object Detection <span class="highlight-title">Pre-train</span>ing: Dynamic Rebalancing
  <span class="highlight-title">Contrastive</span> Learning with Dual Reconstruction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Long Duan, Yong Li, Xiu-Shen Wei, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training plays a vital role in various vision tasks, such as object
recognition and detection. Commonly used pre-training methods, which typically
rely on randomized approaches like uniform or Gaussian distributions to
initialize model parameters, often fall short when confronted with long-tailed
distributions, especially in detection tasks. This is largely due to extreme
data imbalance and the issue of simplicity bias. In this paper, we introduce a
novel pre-training framework for object detection, called Dynamic Rebalancing
Contrastive Learning with Dual Reconstruction (2DRCL). Our method builds on a
Holistic-Local Contrastive Learning mechanism, which aligns pre-training with
object detection by capturing both global contextual semantics and detailed
local patterns. To tackle the imbalance inherent in long-tailed data, we design
a dynamic rebalancing strategy that adjusts the sampling of underrepresented
instances throughout the pre-training process, ensuring better representation
of tail classes. Moreover, Dual Reconstruction addresses simplicity bias by
enforcing a reconstruction task aligned with the self-consistency principle,
specifically benefiting underrepresented tail classes. Experiments on COCO and
LVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in
improving the mAP/AP scores for tail classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous
  Vehicle Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhou, Lin Wang, Qiang Meng, Xiaofan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic and diverse road scenarios is essential for autonomous
vehicle testing and validation. Nevertheless, owing to the complexity and
variability of real-world road environments, creating authentic and varied
scenarios for intelligent driving testing is challenging. In this paper, we
propose DiffRoad, a novel diffusion model designed to produce controllable and
high-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities
of diffusion models to synthesize road layouts from white noise through an
inverse denoising process, preserving real-world spatial features. To enhance
the quality of generated scenarios, we design the Road-UNet architecture,
optimizing the balance between backbone and skip connections for high-realism
scenario generation. Furthermore, we introduce a road scenario evaluation
module that screens adequate and reasonable scenarios for intelligent driving
testing using two critical metrics: road continuity and road reasonableness.
Experimental results on multiple real-world datasets demonstrate DiffRoad's
ability to generate realistic and smooth road structures while maintaining the
original distribution. Additionally, the generated scenarios can be fully
automated into the OpenDRIVE format, facilitating generalized autonomous
vehicle simulation testing. DiffRoad provides a rich and diverse scenario
library for large-scale autonomous vehicle testing and offers valuable insights
for future infrastructure designs that are better suited for autonomous
vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning efficient and provably convergent splitting methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L. M. Kreusser, H. E. Lockyer, E. H. Müller, P. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Splitting methods are widely used for solving initial value problems (IVPs)
due to their ability to simplify complicated evolutions into more manageable
subproblems which can be solved efficiently and accurately. Traditionally,
these methods are derived using analytic and algebraic techniques from
numerical analysis, including truncated Taylor series and their Lie algebraic
analogue, the Baker--Campbell--Hausdorff formula. These tools enable the
development of high-order numerical methods that provide exceptional accuracy
for small timesteps. Moreover, these methods often (nearly) conserve important
physical invariants, such as mass, unitarity, and energy. However, in many
practical applications the computational resources are limited. Thus, it is
crucial to identify methods that achieve the best accuracy within a fixed
computational budget, which might require taking relatively large timesteps. In
this regime, high-order methods derived with traditional methods often exhibit
large errors since they are only designed to be asymptotically optimal. Machine
Learning techniques offer a potential solution since they can be trained to
efficiently solve a given IVP with less computational resources. However, they
are often purely data-driven, come with limited convergence guarantees in the
small-timestep regime and do not necessarily conserve physical invariants. In
this work, we propose a framework for finding machine learned splitting methods
that are computationally efficient for large timesteps and have provable
convergence and conservation guarantees in the small-timestep limit. We
demonstrate numerically that the learned methods, which by construction
converge quadratically in the timestep size, can be significantly more
efficient than established methods for the Schr\"{o}dinger equation if the
computational budget is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph
  Attention for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shravan Venkatraman, Jaskaran Singh Walia, Joe Dhanith P R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classification is a computer vision task where a model analyzes an
image to categorize it into a specific label. Vision Transformers (ViT) improve
this task by leveraging self-attention to capture complex patterns and long
range relationships between image patches. However, a key challenge for ViTs is
efficiently incorporating multiscale feature representations, which is inherent
in CNNs through their hierarchical structure. In this paper, we introduce the
Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework
that addresses this challenge by integrating multi-scale features. Using
EfficientNet as a backbone, the model extracts multi-scale feature maps, which
are divided into patches to preserve semantic information. These patches are
organized into a graph based on spatial and feature similarities, with a Graph
Attention Network (GAT) refining the node embeddings. Finally, a Transformer
encoder captures long-range dependencies and complex interactions. The SAG-ViT
is evaluated on benchmark datasets, demonstrating its effectiveness in
enhancing image classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inherently Interpretable and Uncertainty-Aware Models for Online
  Learning in Cyber-Security Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Kolicic, Alberto Caron, Chris Hicks, Vasilios Mavroudis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the critical need for interpretable and
uncertainty-aware machine learning models in the context of online learning for
high-risk industries, particularly cyber-security. While deep learning and
other complex models have demonstrated impressive predictive capabilities,
their opacity and lack of uncertainty quantification present significant
questions about their trustworthiness. We propose a novel pipeline for online
supervised learning problems in cyber-security, that harnesses the inherent
interpretability and uncertainty awareness of Additive Gaussian Processes
(AGPs) models. Our approach aims to balance predictive performance with
transparency while improving the scalability of AGPs, which represents their
main drawback, potentially enabling security analysts to better validate threat
detection, troubleshoot and reduce false positives, and generally make
trustworthy, informed decisions. This work contributes to the growing field of
interpretable AI by proposing a class of models that can be significantly
beneficial for high-stake decision problems such as the ones typical of the
cyber-security domain. The source code is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Unseen Domain Fake News Detection via Causal Propagation
  Substructures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of fake news on social media poses significant threats to
individuals and society. Text-based and graph-based models have been employed
for fake news detection by analysing news content and propagation networks,
showing promising results in specific scenarios. However, these data-driven
models heavily rely on pre-existing in-distribution data for training, limiting
their performance when confronted with fake news from emerging or previously
unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news
is a challenging yet critical task. In this paper, we introduce the Causal
Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to
enhance zero-shot fake news detection by extracting causal substructures from
propagation graphs using in-distribution data and generalising this approach to
OOD data. The model employs a graph neural network based mask generation
process to identify dominant nodes and edges within the propagation graph,
using these substructures for fake news detection. Additionally, the
performance of CSDA is further improved through contrastive learning in
few-shot scenarios, where a limited amount of OOD data is available for
training. Extensive experiments on public social media datasets demonstrate
that CSDA effectively handles OOD fake news detection, achieving a 7 to 16
percents accuracy improvement over other state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> of probabilistic <span class="highlight-title">generative</span> frameworks for molecular
  simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard John, Lukas Herron, Pratyush Tiwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence is now a widely used tool in molecular
science. Despite the popularity of probabilistic generative models, numerical
experiments benchmarking their performance on molecular data are lacking. In
this work, we introduce and explain several classes of generative models,
broadly sorted into two categories: flow-based models and diffusion models. We
select three representative models: Neural Spline Flows, Conditional Flow
Matching, and Denoising Diffusion Probabilistic Models, and examine their
accuracy, computational cost, and generation speed across datasets with tunable
dimensionality, complexity, and modal asymmetry. Our findings are varied, with
no one framework being the best for all purposes. In a nutshell, (i) Neural
Spline Flows do best at capturing mode asymmetry present in low-dimensional
data, (ii) Conditional Flow Matching outperforms other models for
high-dimensional data with low complexity, and (iii) Denoising Diffusion
Probabilistic Models appears the best for low-dimensional data with high
complexity. Our datasets include a Gaussian mixture model and the dihedral
torsion angle distribution of the Aib\textsubscript{9} peptide, generated via a
molecular dynamics simulation. We hope our taxonomy of probabilistic generative
frameworks and numerical results may guide model selection for a wide range of
molecular tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are nuclear masks all you need for improved out-of-domain
  generalisation? A closer look at cancer classification in histopathology <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhananjay Tomar, Alexander Binder, Andreas Kleppe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalisation in computational histopathology is challenging because
the images are substantially affected by differences among hospitals due to
factors like fixation and staining of tissue and imaging equipment. We
hypothesise that focusing on nuclei can improve the out-of-domain (OOD)
generalisation in cancer detection. We propose a simple approach to improve OOD
generalisation for cancer detection by focusing on nuclear morphology and
organisation, as these are domain-invariant features critical in cancer
detection. Our approach integrates original images with nuclear segmentation
masks during training, encouraging the model to prioritise nuclei and their
spatial arrangement. Going beyond mere data augmentation, we introduce a
regularisation technique that aligns the representations of masks and original
images. We show, using multiple datasets, that our method improves OOD
generalisation and also leads to increased robustness to image corruptions and
adversarial attacks. The source code is available at
https://github.com/undercutspiky/SFL/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability and Generalization for Distributed SGDA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaoxi Zhu, Yan Sun, Li Shen, Bo Du, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimax optimization is gaining increasing attention in modern machine
learning applications. Driven by large-scale models and massive volumes of data
collected from edge devices, as well as the concern to preserve client privacy,
communication-efficient distributed minimax optimization algorithms become
popular, such as Local Stochastic Gradient Descent Ascent (Local-SGDA), and
Local Decentralized SGDA (Local-DSGDA). While most existing research on
distributed minimax algorithms focuses on convergence rates, computation
complexity, and communication efficiency, the generalization performance
remains underdeveloped, whereas generalization ability is a pivotal indicator
for evaluating the holistic performance of a model when fed with unknown data.
In this paper, we propose the stability-based generalization analytical
framework for Distributed-SGDA, which unifies two popular distributed minimax
algorithms including Local-SGDA and Local-DSGDA, and conduct a comprehensive
analysis of stability error, generalization gap, and population risk across
different metrics under various settings, e.g., (S)C-(S)C, PL-SC, and NC-NC
cases. Our theoretical results reveal the trade-off between the generalization
gap and optimization error and suggest hyperparameters choice to obtain the
optimal population risk. Numerical experiments for Local-SGDA and Local-DSGDA
validate the theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-to-<span class="highlight-title">Event</span> <span class="highlight-title">Pretrain</span>ing for 3D Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zepeng Huo, Jason Alan Fries, Alejandro Lozano, Jeya Maria Jose Valanarasu, Ethan Steinberg, Louis Blankemeier, Akshay S. Chaudhari, Curtis Langlotz, Nigam H. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of medical foundation models and the growing availability of
imaging data, scalable pretraining techniques offer a promising way to identify
imaging biomarkers predictive of future disease risk. While current
self-supervised methods for 3D medical imaging models capture local structural
features like organ morphology, they fail to link pixel biomarkers with
long-term health outcomes due to a missing context problem. Current approaches
lack the temporal context necessary to identify biomarkers correlated with
disease progression, as they rely on supervision derived only from images and
concurrent text descriptions. To address this, we introduce time-to-event
pretraining, a pretraining framework for 3D medical imaging models that
leverages large-scale temporal supervision from paired, longitudinal electronic
health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D
images) and time-to-event distributions across thousands of EHR-derived tasks,
our method improves outcome prediction, achieving an average AUROC increase of
23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks.
Importantly, these gains are achieved without sacrificing diagnostic
classification performance. This study lays the foundation for integrating
longitudinal EHR and 3D imaging data to advance clinical risk prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximated Variational Bayesian Inverse Reinforcement Learning for
  <span class="highlight-title">Large Language Model</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Cai, Yuyu Yuan, Jinsheng Shi, Qinhong Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) is crucial for generating
helpful and harmless content. Existing approaches leverage preference-based
human feedback data to learn the reward function and align the LLM with the
feedback data. However, these approaches focus on modeling the reward
difference between the chosen and rejected demonstrations, rather than directly
modeling the true reward from each demonstration. Moreover, these approaches
assume that the reward is only obtained at the end of the sentence, which
overlooks the modeling of intermediate rewards. These issues lead to
insufficient use of training signals in the feedback data, limiting the
representation and generalization ability of the reward and potentially
resulting in reward hacking. In this paper, we formulate LLM alignment as a
Bayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel
training objective, Approximated Variational Alignment (AVA), to perform LLM
alignment through Approximated Variational Reward Imitation Learning (AVRIL).
The BIRL formulation facilitates intermediate reward modeling and direct reward
modeling on each single demonstration, which enhances the utilization of
training signals in the feedback data. Experiments show that AVA outperforms
existing LLM alignment approaches in reward modeling, RL fine-tuning, and
direct optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving hp-Variational Physics-Informed Neural Networks for
  Steady-State Convection-Dominated Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thivin Anandh, Divij Ghose, Himanshu Jain, Pratham Sunkad, Sashikumaar Ganesan, Volker John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes and studies two extensions of applying hp-variational
physics-informed neural networks, more precisely the FastVPINNs framework, to
convection-dominated convection-diffusion-reaction problems. First, a term in
the spirit of a SUPG stabilization is included in the loss functional and a
network architecture is proposed that predicts spatially varying stabilization
parameters. Having observed that the selection of the indicator function in
hard-constrained Dirichlet boundary conditions has a big impact on the accuracy
of the computed solutions, the second novelty is the proposal of a network
architecture that learns good parameters for a class of indicator functions.
Numerical studies show that both proposals lead to noticeably more accurate
results than approaches that can be found in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 11 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pie: Pooling CPU Memory for <span class="highlight-title">LLM</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Ziming Mao, Xiangxi Mo, Shu Liu, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of LLMs has revolutionized natural language processing and
AI analysis, but their increasing size and memory demands present significant
challenges. A common solution is to spill over to CPU memory; however,
traditional GPU-CPU memory swapping often results in higher latency and lower
throughput.
  This paper introduces Pie, an LLM inference framework that addresses these
challenges with performance-transparent swapping and adaptive expansion. By
leveraging predictable memory access patterns and the high bandwidth of modern
hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent
data swapping without affecting foreground computation, expanding effective
memory without added latency. Adaptive expansion dynamically adjusts CPU memory
allocation based on real-time information, optimizing memory usage and
performance under varying conditions.
  Pie maintains low computation latency, high throughput, and high elasticity.
Our experimental evaluation demonstrates that Pie achieves optimal swapping
policy during cache warmup and effectively balances increased memory capacity
with negligible impact on computation. With its extended capacity, Pie
outperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,
Pie can reduce GPU memory usage by up to 1.67X while maintaining the same
performance. Compared to FlexGen, an offline profiling-based swapping solution,
Pie achieves magnitudes lower latency and 9.4X higher throughput.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Probabilistic Inference forTime-Series Data A Robust Latent
  Gaussian Model With <span class="highlight-title">Temporal</span> Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Johansson, Arunselvan Ramaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of robust generative models for highly varied non-stationary
time series data is a complex yet important problem. Traditional models for
time series data prediction, such as Long Short-Term Memory (LSTM), are
inefficient and generalize poorly as they cannot capture complex temporal
relationships. In this paper, we present a probabilistic generative model that
can be trained to capture temporal information, and that is robust to data
errors. We call it Time Deep Latent Gaussian Model (tDLGM). Its novel
architecture is inspired by Deep Latent Gaussian Model (DLGM). Our model is
trained to minimize a loss function based on the negative log loss. One
contributing factor to Time Deep Latent Gaussian Model (tDLGM) robustness is
our regularizer, which accounts for data trends. Experiments conducted show
that tDLGM is able to reconstruct and generate complex time series data, and
that it is robust against to noise and faulty data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression Method for Solar Polarization Spectra Collected from Hinode
  SOT/SP Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jargalmaa Batmunkh, Yusuke Iida, Takayoshi Oba, Haruhisa Iijima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complex structure and extensive details of solar spectral data, combined
with a recent surge in volume, present significant processing challenges. To
address this, we propose a deep learning-based compression technique using deep
autoencoder (DAE) and 1D-convolutional autoencoder (CAE) models developed with
Hinode SOT/SP data. We focused on compressing Stokes I and V polarization
spectra from the quiet Sun, as well as from active regions, providing a novel
insight into comprehensive spectral analysis by incorporating spectra from
extreme magnetic fields. The results indicate that the CAE model outperforms
the DAE model in reconstructing Stokes profiles, demonstrating greater
robustness and achieving reconstruction errors around the observational noise
level. The proposed method has proven effective in compressing Stokes I and V
spectra from both the quiet Sun and active regions, highlighting its potential
for impactful applications in solar spectral analysis, such as detection of
unusual spectral signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing generalization in high energy physics using white-box
  adversarial attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franck Rothen, Samuel Klein, Matthew Leigh, Tobias Golling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is becoming increasingly popular in the context of particle
physics. Supervised learning, which uses labeled Monte Carlo (MC) simulations,
remains one of the most widely used methods for discriminating signals beyond
the Standard Model. However, this paper suggests that supervised models may
depend excessively on artifacts and approximations from Monte Carlo
simulations, potentially limiting their ability to generalize well to real
data. This study aims to enhance the generalization properties of supervised
models by reducing the sharpness of local minima. It reviews the application of
four distinct white-box adversarial attacks in the context of classifying Higgs
boson decay signals. The attacks are divided into weight space attacks, and
feature space attacks. To study and quantify the sharpness of different local
minima this paper presents two analysis methods: gradient ascent and reduced
Hessian eigenvalue analysis. The results show that white-box adversarial
attacks significantly improve generalization performance, albeit with increased
computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 8 tables, 3 algorithms, to be published in
  Physical Review D (PRD), presented at the ML4Jets 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Centralized-Distributed Transfer Model for Cross-Domain Recommendation
  Based on Multi-Source Heterogeneous Transfer Learning <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Xu, Ziliang Wang, Wei Zheng, Yuhao Ma, Chenglin Wang, Nengxue Jiang, Cai Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation (CDR) methods are proposed to tackle the sparsity
problem in click through rate (CTR) estimation. Existing CDR methods directly
transfer knowledge from the source domains to the target domain and ignore the
heterogeneities among domains, including feature dimensional heterogeneity and
latent space heterogeneity, which may lead to negative transfer. Besides, most
of the existing methods are based on single-source transfer, which cannot
simultaneously utilize knowledge from multiple source domains to further
improve the model performance in the target domain. In this paper, we propose a
centralized-distributed transfer model (CDTM) for CDR based on multi-source
heterogeneous transfer learning. To address the issue of feature dimension
heterogeneity, we build a dual embedding structure: domain specific embedding
(DSE) and global shared embedding (GSE) to model the feature representation in
the single domain and the commonalities in the global space,separately. To
solve the latent space heterogeneity, the transfer matrix and attention
mechanism are used to map and combine DSE and GSE adaptively. Extensive offline
and online experiments demonstrate the effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: 2022 IEEE International Conference on Data Mining
  (ICDM) (The authors were affiliated Hangzhou NetEase Cloud Music Technology
  Co., Ltd.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards efficient compression and communication for prototype-based
  decentralized learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Fernández-Piñeiro, Manuel Ferández-Veiga, Rebeca P. Díaz-Redondo, Ana Fernández-Vilas, Martín González-Soto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In prototype-based federated learning, the exchange of model parameters
between clients and the master server is replaced by transmission of prototypes
or quantized versions of the data samples to the aggregation server. A fully
decentralized deployment of prototype- based learning, without a central
agregartor of prototypes, is more robust upon network failures and reacts
faster to changes in the statistical distribution of the data, suggesting
potential advantages and quick adaptation in dynamic learning tasks, e.g., when
the data sources are IoT devices or when data is non-iid. In this paper, we
consider the problem of designing a communication-efficient decentralized
learning system based on prototypes. We address the challenge of prototype
redundancy by leveraging on a twofold data compression technique, i.e., sending
only update messages if the prototypes are informationtheoretically useful (via
the Jensen-Shannon distance), and using clustering on the prototypes to
compress the update messages used in the gossip protocol. We also use parallel
instead of sequential gossiping, and present an analysis of its
age-of-information (AoI). Our experimental results show that, with these
improvements, the communications load can be substantially reduced without
decreasing the convergence rate of the learning algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 tables, 7 figures, 6 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Good is Chat<span class="highlight-title">GPT</span> at Audiovisual Deepfake Detection: A Comparative
  Study of Chat<span class="highlight-title">GPT</span>, AI Models and Human Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deepfakes involving audiovisual manipulations are a growing threat
because they are difficult to detect with the naked eye or using unimodal deep
learningbased forgery detection methods. Audiovisual forensic models, while
more capable than unimodal models, require large training datasets and are
computationally expensive for training and inference. Furthermore, these models
lack interpretability and often do not generalize well to unseen manipulations.
In this study, we examine the detection capabilities of a large language model
(LLM) (i.e., ChatGPT) to identify and account for any possible visual and
auditory artifacts and manipulations in audiovisual deepfake content. Extensive
experiments are conducted on videos from a benchmark multimodal deepfake
dataset to evaluate the detection performance of ChatGPT and compare it with
the detection capabilities of state-of-the-art multimodal forensic models and
humans. Experimental results demonstrate the importance of domain knowledge and
prompt engineering for video forgery detection tasks using LLMs. Unlike
approaches based on end-to-end learning, ChatGPT can account for spatial and
spatiotemporal artifacts and inconsistencies that may exist within or across
modalities. Additionally, we discuss the limitations of ChatGPT for multimedia
forensic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weight-Averaged Model-merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Wang, Congbo Ma, Ibrahim Almakky, Ian Reid, Gustavo Carneiro, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight-averaged model-merging has emerged as a powerful approach in deep
learning, capable of enhancing model performance without fine-tuning or
retraining. However, the underlying mechanisms that explain its effectiveness
remain largely unexplored. In this paper, we investigate this technique from
three novel perspectives to provide deeper insights into how and why
weight-averaged model-merging works: (1) we examine the intrinsic patterns
captured by the learning of the model weights, through the visualizations of
their patterns on several datasets, showing that these weights often encode
structured and interpretable patterns; (2) we investigate model ensemble
merging strategies based on averaging on weights versus averaging on features,
providing detailed analyses across diverse architectures and datasets; and (3)
we explore the impact on model-merging prediction stability in terms of
changing the parameter magnitude, revealing insights into the way of weight
averaging works as regularization by showing the robustness across different
parameter scales. Our findings shed light on the "black box" of weight-averaged
model-merging, offering valuable insights and practical recommendations that
advance the model-merging process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FluidML: Fast and Memory Efficient Inference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Liu, Hang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models deployed on edge devices have enabled numerous
exciting new applications, such as humanoid robots, AR glasses, and autonomous
vehicles. However, the computing resources available on these edge devices are
not catching up with the ever-growing number of parameters in these models. As
the models become bigger and more complicated, the novel yet sophisticated
structure challenges the inference runtime optimization. We present FluidML, a
generic runtime memory management and optimization framework that can flexibly
transform the model execution blueprint to achieve faster and more
memory-efficient inference. Evaluations across different platforms show that
FluidML can consistently reduce the end-to-end inference latency by up to
25.38% for popular language models and reduce peak memory usage by up to
41.47%, compared to state-of-the-art approaches. FluidML is of ~30K line of
codes, built for general-purpose usage, and will be released as an open-source
inference runtime optimization framework to the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the "Heatmap + Monte Carlo Tree Search" Paradigm for Solving
  Large Scale TSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhao Pan, Chenguang Wang, Chaolong Ying, Ye Xue, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Travelling Salesman Problem (TSP) remains a fundamental challenge in
combinatorial optimization, inspiring diverse algorithmic strategies. This
paper revisits the "heatmap + Monte Carlo Tree Search (MCTS)" paradigm that has
recently gained traction for learning-based TSP solutions. Within this
framework, heatmaps encode the likelihood of edges forming part of the optimal
tour, and MCTS refines this probabilistic guidance to discover optimal
solutions. Contemporary approaches have predominantly emphasized the refinement
of heatmap generation through sophisticated learning models, inadvertently
sidelining the critical role of MCTS. Our extensive empirical analysis reveals
two pivotal insights: 1) The configuration of MCTS strategies profoundly
influences the solution quality, demanding meticulous tuning to leverage their
full potential; 2) Our findings demonstrate that a rudimentary and
parameter-free heatmap, derived from the intrinsic $k$-nearest nature of TSP,
can rival or even surpass the performance of complicated heatmaps, with strong
generalizability across various scales. Empirical evaluations across various
TSP scales underscore the efficacy of our approach, achieving competitive
results. These observations challenge the prevailing focus on heatmap
sophistication, advocating a reevaluation of the paradigm to harness both
components synergistically. Our code is available at:
https://github.com/LOGO-CUHKSZ/rethink_mcts_tsp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classical Verification of Quantum Learning Advantages with Noises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Ma, Jiaxi Su, Dong-Ling Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical verification of quantum learning allows classical clients to
reliably leverage quantum computing advantages by interacting with untrusted
quantum servers. Yet, current quantum devices available in practice suffers
from a variety of noises and whether existed classical verification protocols
carry over to noisy scenarios remains unclear. Here, we propose an efficient
classical error rectification algorithm to reconstruct the noise-free results
given by the quantum Fourier sampling circuit with practical constant-level
noises. In particular, we prove that the error rectification algorithm can
restore the heavy Fourier coefficients by using a small number of noisy samples
that scales logarithmically with the problem size. We apply this algorithm to
the agnostic parity learning task with uniform input marginal and prove that
this task can be accomplished in an efficient way on noisy quantum devices with
our algorithm. In addition, we prove that a classical client with access to the
random example oracle can verify the agnostic parity learning results from the
noisy quantum prover in an efficient way, under the condition that the Fourier
coefficients are sparse. Our results demonstrate the feasibility of classical
verification of quantum learning advantages with noises, which provide a
valuable guide for both theoretical studies and practical applications with
current noisy intermediate scale quantum devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ghost-Connect Net: A Generalization-Enhanced Guidance For Sparse Deep
  Networks Under Distribution Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Isabelle Wisell, Salimeh Yasaei Sekeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse deep neural networks (DNNs) excel in real-world applications like
robotics and computer vision, by reducing computational demands that hinder
usability. However, recent studies aim to boost DNN efficiency by trimming
redundant neurons or filters based on task relevance, but neglect their
adaptability to distribution shifts. We aim to enhance these existing
techniques by introducing a companion network, Ghost Connect-Net (GC-Net), to
monitor the connections in the original network with distribution
generalization advantage. GC-Net's weights represent connectivity measurements
between consecutive layers of the original network. After pruning GC-Net, the
pruned locations are mapped back to the original network as pruned connections,
allowing for the combination of magnitude and connectivity-based pruning
methods. Experimental results using common DNN benchmarks, such as CIFAR-10,
Fashion MNIST, and Tiny ImageNet show promising results for hybridizing the
method, and using GC-Net guidance for later layers of a network and direct
pruning on earlier layers. We provide theoretical foundations for GC-Net's
approach to improving generalization under distribution shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 3 subfigures, 42 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic technology impact analysis: A multi-task learning approach to
  patent citation prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjin Seol, Jaewoong Choi, Seunghyun Lee, Janghyeok Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models are valuable tools for analyzing the impact of
technology using patent citation information. However, existing ML-based
methods often struggle to account for the dynamic nature of the technology
impact over time and the interdependencies of these impacts across different
periods. This study proposes a multi-task learning (MTL) approach to enhance
the prediction of technology impact across various time frames by leveraging
knowledge sharing and simultaneously monitoring the evolution of technology
impact. First, we quantify the technology impacts and identify patterns through
citation analysis over distinct time periods. Next, we develop MTL models to
predict citation counts using multiple patent indicators over time. Finally, we
examine the changes in key input indicators and their patterns over different
periods using the SHapley Additive exPlanation method. We also offer guidelines
for validating and interpreting the results by employing statistical methods
and natural language processing techniques. A case study on battery
technologies demonstrates that our approach not only deepens the understanding
of technology impact, but also improves prediction accuracy, yielding valuable
insights for both academia and industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeBaTeR: Denoising Bipartite <span class="highlight-title">Temporal</span> Graph for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu He, Jose Sepulveda, Mostafa Rahmani, Alyssa Woo, Fei Wang, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the difficulty of acquiring large-scale explicit user feedback,
implicit feedback (e.g., clicks or other interactions) is widely applied as an
alternative source of data, where user-item interactions can be modeled as a
bipartite graph. Due to the noisy and biased nature of implicit real-world
user-item interactions, identifying and rectifying noisy interactions are vital
to enhance model performance and robustness. Previous works on purifying
user-item interactions in collaborative filtering mainly focus on mining the
correlation between user/item embeddings and noisy interactions, neglecting the
benefit of temporal patterns in determining noisy interactions. Time
information, while enhancing the model utility, also bears its natural
advantage in helping to determine noisy edges, e.g., if someone usually watches
horror movies at night and talk shows in the morning, a record of watching a
horror movie in the morning is more likely to be noisy interaction. Armed with
this observation, we introduce a simple yet effective mechanism for generating
time-aware user/item embeddings and propose two strategies for denoising
bipartite temporal graph in recommender systems (DeBaTeR): the first is through
reweighting the adjacency matrix (DeBaTeR-A), where a reliability score is
defined to reweight the edges through both soft assignment and hard assignment;
the second is through reweighting the loss function (DeBaTeR-L), where weights
are generated to reweight user-item samples in the losses. Extensive
experiments have been conducted to demonstrate the efficacy of our methods and
illustrate how time information indeed helps identifying noisy edges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for
  Responsible AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Giddens, Fang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data-driven and AI-based decision making gains widespread adoption in most
disciplines, it is crucial that both data privacy and decision fairness are
appropriately addressed. While differential privacy (DP) provides a robust
framework for guaranteeing privacy and several widely accepted methods have
been proposed for improving fairness, the vast majority of existing literature
treats the two concerns independently. For methods that do consider privacy and
fairness simultaneously, they often only apply to a specific machine learning
task, limiting their generalizability. In response, we introduce SAFES, a
Sequential PrivAcy and Fairness Enhancing data Synthesis procedure that
sequentially combines DP data synthesis with a fairness-aware data
transformation. SAFES allows full control over the privacy-fairness-utility
trade-off via tunable privacy and fairness parameters. We illustrate SAFES by
combining AIM, a graphical model-based DP data synthesizer, with a popular
fairness-aware data pre-processing transformation. Empirical evaluations on the
Adult and COMPAS datasets demonstrate that for reasonable privacy loss,
SAFES-generated synthetic data achieve significantly improved fairness metrics
with relatively low utility loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid deep additive neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyu Min Kim, Jeong Min Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional neural networks (multi-layer perceptrons) have become an
important tool in data science due to their success across a wide range of
tasks. However, their performance is sometimes unsatisfactory, and they often
require a large number of parameters, primarily due to their reliance on the
linear combination structure. Meanwhile, additive regression has been a popular
alternative to linear regression in statistics. In this work, we introduce
novel deep neural networks that incorporate the idea of additive regression.
Our neural networks share architectural similarities with Kolmogorov-Arnold
networks but are based on simpler yet flexible activation and basis functions.
Additionally, we introduce several hybrid neural networks that combine this
architecture with that of traditional neural networks. We derive their
universal approximation properties and demonstrate their effectiveness through
simulation studies and a real-data application. The numerical results indicate
that our neural networks generally achieve better performance than traditional
neural networks while using fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing <span class="highlight-title">Diffusion</span> Models: Alias-Free Resampling and Enhanced
  Rotational Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Fahim Anjum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image generation, particularly via diffusion models, have
led to impressive improvements in image synthesis quality. Despite this,
diffusion models are still challenged by model-induced artifacts and limited
stability in image fidelity. In this work, we hypothesize that the primary
cause of this issue is the improper resampling operation that introduces
aliasing in the diffusion model and a careful alias-free resampling dictated by
image processing theory can improve the model's performance in image synthesis.
We propose the integration of alias-free resampling layers into the UNet
architecture of diffusion models without adding extra trainable parameters,
thereby maintaining computational efficiency. We then assess whether these
theory-driven modifications enhance image quality and rotational equivariance.
Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and
MNIST-M, reveal consistent gains in image quality, particularly in terms of FID
and KID scores. Furthermore, we propose a modified diffusion process that
enables user-controlled rotation of generated images without requiring
additional training. Our findings highlight the potential of theory-driven
enhancements such as alias-free resampling in generative models to improve
image quality while maintaining model efficiency and pioneer future research
directions to incorporate them into video-generating diffusion models, enabling
deeper exploration of the applications of alias-free resampling in generative
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Scalable Handwriting Communication via EEG Decoding and Latent
  Embedding Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Young Kim, Deok-Seon Kim, Seo-Hyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, brain-computer interfaces have made advances in decoding
various motor-related tasks, including gesture recognition and movement
classification, utilizing electroencephalogram (EEG) data. These developments
are fundamental in exploring how neural signals can be interpreted to recognize
specific physical actions. This study centers on a written alphabet
classification task, where we aim to decode EEG signals associated with
handwriting. To achieve this, we incorporate hand kinematics to guide the
extraction of the consistent embeddings from high-dimensional neural recordings
using auxiliary variables (CEBRA). These CEBRA embeddings, along with the EEG,
are processed by a parallel convolutional neural network model that extracts
features from both data sources simultaneously. The model classifies nine
different handwritten characters, including symbols such as exclamation marks
and commas, within the alphabet. We evaluate the model using a quantitative
five-fold cross-validation approach and explore the structure of the embedding
space through visualizations. Our approach achieves a classification accuracy
of 91 % for the nine-class task, demonstrating the feasibility of fine-grained
handwriting decoding from EEG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 1 table, Name of Conference: International
  Conference on Brain-Computer Interface</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationality based Innate-Values-driven Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innate values describe agents' intrinsic motivations, which reflect their
inherent interests and preferences to pursue goals and drive them to develop
diverse skills satisfying their various needs. The essence of reinforcement
learning (RL) is learning from interaction based on reward-driven behaviors,
much like natural agents. It is an excellent model to describe the
innate-values-driven (IV) behaviors of AI agents. Especially developing the
awareness of the AI agent through balancing internal and external utilities
based on its needs in different tasks is a crucial problem for individuals
learning to support AI agents integrating human society with safety and harmony
in the long term. This paper proposes a hierarchical compound intrinsic value
reinforcement learning model -- innate-values-driven reinforcement learning
termed IVRL to describe the complex behaviors of AI agents' interaction. We
formulated the IVRL model and proposed two IVRL models: DQN and A2C. By
comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the
Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we
demonstrated that rationally organizing various individual needs can
effectively achieve better performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2401.05572</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAINRec: Graph and Attention Integrated Approach for Real-Time
  Session-Based Item Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavtosh Rath, Pushkar Chennu, David Relyea, Prathyusha Kanmanth Reddy, Amit Pande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in session-based recommendation models using deep
learning techniques have demonstrated significant performance improvements.
While they can enhance model sophistication and improve the relevance of
recommendations, they also make it challenging to implement a scalable
real-time solution. To addressing this challenge, we propose GRAINRec- a Graph
and Attention Integrated session-based recommendation model that generates
recommendations in real-time. Our scope of work is item recommendations in
online retail where a session is defined as an ordered sequence of digital
guest actions, such as page views or adds to cart. The proposed model generates
recommendations by considering the importance of all items in the session
together, letting us predict relevant recommendations dynamically as the
session evolves. We also propose a heuristic approach to implement real-time
inferencing that meets Target platform's service level agreement (SLA). The
proposed architecture lets us predict relevant recommendations dynamically as
the session evolves, rather than relying on pre-computed recommendations for
each item. Evaluation results of the proposed model show an average improvement
of 1.5% across all offline evaluation metrics. A/B tests done over a 2 week
duration showed an increase of 10% in click through rate and 9% increase in
attributable demand. Extensive ablation studies are also done to understand our
model performance for different parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 IEEE International Conference on Big Data (IEEE
  BigData 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laplace Transform Interpretation of Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishav Chourasia, Uzair Javaid, Biplap Sikdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a set of useful expressions of Differential Privacy (DP) notions
in terms of the Laplace transform of the privacy loss distribution. Its bare
form expression appears in several related works on analyzing DP, either as an
integral or an expectation. We show that recognizing the expression as a
Laplace transform unlocks a new way to reason about DP properties by exploiting
the duality between time and frequency domains. Leveraging our interpretation,
we connect the $(q, \rho(q))$-R\'enyi DP curve and the $(\epsilon,
\delta(\epsilon))$-DP curve as being the Laplace and inverse-Laplace transforms
of one another. This connection shows that the R\'enyi divergence is
well-defined for complex orders $q = \gamma + i \omega$. Using our Laplace
transform-based analysis, we also prove an adaptive composition theorem for
$(\epsilon, \delta)$-DP guarantees that is exactly tight (i.e., matches even in
constants) for all values of $\epsilon$. Additionally, we resolve an issue
regarding symmetry of $f$-DP on subsampling that prevented equivalence across
all functional DP notions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity-Aware Training of Deep Neural Networks for Optimal Structure
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Frank Ingmar Guenter, Athanasios Sideris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel algorithm for combined unit/filter and layer pruning of
deep neural networks that functions during training and without requiring a
pre-trained network to apply. Our algorithm optimally trades-off learning
accuracy and pruning levels while balancing layer vs. unit/filter pruning and
computational vs. parameter complexity using only three user-defined
parameters, which are easy to interpret and tune. The optimal network structure
is found as the solution of a stochastic optimization problem over the network
weights and the parameters of variational Bernoulli distributions for 0/1
Random Variables scaling the units and layers of the network. Pruning occurs
when a variational parameter converges to 0 rendering the corresponding
structure permanently inactive, thus saving computations during training and
prediction. A key contribution of our approach is to define a cost function
that combines the objectives of prediction accuracy and network pruning in a
computational/parameter complexity-aware manner and the automatic selection of
the many regularization parameters. We show that the solutions of the
optimization problem to which the algorithm converges are deterministic
networks. We analyze the ODE system that underlies our stochastic optimization
algorithm and establish domains of attraction around zero for the dynamics of
the network parameters. These results provide theoretical support for safely
pruning units/filters and/or layers during training and lead to practical
pruning conditions. We evaluate our method on the CIFAR-10/100 and ImageNet
datasets using ResNet architectures and demonstrate that our method improves
upon layer only or unit only pruning and favorably competes with combined
unit/filter and layer pruning algorithms requiring pre-trained networks with
respect to pruning ratios and test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Graph Simulator for Complex Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyun Choi, Sungyeop Lee, B. Kahng, Junghyo Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical simulation is a predominant tool for studying the dynamics in
complex systems, but large-scale simulations are often intractable due to
computational limitations. Here, we introduce the Neural Graph Simulator (NGS)
for simulating time-invariant autonomous systems on graphs. Utilizing a graph
neural network, the NGS provides a unified framework to simulate diverse
dynamical systems with varying topologies and sizes without constraints on
evaluation times through its non-uniform time step and autoregressive approach.
The NGS offers significant advantages over numerical solvers by not requiring
prior knowledge of governing equations and effectively handling noisy or
missing data with a robust training scheme. It demonstrates superior
computational efficiency over conventional methods, improving performance by
over $10^5$ times in stiff problems. Furthermore, it is applied to real traffic
data, forecasting traffic flow with state-of-the-art accuracy. The versatility
of the NGS extends beyond the presented cases, offering numerous potential
avenues for enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FxTS-Net: Fixed-Time Stable Learning Framework for Neural ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyang Luo, Yan Zou, Wanying Li, Nanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Ordinary Differential Equations (Neural ODEs), as a novel category of
modeling big data methods, cleverly link traditional neural networks and
dynamical systems. However, it is challenging to ensure the dynamics system
reaches a correctly predicted state within a user-defined fixed time. To
address this problem, we propose a new method for training Neural ODEs using
fixed-time stability (FxTS) Lyapunov conditions. Our framework, called
FxTS-Net, is based on the novel FxTS loss (FxTS-Loss) designed on Lyapunov
functions, which aims to encourage convergence to accurate predictions in a
user-defined fixed time. We also provide an innovative approach for
constructing Lyapunov functions to meet various tasks and network architecture
requirements, achieved by leveraging supervised information during training. By
developing a more precise time upper bound estimation for bounded
non-vanishingly perturbed systems, we demonstrate that minimizing FxTS-Loss not
only guarantees FxTS behavior of the dynamics but also input perturbation
robustness. For optimising FxTS-Loss, we also propose a learning algorithm, in
which the simulated perturbation sampling method can capture sample points in
critical regions to approximate FxTS-Loss. Experimentally, we find that
FxTS-Net provides better prediction performance and better robustness under
input perturbation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently learning and sampling multimodal distributions with
  data-based initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Koehler, Holden Lee, Thuy-Duong Vuong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of sampling a multimodal distribution with a Markov
chain given a small number of samples from the stationary measure. Although
mixing can be arbitrarily slow, we show that if the Markov chain has a $k$th
order spectral gap, initialization from a set of $\tilde O(k/\varepsilon^2)$
samples from the stationary distribution will, with high probability over the
samples, efficiently generate a sample whose conditional law is
$\varepsilon$-close in TV distance to the stationary measure. In particular,
this applies to mixtures of $k$ distributions satisfying a Poincar\'e
inequality, with faster convergence when they satisfy a log-Sobolev inequality.
Our bounds are stable to perturbations to the Markov chain, and in particular
work for Langevin diffusion over $\mathbb R^d$ with score estimation error, as
well as Glauber dynamics combined with approximation error from
pseudolikelihood estimation. This justifies the success of data-based
initialization for score matching methods despite slow mixing for the data
distribution, and improves and generalizes the results of Koehler and Vuong
(2023) to have linear, rather than exponential, dependence on $k$ and apply to
arbitrary semigroups. As a consequence of our results, we show for the first
time that a natural class of low-complexity Ising measures can be efficiently
learned from samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Reasoning Costs - The Path of Optimization for Chain of <span class="highlight-title">Thought</span>
  via Sparse Attention Mechanism <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to address the chain of thought in the large language model
inference cost surge, this research proposes to use a sparse attention
mechanism that only focuses on a few relevant tokens. The researcher
constructed a new attention mechanism and used GiantRabbit trained with custom
GPTs as an experimental tool. The experiment tested and compared the reasoning
time, correctness score and chain of thought length of this model and o1
Preview in solving the linear algebra test questions of MIT OpenCourseWare. The
results show that GiantRabbit's reasoning time and chain of thought length are
significantly lower than o1 Preview, confirming the feasibility of the sparse
attention mechanism in reducing chain of thought reasoning. Detailed
architectural details and experimental process have been uploaded to Github,
the link is:https://github.com/brucewang123456789/GeniusTrail.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The main text is 9 pages, totaling 13 pages; 5 figures, 3 tables;
  preprints have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralDEM -- Real-time Simulation of Industrial Particulate Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Alkin, Tobias Kronlachner, Samuele Papa, Stefan Pirker, Thomas Lichtenegger, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in computing power have made it possible to numerically simulate
large-scale fluid-mechanical and/or particulate systems, many of which are
integral to core industrial processes. Among the different numerical methods
available, the discrete element method (DEM) provides one of the most accurate
representations of a wide range of physical systems involving granular and
discontinuous materials. Consequently, DEM has become a widely accepted
approach for tackling engineering problems connected to granular flows and
powder mechanics. Additionally, DEM can be integrated with grid-based
computational fluid dynamics (CFD) methods, enabling the simulation of chemical
processes taking place, e.g., in fluidized beds. However, DEM is
computationally intensive because of the intrinsic multiscale nature of
particulate systems, restricting simulation duration or number of particles.
Towards this end, NeuralDEM presents an end-to-end approach to replace slow
numerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEM
is capable of picturing long-term transport processes across different regimes
using macroscopic observables without any reference to microscopic model
parameters. First, NeuralDEM treats the Lagrangian discretization of DEM as an
underlying continuous field, while simultaneously modeling macroscopic behavior
directly as additional auxiliary fields. Second, NeuralDEM introduces
multi-branch neural operators scalable to real-time modeling of
industrially-sized scenarios - from slow and pseudo-steady to fast and
transient. Such scenarios have previously posed insurmountable challenges for
deep learning models. Notably, NeuralDEM faithfully models coupled CFD-DEM
fluidized bed reactors of 160k CFD cells and 500k DEM particles for
trajectories of 28s. NeuralDEM will open many new doors to advanced engineering
and much faster process cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://nx-ai.github.io/NeuralDEM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards efficient compression and communication for prototype-based
  decentralized learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Fernández-Piñeiro, Manuel Ferández-Veiga, Rebeca P. Díaz-Redondo, Ana Fernández-Vilas, Martín González-Soto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In prototype-based federated learning, the exchange of model parameters
between clients and the master server is replaced by transmission of prototypes
or quantized versions of the data samples to the aggregation server. A fully
decentralized deployment of prototype-based learning, without a central
agregartor of prototypes, is more robust upon network failures and reacts
faster to changes in the statistical distribution of the data, suggesting
potential advantages and quick adaptation in dynamic learning tasks, e.g., when
the data sources are IoT devices or when data is non-iid. In this paper, we
consider the problem of designing a communication-efficient decentralized
learning system based on prototypes. We address the challenge of prototype
redundancy by leveraging on a twofold data compression technique, i.e., sending
only update messages if the prototypes are informationtheoretically useful (via
the Jensen-Shannon distance), and using clustering on the prototypes to
compress the update messages used in the gossip protocol. We also use parallel
instead of sequential gossiping, and present an analysis of its
age-of-information (AoI). Our experimental results show that, with these
improvements, the communications load can be substantially reduced without
decreasing the convergence rate of the learning algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 tables, 7 figures, 6 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAINRec: Graph and Attention Integrated Approach for Real-Time
  Session-Based Item Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavtosh Rath, Pushkar Chennu, David Relyea, Prathyusha Kanmanth Reddy, Amit Pande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in session-based recommendation models using deep
learning techniques have demonstrated significant performance improvements.
While they can enhance model sophistication and improve the relevance of
recommendations, they also make it challenging to implement a scalable
real-time solution. To addressing this challenge, we propose GRAINRec: a Graph
and Attention Integrated session-based recommendation model that generates
recommendations in real-time. Our scope of work is item recommendations in
online retail where a session is defined as an ordered sequence of digital
guest actions, such as page views or adds to cart. The proposed model generates
recommendations by considering the importance of all items in the session
together, letting us predict relevant recommendations dynamically as the
session evolves. We also propose a heuristic approach to implement real-time
inferencing that meets Target platform's service level agreement (SLA). The
proposed architecture lets us predict relevant recommendations dynamically as
the session evolves, rather than relying on pre-computed recommendations for
each item. Evaluation results of the proposed model show an average improvement
of 1.5% across all offline evaluation metrics. A/B tests done over a 2 week
duration showed an increase of 10% in click through rate and 9% increase in
attributable demand. Extensive ablation studies are also done to understand our
model performance for different parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 IEEE International Conference on Big Data (IEEE
  BigData 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Reasoning Costs -- The Path of Optimization for Chain of
  <span class="highlight-title">Thought</span> via Sparse Attention Mechanism <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to address the chain of thought in the large language model
inference cost surge, this research proposes to use a sparse attention
mechanism that only focuses on a few relevant tokens. The researcher
constructed a new attention mechanism and used GiantRabbit trained with custom
GPTs as an experimental tool. The experiment tested and compared the reasoning
time, correctness score and chain of thought length of this model and o1
Preview in solving the linear algebra test questions of MIT OpenCourseWare. The
results show that GiantRabbit's reasoning time and chain of thought length are
significantly lower than o1 Preview, confirming the feasibility of the sparse
attention mechanism in reducing chain of thought reasoning. Detailed
architectural details and experimental process have been uploaded to Github,
the link is:https://github.com/brucewang123456789/GeniusTrail.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The main text is 9 pages, totaling 13 pages; 5 figures, 3 tables;
  preprints have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Maritime Trajectory <span class="highlight-title">Forecast</span>ing via H3 Index and Causal
  <span class="highlight-title">Language Model</span>ling (CLM) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Drapier, Aladine Chetouani, Aurélien Chateigner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction of ship trajectories is a growing field of study in artificial
intelligence. Traditional methods rely on the use of LSTM, GRU networks, and
even Transformer architectures for the prediction of spatio-temporal series.
This study proposes a viable alternative for predicting these trajectories
using only GNSS positions. It considers this spatio-temporal problem as a
natural language processing problem. The latitude/longitude coordinates of AIS
messages are transformed into cell identifiers using the H3 index. Thanks to
the pseudo-octal representation, it becomes easier for language models to learn
the spatial hierarchy of the H3 index. The method is compared with a classical
Kalman filter, widely used in the maritime domain, and introduces the Fr\'echet
distance as the main evaluation metric. We show that it is possible to predict
ship trajectories quite precisely up to 8 hours ahead with 30 minutes of
context, using solely GNSS positions, without relying on any additional
information such as speed, course, or external conditions - unlike many
traditional methods. We demonstrate that this alternative works well enough to
predict trajectories worldwide.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoDefense: Multi-<span class="highlight-title">Agent</span> <span class="highlight-title">LLM</span> Defense against Jailbreak Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive pre-training in moral alignment to prevent generating
harmful information, large language models (LLMs) remain vulnerable to
jailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense
framework that filters harmful responses from LLMs. With the response-filtering
mechanism, our framework is robust against different jailbreak attack prompts,
and can be used to defend different victim models. AutoDefense assigns
different roles to LLM agents and employs them to complete the defense task
collaboratively. The division in tasks enhances the overall
instruction-following of LLMs and enables the integration of other defense
components as tools. With AutoDefense, small open-source LMs can serve as
agents and defend larger models against jailbreak attacks. Our experiments show
that AutoDefense can effectively defense against different jailbreak attacks,
while maintaining the performance at normal user request. For example, we
reduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using
LLaMA-2-13b with a 3-agent system. Our code and data are publicly available at
https://github.com/XHMY/AutoDefense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-<span class="highlight-title">Agent</span> Loco-Manipulation for Long-Horizon Quadrupedal
  Pushing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, quadrupedal locomotion has achieved significant success, but their
manipulation capabilities, particularly in handling large objects, remain
limited, restricting their usefulness in demanding real-world applications such
as search and rescue, construction, industrial automation, and room
organization. This paper tackles the task of obstacle-aware, long-horizon
pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent
reinforcement learning framework with three levels of control. The high-level
controller integrates an RRT planner and a centralized adaptive policy to
generate subgoals, while the mid-level controller uses a decentralized
goal-conditioned policy to guide the robots toward these sub-goals. A
pre-trained low-level locomotion policy executes the movement commands. We
evaluate our method against several baselines in simulation, demonstrating
significant improvements over baseline approaches, with 36.0% higher success
rates and 24.5% reduction in completion time than the best baseline. Our
framework successfully enables long-horizon, obstacle-aware manipulation tasks
like Push-Cuboid and Push-T on Go1 robots in the real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Consistency Tuning: Understanding and Improving Consistency
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yun Wang, Zhengyang Geng, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models achieve superior generation quality but suffer from slow
generation speed due to the iterative nature of denoising. In contrast,
consistency models, a new generative family, achieve competitive performance
with significantly faster sampling. These models are trained either through
consistency distillation, which leverages pretrained diffusion models, or
consistency training/tuning directly from raw data. In this work, we propose a
novel framework for understanding consistency models by modeling the denoising
process of the diffusion model as a Markov Decision Process (MDP) and framing
consistency model training as the value estimation through Temporal
Difference~(TD) Learning. More importantly, this framework allows us to analyze
the limitations of current consistency training/tuning strategies. Built upon
Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),
which incorporates variance-reduced learning using the score identity. SCT
leads to significant performance improvements on benchmarks such as CIFAR-10
and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID
1.55, a new SoTA for consistency models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/G-U-N/Stable-Consistency-Tuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Imitation to Refinement -- Residual RL for Precise Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in behavior cloning (BC), like action-chunking and diffusion, have
enabled impressive capabilities. Still, imitation alone remains insufficient
for learning reliable policies for tasks requiring precise aligning and
inserting of objects, like assembly. Our key insight is that chunked BC
policies effectively function as trajectory planners, enabling long-horizon
tasks. Conversely, as they execute action chunks open-loop, they lack the
fine-grained reactivity necessary for reliable execution. Further, we find that
the performance of BC policies saturates despite increasing data. Reinforcement
learning (RL) is a natural way to overcome BC's limitations, but it is not
straightforward to apply directly to action-chunked models like diffusion
policies. We present a simple yet effective method, ResiP (Residual for Precise
Manipulation), that sidesteps these challenges by augmenting a frozen, chunked
BC model with a fully closed-loop residual policy trained with RL. The residual
policy is trained via on-policy RL, addressing distribution shifts and
introducing reactive control without altering the BC trajectory planner.
Evaluation on high-precision manipulation tasks demonstrates strong performance
of ResiP over BC methods and direct RL fine-tuning. Videos, code, and data are
available at https://residual-assembly.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://residual-assembly.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant Symmetry Breaking Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02681v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02681v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YuQing Xie, Tess Smidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant neural networks (ENNs) have been shown to be extremely effective
in applications involving underlying symmetries. By construction ENNs cannot
produce lower symmetry outputs given a higher symmetry input. However, symmetry
breaking occurs in many physical systems and we may obtain a less symmetric
stable state from an initial highly symmetric one. Hence, it is imperative that
we understand how to systematically break symmetry in ENNs. In this work, we
propose a novel symmetry breaking framework that is fully equivariant and is
the first which fully addresses spontaneous symmetry breaking. We emphasize
that our approach is general and applicable to equivariance under any group. To
achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather
than redesign existing networks, we design sets of symmetry breaking objects
which we feed into our network based on the symmetry of our inputs and outputs.
We show there is a natural way to define equivariance on these sets, which
gives an additional constraint. Minimizing the size of these sets equates to
data efficiency. We prove that minimizing these sets translates to a well
studied group theory problem, and tabulate solutions to this problem for the
point groups. Finally, we provide some examples of symmetry breaking to
demonstrate how our approach works in practice. The code for these examples is
available at \url{https://github.com/atomicarchitects/equivariant-SBS}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 19 figures Published in Transactions on Machine Learning
  Research, October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery and Classification Using Lempel-Ziv Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Dhruthi, Nithin Nagaraj, Harikrishnan N B
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal relationships in the decision-making processes of machine
learning algorithms is a crucial step toward achieving explainable Artificial
Intelligence (AI). In this research, we introduce a novel causality measure and
a distance metric derived from Lempel-Ziv (LZ) complexity. We explore how the
proposed causality measure can be used in decision trees by enabling splits
based on features that most strongly \textit{cause} the outcome. We further
evaluate the effectiveness of the causality-based decision tree and the
distance-based decision tree in comparison to a traditional decision tree using
Gini impurity. While the proposed methods demonstrate comparable classification
performance overall, the causality-based decision tree significantly
outperforms both the distance-based decision tree and the Gini-based decision
tree on datasets generated from causal models. This result indicates that the
proposed approach can capture insights beyond those of classical decision
trees, especially in causally structured data. Based on the features used in
the LZ causal measure based decision tree, we introduce a causal strength for
each features in the dataset so as to infer the predominant causal variables
for the occurrence of the outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Sampling Correction via Approximately 10 Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Wang, Wei Peng, Lijiang Li, Wenyu Chen, Yuren Cai, Songzhi Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models (DPMs) have demonstrated exceptional
performance in generative tasks, but this comes at the expense of sampling
efficiency. To enhance sampling speed without sacrificing quality, various
distillation-based accelerated sampling algorithms have been recently proposed.
However, they typically require significant additional training costs and model
parameter storage, which limit their practical application. In this work, we
propose PCA-based Adaptive Search (PAS), which optimizes existing solvers for
DPMs with minimal learnable parameters and training costs. Specifically, we
first employ PCA to obtain a few orthogonal unit basis vectors to span the
high-dimensional sampling space, which enables us to learn just a set of
coordinates to correct the sampling direction; furthermore, based on the
observation that the cumulative truncation error exhibits an ``S''-shape, we
design an adaptive search strategy that further enhances the sampling
efficiency and reduces the number of stored parameters to approximately 10.
Extensive experiments demonstrate that PAS can significantly enhance existing
fast solvers in a plug-and-play manner with negligible costs. For instance, on
CIFAR10, PAS requires only 12 parameters and less than 1 minute of training on
a single NVIDIA A100 GPU to optimize the DDIM from 15.69 FID (NFE=10) to 4.37.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Generative</span> Forests <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Nock, Mathieu Guillame-Bert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on generative AI for a type of data that still represent one of the
most prevalent form of data: tabular data. Our paper introduces two key
contributions: a new powerful class of forest-based models fit for such tasks
and a simple training algorithm with strong convergence guarantees in a
boosting model that parallels that of the original weak / strong supervised
learning setting. This algorithm can be implemented by a few tweaks to the most
popular induction scheme for decision tree induction (i.e. supervised learning)
with two classes. Experiments on the quality of generated data display
substantial improvements compared to the state of the art. The losses our
algorithm minimize and the structure of our models make them practical for
related tasks that require fast estimation of a density given a generative
model and an observation (even partially specified): such tasks include missing
data imputation and density estimation. Additional experiments on these tasks
reveal that our models can be notably good contenders to diverse state of the
art methods, relying on models as diverse as (or mixing elements of) trees,
neural nets, kernels or graphical models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Li, Zhicheng Sun, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the endeavor to make autonomous robots take actions, task planning is a
major challenge that requires translating high-level task descriptions into
long-horizon action sequences. Despite recent advances in language model
agents, they remain prone to planning errors and limited in their ability to
plan ahead. To address these limitations in robotic planning, we advocate a
self-refining scheme that iteratively refines a draft plan until an equilibrium
is reached. Remarkably, this process can be optimized end-to-end from an
analytical perspective without the need to curate additional verifiers or
reward models, allowing us to train self-refining planners in a simple
supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling
procedure is devised for efficient closed-loop planning that incorporates
useful feedback from the environment (or an internal world model). Our method
is evaluated on the VirtualHome-Env benchmark, showing advanced performance
with better scaling for inference computation. Code is available at
https://github.com/Singularity0104/equilibrium-planner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffPAD: Denoising <span class="highlight-title">Diffusion</span>-based Adversarial Patch Decontamination <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving adversarial machine learning landscape, developing
effective defenses against patch attacks has become a critical challenge,
necessitating reliable solutions to safeguard real-world AI systems. Although
diffusion models have shown remarkable capacity in image synthesis and have
been recently utilized to counter $\ell_p$-norm bounded attacks, their
potential in mitigating localized patch attacks remains largely underexplored.
In this work, we propose DiffPAD, a novel framework that harnesses the power of
diffusion models for adversarial patch decontamination. DiffPAD first performs
super-resolution restoration on downsampled input images, then adopts
binarization, dynamic thresholding scheme and sliding window for effective
localization of adversarial patches. Such a design is inspired by the
theoretically derived correlation between patch size and diffusion restoration
error that is generalized across diverse patch attack scenarios. Finally,
DiffPAD applies inpainting techniques to the original input images with the
estimated patch region being masked. By integrating closed-form solutions for
super-resolution restoration and image inpainting into the conditional reverse
sampling process of a pre-trained diffusion model, DiffPAD obviates the need
for text guidance or fine-tuning. Through comprehensive experiments, we
demonstrate that DiffPAD not only achieves state-of-the-art adversarial
robustness against patch attacks but also excels in recovering naturalistic
images without patch remnants. The source code is available at
https://github.com/JasonFu1998/DiffPAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Terracorder: Sense Long and Prosper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Millar, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-situ sensing devices need to be deployed in remote environments for long
periods of time; minimizing their power consumption is vital for maximising
both their operational lifetime and coverage. We introduce Terracorder -- a
versatile multi-sensor device -- and showcase its exceptionally low power
consumption using an on-device reinforcement learning scheduler. We prototype a
unique device setup for biodiversity monitoring and compare its battery life
using our scheduler against a number of fixed schedules; the scheduler captures
more than 80% of events at less than 50% of the number of activations of the
best-performing fixed schedule. We then explore how a collaborative scheduler
can maximise the useful operation of a network of devices, improving overall
network power consumption and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Similarity-Based Oversampling Method for Multi-label Imbalanced Text
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Hakki Karaman, Gulser Koksal, Levent Eriskin, Salih Salihoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, as data availability increases, obtaining labeled
data for machine learning (ML) projects remains challenging due to the high
costs and intensive efforts required for data annotation. Many ML projects,
particularly those focused on multi-label classification, also grapple with
data imbalance issues, where certain classes may lack sufficient data to train
effective classifiers. This study introduces and examines a novel oversampling
method for multi-label text classification, designed to address performance
challenges associated with data imbalance. The proposed method identifies
potential new samples from unlabeled data by leveraging similarity measures
between instances. By iteratively searching the unlabeled dataset, the method
locates instances similar to those in underrepresented classes and evaluates
their contribution to classifier performance enhancement. Instances that
demonstrate performance improvement are then added to the labeled dataset.
Experimental results indicate that the proposed approach effectively enhances
classifier performance post-oversampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03227v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03227v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Roca, Grégory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In MRI studies, the aggregation of imaging data from multiple acquisition
sites enhances sample size but may introduce site-related variabilities that
hinder consistency in subsequent analyses. Deep learning methods for image
translation have emerged as a solution for harmonizing MR images across sites.
In this study, we introduce IGUANe (Image Generation with Unified Adversarial
Networks), an original 3D model that leverages the strengths of domain
translation and straightforward application of style transfer methods for
multicenter brain MR image harmonization. IGUANe extends CycleGAN by
integrating an arbitrary number of domains for training through a many-to-one
architecture. The framework based on domain pairs enables the implementation of
sampling strategies that prevent confusion between site-related and biological
variabilities. During inference, the model can be applied to any image, even
from an unknown acquisition site, making it a universal generator for
harmonization. Trained on a dataset comprising T1-weighted images from 11
different scanners, IGUANe was evaluated on data from unseen sites. The
assessments included the transformation of MR images with traveling subjects,
the preservation of pairwise distances between MR images within domains, the
evolution of volumetric patterns related to age and Alzheimer$'$s disease (AD),
and the performance in age regression and patient classification tasks.
Comparisons with other harmonization and normalization methods suggest that
IGUANe better preserves individual information in MR images and is more
suitable for maintaining and reinforcing variabilities related to age and AD.
Future studies may further assess IGUANe in other multicenter contexts, either
using the same model or retraining it for applications to different image
modalities. IGUANe is available at
https://github.com/RocaVincent/iguane_harmonization.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Volume-Preserving <span class="highlight-title">Transformer</span>s for Learning Time Series Data with
  Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11166v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11166v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brantner, Guillaume de Romemont, Michael Kraus, Zeyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two of the many trends in neural network research of the past few years have
been (i) the learning of dynamical systems, especially with recurrent neural
networks such as long short-term memory networks (LSTMs) and (ii) the
introduction of transformer neural networks for natural language processing
(NLP) tasks.
  While some work has been performed on the intersection of these two trends,
those efforts were largely limited to using the vanilla transformer directly
without adjusting its architecture for the setting of a physical system.
  In this work we develop a transformer-inspired neural network and use it to
learn a dynamical system. We (for the first time) change the activation
function of the attention layer to imbue the transformer with
structure-preserving properties to improve long-term stability. This is shown
to be of great advantage when applying the neural network to learning the
trajectory of a rigid body.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Will be published as part of "Cemracs Proceedings 2023" (status:
  accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning-enabled velocity model building with uncertainty
  quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Orozco, Huseyin Tuna Erdinc, Yunlin Zeng, Mathias Louboutin, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately characterizing migration velocity models is crucial for a wide
range of geophysical applications, from hydrocarbon exploration to monitoring
of CO2 sequestration projects. Traditional velocity model building methods such
as Full-Waveform Inversion (FWI) are powerful but often struggle with the
inherent complexities of the inverse problem, including noise, limited
bandwidth, receiver aperture and computational constraints. To address these
challenges, we propose a scalable methodology that integrates generative
modeling, in the form of Diffusion networks, with physics-informed summary
statistics, making it suitable for complicated imaging problems including field
datasets. By defining these summary statistics in terms of subsurface-offset
image volumes for poor initial velocity models, our approach allows for
computationally efficient generation of Bayesian posterior samples for
migration velocity models that offer a useful assessment of uncertainty. To
validate our approach, we introduce a battery of tests that measure the quality
of the inferred velocity models, as well as the quality of the inferred
uncertainties. With modern synthetic datasets, we reconfirm gains from using
subsurface-image gathers as the conditioning observable. For complex velocity
model building involving salt, we propose a new iterative workflow that refines
amortized posterior approximations with salt flooding and demonstrate how the
uncertainty in the velocity model can be propagated to the final product
reverse time migrated images. Finally, we present a proof of concept on field
datasets to show that our method can scale to industry-sized problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition
  Path Sampling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqi Du, Michael Plainer, Rob Brekelmans, Chenru Duan, Frank Noé, Carla P. Gomes, Alán Aspuru-Guzik, Kirill Neklyudov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare event sampling in dynamical systems is a fundamental problem arising in
the natural sciences, which poses significant computational challenges due to
an exponentially large space of trajectories. For settings where the dynamical
system of interest follows a Brownian motion with known drift, the question of
conditioning the process to reach a given endpoint or desired rare event is
definitively answered by Doob's h-transform. However, the naive estimation of
this transform is infeasible, as it requires simulating sufficiently many
forward trajectories to estimate rare event probabilities. In this work, we
propose a variational formulation of Doob's h-transform as an optimization
problem over trajectories between a given initial point and the desired ending
point. To solve this optimization, we propose a simulation-free training
objective with a model parameterization that imposes the desired boundary
conditions by design. Our approach significantly reduces the search space over
trajectories and avoids expensive trajectory simulation and inefficient
importance sampling estimators which are required in existing methods. We
demonstrate the ability of our method to find feasible transition paths on
real-world molecular simulation and protein folding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Spotlight at Conference on Neural Information Processing
  Systems (NeurIPS 2024); Alanine dipeptide results updated after fixing
  unphysical parameterization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tract-RLFormer: A Tract-Specific RL policy based Decoder-only
  <span class="highlight-title">Transformer</span> Network <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fiber tractography is a cornerstone of neuroimaging, enabling the detailed
mapping of the brain's white matter pathways through diffusion MRI. This is
crucial for understanding brain connectivity and function, making it a valuable
tool in neurological applications. Despite its importance, tractography faces
challenges due to its complexity and susceptibility to false positives,
misrepresenting vital pathways. To address these issues, recent strategies have
shifted towards deep learning, utilizing supervised learning, which depends on
precise ground truth, or reinforcement learning, which operates without it. In
this work, we propose Tract-RLFormer, a network utilizing both supervised and
reinforcement learning, in a two-stage policy refinement process that markedly
improves the accuracy and generalizability across various data-sets. By
employing a tract-specific approach, our network directly delineates the tracts
of interest, bypassing the traditional segmentation process. Through rigorous
validation on datasets such as TractoInferno, HCP, and ISMRM-2015, our
methodology demonstrates a leap forward in tractography, showcasing its ability
to accurately map the brain's white matter tracts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 27th International Conference on Pattern Recognition
  (ICPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeMaScore : a new evaluation metric for automatic speech recognition
  tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitha Sasindran, Harsha Yelchuri, T. V. Prabhakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we present SeMaScore, generated using a segment-wise mapping
and scoring algorithm that serves as an evaluation metric for automatic speech
recognition tasks. SeMaScore leverages both the error rate and a more robust
similarity score. We show that our algorithm's score generation improves upon
the state-of-the-art BERTScore. Our experimental results show that SeMaScore
corresponds well with expert human assessments, signal-to-noise ratio levels,
and other natural language metrics. We outperform BERTScore by 41x in metric
computation speed. Overall, we demonstrate that SeMaScore serves as a more
dependable evaluation metric, particularly in real-world situations involving
atypical speech patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfiBench: Evaluating the Question-Answering Capabilities of Code Large
  <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models for code (code LLMs) have witnessed tremendous progress
in recent years. With the rapid development of code LLMs, many popular
evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to
measure the performance of code LLMs with a particular focus on code generation
tasks. However, they are insufficient to cover the full range of expected
capabilities of code LLMs, which span beyond code generation to answering
diverse coding-related questions. To fill this gap, we propose InfiBench, the
first large-scale freeform question-answering (QA) benchmark for code to our
knowledge, comprising 234 carefully selected high-quality Stack Overflow
questions that span across 15 programming languages. InfiBench uses four types
of model-free automatic metrics to evaluate response correctness where domain
experts carefully concretize the criterion for each question. We conduct a
systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a
series of novel and insightful findings. Our detailed analyses showcase
potential directions for further advancement of code LLMs. InfiBench is fully
open source at https://infi-coder.github.io/infibench and continuously
expanding to foster more scientific and systematic practices for code LLM
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages. Appear at NeurIPS 2024 Datasets and Benchmarks track.
  Project website: https://infi-coder.github.io/infibench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Estimation of Conditional Mean and Covariance for Unbalanced
  Panels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damir Filipovic, Paul Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a nonparametric, kernel-based joint estimator for conditional mean
and covariance matrices in large unbalanced panels. Our estimator, with proven
consistency and finite-sample guarantees, is applied to a comprehensive panel
of monthly US stock excess returns from 1962 to 2021, conditioned on
macroeconomic and firm-specific covariates. The estimator captures time-varying
cross-sectional dependencies effectively, demonstrating robust statistical
performance. In asset pricing, it generates conditional mean-variance efficient
portfolios with out-of-sample Sharpe ratios that substantially exceed those of
equal-weighted benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedding Hardware Approximations in Discrete Genetic-based Training for
  Printed MLPs <span class="chip">DATE'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentia Afentaki, Michael Hefenbrock, Georgios Zervakis, Mehdi B. Tahoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Printed Electronics (PE) stands out as a promisingtechnology for widespread
computing due to its distinct attributes, such as low costs and flexible
manufacturing. Unlike traditional silicon-based technologies, PE enables
stretchable, conformal,and non-toxic hardware. However, PE are constrained by
larger feature sizes, making it challenging to implement complex circuits such
as machine learning (ML) classifiers. Approximate computing has been proven to
reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs).
In this paper, we maximize the benefits of approximate computing by integrating
hardware approximation into the MLP training process. Due to the discrete
nature of hardware approximation, we propose and implement a genetic-based,
approximate, hardware-aware training approach specifically designed for printed
MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction
compared to the baseline while outperforming state of-the-art approximate and
stochastic printed MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 27th Design, Automation and Test in
  Europe Conference (DATE'24), Mar 25-27 2024, Valencia, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bespoke Approximation of Multiplication-Accumulation and Activation
  Targeting Printed Multilayer Perceptrons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florentia Afentaki, Gurol Saglam, Argyris Kokkinis, Kostas Siozios, Georgios Zervakis, Mehdi B Tahoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Printed Electronics (PE) feature distinct and remarkable characteristics that
make them a prominent technology for achieving true ubiquitous computing. This
is particularly relevant in application domains that require conformal and
ultra-low cost solutions, which have experienced limited penetration of
computing until now. Unlike silicon-based technologies, PE offer unparalleled
features such as non-recurring engineering costs, ultra-low manufacturing cost,
and on-demand fabrication of conformal, flexible, non-toxic, and stretchable
hardware. However, PE face certain limitations due to their large feature
sizes, that impede the realization of complex circuits, such as machine
learning classifiers. In this work, we address these limitations by leveraging
the principles of Approximate Computing and Bespoke (fully-customized) design.
We propose an automated framework for designing ultra-low power Multilayer
Perceptron (MLP) classifiers which employs, for the first time, a holistic
approach to approximate all functions of the MLP's neurons: multiplication,
accumulation, and activation. Through comprehensive evaluation across various
MLPs of varying size, our framework demonstrates the ability to enable
battery-powered operation of even the most intricate MLP architecture examined,
significantly surpassing the current state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 42th IEEE/ACM International
  Conference on Computer Aided Design (ICCAD) 2023, San Francisco, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResBit: Residual Bit Vector for Categorical Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17196v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17196v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masane Fuchi, Amar Zanashir, Hiroto Minami, Tomohiro Takagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-hot vectors, a common method for representing discrete/categorical data,
in machine learning are widely used because of their simplicity and
intuitiveness. However, one-hot vectors suffer from a linear increase in
dimensionality, posing computational and memory challenges, especially when
dealing with datasets containing numerous categories. In this paper, we focus
on tabular data generation, and reveal the multinomial diffusion faces the mode
collapse phenomenon when the cardinality is high. Moreover, due to the
limitations of one-hot vectors, the training phase takes time longer in such a
situation. To address these issues, we propose Residual Bit Vectors (ResBit), a
technique for densely representing categorical data. ResBit is an extension of
analog bits and overcomes limitations of analog bits when applied to tabular
data generation. Our experiments demonstrate that ResBit not only accelerates
training but also maintains performance when compared with the situations
before applying ResBit. Furthermore, our results indicate that many existing
methods struggle with high-cardinality data, underscoring the need for
lower-dimensional representations, such as ResBit and latent vectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 29 tables, and 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Boost Any Loss Function <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Nock, Yishay Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boosting is a highly successful ML-born optimization setting in which one is
required to computationally efficiently learn arbitrarily good models based on
the access to a weak learner oracle, providing classifiers performing at least
slightly differently from random guessing. A key difference with gradient-based
optimization is that boosting's original model does not requires access to
first order information about a loss, yet the decades long history of boosting
has quickly evolved it into a first order optimization setting -- sometimes
even wrongfully defining it as such. Owing to recent progress extending
gradient-based optimization to use only a loss' zeroth ($0^{th}$) order
information to learn, this begs the question: what loss functions can be
efficiently optimized with boosting and what is the information really needed
for boosting to meet the original boosting blueprint's requirements?
  We provide a constructive formal answer essentially showing that any loss
function can be optimized with boosting and thus boosting can achieve a feat
not yet known to be possible in the classical $0^{th}$ order setting, since
loss functions are not required to be be convex, nor differentiable or
Lipschitz -- and in fact not required to be continuous either. Some tools we
use are rooted in quantum calculus, the mathematical field -- not to be
confounded with quantum computation -- that studies calculus without passing to
the limit, and thus without using first order information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Green and Human-Like Artificial Intelligence: A Complete <span class="highlight-title">Survey</span>
  on Contemporary Few-Shot Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tsoumplekas, Vladislav Li, Panagiotis Sarigiannidis, Vasileios Argyriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite deep learning's widespread success, its data-hungry and
computationally expensive nature makes it impractical for many data-constrained
real-world applications. Few-Shot Learning (FSL) aims to address these
limitations by enabling rapid adaptation to novel learning tasks, seeing
significant growth in recent years. This survey provides a comprehensive
overview of the field's latest advancements. Initially, FSL is formally
defined, and its relationship with different learning fields is presented. A
novel taxonomy is introduced, extending previously proposed ones, and
real-world applications in classic and novel fields are described. Finally,
recent trends shaping the field, outstanding challenges, and promising future
research directions are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures. Submitted to ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Large Language Model</span>s Truly Grasp Mathematics? An Empirical
  Exploration From Cognitive Psychology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cognitive mechanism by which Large Language Models (LLMs) solve
mathematical problems remains a widely debated and unresolved issue. Currently,
there is little interpretable experimental evidence that connects LLMs'
problem-solving with human cognitive psychology.To determine if LLMs possess
human-like mathematical reasoning, we modified the problems used in the human
Cognitive Reflection Test (CRT). Our results show that, even with the use of
Chains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model
(noted for its reasoning capabilities), have a high error rate when solving
these modified CRT problems. Specifically, the average accuracy rate dropped by
up to 50% compared to the original questions.Further analysis of LLMs'
incorrect answers suggests that they primarily rely on pattern matching from
their training data, which aligns more with human intuition (System 1 thinking)
rather than with human-like reasoning (System 2 thinking). This finding
challenges the belief that LLMs have genuine mathematical reasoning abilities
comparable to humans. As a result, this work may adjust overly optimistic views
on LLMs' progress towards artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical mixtures of Unigram models for short text clustering: the
  role of Beta-Liouville priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimo Bilancia, Samuele Magro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a variant of the Multinomial mixture model tailored for
the unsupervised classification of short text data. Traditionally, the
Multinomial probability vector in this hierarchical model is assigned a
Dirichlet prior distribution. Here, however, we explore an alternative
prior--the Beta-Liouville distribution--which offers a more flexible
correlation structure than the Dirichlet. We examine the theoretical properties
of the Beta-Liouville distribution, focusing on its conjugacy with the
Multinomial likelihood. This property enables the derivation of update
equations for a CAVI (Coordinate Ascent Variational Inference) variational
algorithm, facilitating the approximate posterior estimation of model
parameters. Additionally, we propose a stochastic variant of the CAVI algorithm
that enhances scalability. The paper concludes with data examples that
demonstrate effective strategies for setting the Beta-Liouville
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 4 figures. Submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An improved tabular data generator with VAE-GMM integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patricia A. Apellániz, Juan Parras, Santiago Zazo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising use of machine learning in various fields requires robust methods
to create synthetic tabular data. Data should preserve key characteristics
while addressing data scarcity challenges. Current approaches based on
Generative Adversarial Networks, such as the state-of-the-art CTGAN model,
struggle with the complex structures inherent in tabular data. These data often
contain both continuous and discrete features with non-Gaussian distributions.
Therefore, we propose a novel Variational Autoencoder (VAE)-based model that
addresses these limitations. Inspired by the TVAE model, our approach
incorporates a Bayesian Gaussian Mixture model (BGM) within the VAE
architecture. This avoids the limitations imposed by assuming a strictly
Gaussian latent space, allowing for a more accurate representation of the
underlying data distribution during data generation. Furthermore, our model
offers enhanced flexibility by allowing the use of various differentiable
distributions for individual features, making it possible to handle both
continuous and discrete data types. We thoroughly validate our model on three
real-world datasets with mixed data types, including two medically relevant
ones, based on their resemblance and utility. This evaluation demonstrates
significant outperformance against CTGAN and TVAE, establishing its potential
as a valuable tool for generating synthetic tabular data in various domains,
particularly in healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Jia wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer architecture predominates across various models. As the heart
of the transformer, attention has a computational complexity of O(N^2),
compared to O(N) for linear transformations. When handling large sequence
lengths, attention becomes the primary time-consuming component. Although
quantization has proven to be an effective method for accelerating model
inference, existing quantization methods primarily focus on optimizing the
linear layer. In response, we first analyze the feasibility of quantization in
attention detailedly. Following that, we propose SageAttention, a highly
efficient and accurate quantization method for attention. The OPS (operations
per second) of our approach outperforms FlashAttention2 and xformers by about
2.1 times and 2.7 times, respectively. SageAttention also achieves superior
accuracy performance over FlashAttention3. Comprehensive experiments confirm
that our approach incurs almost no end-to-end metrics loss across diverse
models, including those for large language processing, image generation, and
video generation. The codes are available at
https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Evaluation of Clustered Federated Learning Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ben Ali, Omar El-Rifai, Imen Megdiche, André Peninou, Olivier Teste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over recent years, Federated Learning (FL) has proven to be one of the most
promising methods of distributed learning which preserves data privacy. As the
method evolved and was confronted to various real-world scenarios, new
challenges have emerged. One such challenge is the presence of highly
heterogeneous (often referred as non-IID) data distributions among participants
of the FL protocol. A popular solution to this hurdle is Clustered Federated
Learning (CFL), which aims to partition clients into groups where the
distribution are homogeneous. In the literature, state-of-the-art CFL
algorithms are often tested using a few cases of data heterogeneities, without
systematically justifying the choices. Further, the taxonomy used for
differentiating the different heterogeneity scenarios is not always
straightforward. In this paper, we explore the performance of two
state-of-theart CFL algorithms with respect to a proposed taxonomy of data
heterogeneities in federated learning (FL). We work with three image
classification datasets and analyze the resulting clusters against the
heterogeneity classes using extrinsic clustering metrics. Our objective is to
provide a clearer understanding of the relationship between CFL performances
and data heterogeneity scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Segment Clustering Strategy for Hierarchical Federated Learning in
  Heterogeneous Wireless Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Sun, Erwu Liu, Wei Ni, Kanglei Yu, Xinyu Qu, Rui Wang, Yanlong Bi, Chuanchun Zhang, Abbas Jamalipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-independent and identically distributed (Non- IID) data adversely affects
federated learning (FL) while heterogeneity in communication quality can
undermine the reliability of model parameter transmission, potentially
degrading wireless FL convergence. This paper proposes a novel dual-segment
clustering (DSC) strategy that jointly addresses communication and data
heterogeneity in FL. This is achieved by defining a new signal-to-noise ratio
(SNR) matrix and information quantity matrix to capture the communication and
data heterogeneity, respectively. The celebrated affinity propagation algorithm
is leveraged to iteratively refine the clustering of clients based on the newly
defined matrices effectively enhancing model aggregation in heterogeneous
environments. The convergence analysis and experimental results show that the
DSC strategy can improve the convergence rate of wireless FL and demonstrate
superior accuracy in heterogeneous environments compared to classical
clustering methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xPerT: Extended Persistence <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A persistence diagram provides a compact summary of persistent homology,
which captures the topological features of a space at different scales.
However, due to its nature as a set, incorporating it as a feature into a
machine learning framework is challenging. Several methods have been proposed
to use persistence diagrams as input for machine learning models, but they
often require complex preprocessing steps and extensive hyperparameter tuning.
In this paper, we propose a novel transformer architecture called the
\textit{Extended Persistence Transformer (xPerT)}, which is highly scalable
than the compared to Persformer, an existing transformer for persistence
diagrams. xPerT reduces GPU memory usage by over 90\% and improves accuracy on
multiple datasets. Additionally, xPerT does not require complex preprocessing
steps or extensive hyperparameter tuning, making it easy to use in practice.
Our code is available at https://github.com/sehunfromdaegu/xpert.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Small <span class="highlight-title">Language Model</span>s Learn, Unlearn, and Retain Noise Patterns? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small Language Models (SLMs) are generally considered more compact versions
of large language models (LLMs). This study investigates the ability of SLMs
with parameters between 1 and 3 billion to learn, retain, and subsequently
eliminate different types of noise present in the data. Four pre-trained SLMs
were utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The
models were instruction-tuned on noise-free data and tested using in-context
examples to determine if they could learn noise through examples. Subsequently,
noise patterns were introduced in instruction tuning to evaluate the noise
learning, unlearning, and retention capabilities of the models. Olmo, the
smallest model, was highly sensitive to noise, quickly adapting to noisy
patterns. Phi2 resisted learning character-level and transliteration noise,
likely due to its carefully curated, structured, and high-quality pretraining
data. Gemma excelled with transliteration noise, likely benefiting from its
multilingual pretraining. The findings can be used to develop robust training
strategies for SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Roles of <span class="highlight-title">Generative</span> Artificial Intelligence in Internet of Electric
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15750v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15750v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Zhang, Dusit Niyato, Wei Zhang, Changyuan Zhao, Hongyang Du, Abbas Jamalipour, Sumei Sun, Yiyang Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of generative artificial intelligence (GenAI) models,
their capabilities are expanding significantly beyond content generation and
the models are increasingly being used across diverse applications.
Particularly, GenAI shows great potential in addressing challenges in the
electric vehicle (EV) ecosystem ranging from charging management to
cyber-attack prevention. In this paper, we specifically consider Internet of
electric vehicles (IoEV) and we categorize GenAI for IoEV into four different
layers namely, EV's battery layer, individual EV layer, smart grid layer, and
security layer. We introduce various GenAI techniques used in each layer of
IoEV applications. Subsequently, public datasets available for training the
GenAI models are summarized. Finally, we provide recommendations for future
directions. This survey not only categorizes the applications of GenAI in IoEV
across different layers but also serves as a valuable resource for researchers
and practitioners by highlighting the design and implementation challenges
within each layer. Furthermore, it provides a roadmap for future research
directions, enabling the development of more robust and efficient IoEV systems
through the integration of advanced GenAI techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Robust Safe Sample Elimination under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroyuki Hanada, Tatsuya Aoyama, Satoshi Akahane, Tomonari Tanaka, Yoshito Okura, Yu Inatsu, Noriaki Hashimoto, Shion Takeno, Taro Murayama, Hanju Lee, Shinya Kojima, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a machine learning setup where one training dataset is used to
train multiple models across slightly different data distributions. This occurs
when customized models are needed for various deployment environments. To
reduce storage and training costs, we propose the DRSSS method, which combines
distributionally robust (DR) optimization and safe sample screening (SSS). The
key benefit of this method is that models trained on the reduced dataset will
perform the same as those trained on the full dataset for all possible
different environments. In this paper, we focus on covariate shift as a type of
data distribution change and demonstrate the effectiveness of our method
through experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning of Nash Equilibrium for Black-Box Games via Gaussian
  Processes <span class="chip">UAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbiao Han, Fengxue Zhang, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the challenge of learning in black-box games, where
the underlying utility function is unknown to any of the agents. While there is
an extensive body of literature on the theoretical analysis of algorithms for
computing the Nash equilibrium with complete information about the game,
studies on Nash equilibrium in black-box games are less common. In this paper,
we focus on learning the Nash equilibrium when the only available information
about an agent's payoff comes in the form of empirical queries. We provide a
no-regret learning algorithm that utilizes Gaussian processes to identify the
equilibrium in such games. Our approach not only ensures a theoretical
convergence rate but also demonstrates effectiveness across a variety
collection of games through experimental validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40th Conference on Uncertainty in Artificial Intelligence (UAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Budgeted Matching with General Bids <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Yang, Pengfei Li, Adam Wierman, Shaolei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Budgeted Matching (OBM) is a classic problem with important
applications in online advertising, online service matching, revenue
management, and beyond. Traditional online algorithms typically assume a small
bid setting, where the maximum bid-to-budget ratio (\kappa) is infinitesimally
small. While recent algorithms have tried to address scenarios with non-small
or general bids, they often rely on the Fractional Last Matching (FLM)
assumption, which allows for accepting partial bids when the remaining budget
is insufficient. This assumption, however, does not hold for many applications
with indivisible bids. In this paper, we remove the FLM assumption and tackle
the open problem of OBM with general bids. We first establish an upper bound of
1-\kappa on the competitive ratio for any deterministic online algorithm. We
then propose a novel meta algorithm, called MetaAd, which reduces to different
algorithms with first known provable competitive ratios parameterized by the
maximum bid-to-budget ratio \kappa \in [0, 1]. As a by-product, we extend
MetaAd to the FLM setting and get provable competitive algorithms. Finally, we
apply our competitive analysis to the design learning-augmented algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Mamba: Efficient Visual Representation Learning with
  Bidirectional State Space Model <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently the state space models (SSMs) with efficient hardware-aware designs,
i.e., the Mamba deep learning model, have shown great potential for long
sequence modeling. Meanwhile building efficient and generic vision backbones
purely upon SSMs is an appealing direction. However, representing visual data
is challenging for SSMs due to the position-sensitivity of visual data and the
requirement of global context for visual understanding. In this paper, we show
that the reliance on self-attention for visual representation learning is not
necessary and propose a new generic vision backbone with bidirectional Mamba
blocks (Vim), which marks the image sequences with position embeddings and
compresses the visual representation with bidirectional state space models. On
ImageNet classification, COCO object detection, and ADE20k semantic
segmentation tasks, Vim achieves higher performance compared to
well-established vision transformers like DeiT, while also demonstrating
significantly improved computation & memory efficiency. For example, Vim is
2.8$\times$ faster than DeiT and saves 86.8% GPU memory when performing batch
inference to extract features on images with a resolution of 1248$\times$1248.
The results demonstrate that Vim is capable of overcoming the computation &
memory constraints on performing Transformer-style understanding for
high-resolution images and it has great potential to be the next-generation
backbone for vision foundation models. Code is available at
https://github.com/hustvl/Vim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Vision Mamba (Vim) is accepted by ICML 2024. Code is available at
  https://github.com/hustvl/Vim</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autobidders with Budget and ROI Constraints: Efficiency, Regret, and
  Pacing Dynamics <span class="chip">COLT 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13306v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13306v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan Lucier, Sarath Pattathil, Aleksandrs Slivkins, Mengxiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a game between autobidding algorithms that compete in an online
advertising platform. Each autobidder is tasked with maximizing its
advertiser's total value over multiple rounds of a repeated auction, subject to
budget and return-on-investment constraints. We propose a gradient-based
learning algorithm that is guaranteed to satisfy all constraints and achieves
vanishing individual regret. Our algorithm uses only bandit feedback and can be
used with the first- or second-price auction, as well as with any
"intermediate" auction format. Our main result is that when these autobidders
play against each other, the resulting expected liquid welfare over all rounds
is at least half of the expected optimal liquid welfare achieved by any
allocation. This holds whether or not the bidding dynamics converges to an
equilibrium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at COLT 2024. Numerical experiments added since Jun'24
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Neural Goodness-of-Fit Test for Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoran Zhang, Wenbin Zhou, Liyan Xie, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data are crucial across diverse domains such as finance and
healthcare, where accurate forecasting and decision-making rely on advanced
modeling techniques. While generative models have shown great promise in
capturing the intricate dynamics inherent in time series, evaluating their
performance remains a major challenge. Traditional evaluation metrics fall
short due to the temporal dependencies and potential high dimensionality of the
features. In this paper, we propose the REcurrent NeurAL (RENAL)
Goodness-of-Fit test, a novel and statistically rigorous framework for
evaluating generative time series models. By leveraging recurrent neural
networks, we transform the time series into conditionally independent data
pairs, enabling the application of a chi-square-based goodness-of-fit test to
the temporal dependencies within the data. This approach offers a robust,
theoretically grounded solution for assessing the quality of generative models,
particularly in settings with limited time sequences. We demonstrate the
efficacy of our method across both synthetic and real-world datasets,
outperforming existing methods in terms of reliability and accuracy. Our method
fills a critical gap in the evaluation of time series generative models,
offering a tool that is both practical and adaptable to high-stakes
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-13T00:00:00Z">2024-11-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">62</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Limited Impact of Medical Adaptation of Large Language and
  Vision-<span class="highlight-title">Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare ten
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting and supervised fine-tuning regimes for medical question-answering
(QA). For instance, across all tasks and model pairs we consider in the 3-shot
setting, medical LLMs only outperform their base models in 22.7% of cases,
reach a (statistical) tie in 36.8% of cases, and are significantly worse than
their base models in the remaining 40.5% of cases. Our conclusions are based on
(i) comparing each medical model head-to-head, directly against the
corresponding base model; (ii) optimizing the prompts for each model separately
in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty
in comparisons. While these basic practices are not consistently adopted in the
literature, our ablations show that they substantially impact conclusions.
Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs
can show performance improvements, but the benefits do not carry over to tasks
based on clinical notes. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes
  additional results on clinical note QA tasks and supervised fine-tuning
  evaluations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CamemBERT 2.0: A Smarter French <span class="highlight-title">Language Model</span> Aged to Perfection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Antoun, Francis Kulumba, Rian Touchent, Éric de la Clergerie, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  French language models, such as CamemBERT, have been widely adopted across
industries for natural language processing (NLP) tasks, with models like
CamemBERT seeing over 4 million downloads per month. However, these models face
challenges due to temporal concept drift, where outdated training data leads to
a decline in performance, especially when encountering new topics and
terminology. This issue emphasizes the need for updated models that reflect
current linguistic trends. In this paper, we introduce two new versions of the
CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these
challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use
of the Replaced Token Detection (RTD) objective for better contextual
understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked
Language Modeling (MLM) objective. Both models are trained on a significantly
larger and more recent dataset with longer context length and an updated
tokenizer that enhances tokenization performance for French. We evaluate the
performance of these models on both general-domain NLP tasks and
domain-specific applications, such as medical field tasks, demonstrating their
versatility and effectiveness across a range of use cases. Our results show
that these updated models vastly outperform their predecessors, making them
valuable tools for modern NLP systems. All our new models, as well as
intermediate checkpoints, are made openly available on Huggingface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can sparse autoencoders be used to decompose and interpret steering
  vectors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Mayne, Yushi Yang, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steering vectors are a promising approach to control the behaviour of large
language models. However, their underlying mechanisms remain poorly understood.
While sparse autoencoders (SAEs) may offer a potential method to interpret
steering vectors, recent findings show that SAE-reconstructed vectors often
lack the steering properties of the original vectors. This paper investigates
why directly applying SAEs to steering vectors yields misleading
decompositions, identifying two reasons: (1) steering vectors fall outside the
input distribution for which SAEs are designed, and (2) steering vectors can
have meaningful negative projections in feature directions, which SAEs are not
designed to accommodate. These limitations hinder the direct use of SAEs for
interpreting steering vectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Cross-lingual Transfer Learning with Multiple Source and
  Target Languages for Information Extraction: Language Selection and
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Trung Ngo, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of previous researches addressing multi-lingual IE are limited
to zero-shot cross-lingual single-transfer (one-to-one) setting, with
high-resource languages predominantly as source training data. As a result,
these works provide little understanding and benefit for the realistic goal of
developing a multi-lingual IE system that can generalize to as many languages
as possible. Our study aims to fill this gap by providing a detailed analysis
on Cross-Lingual Multi-Transferability (many-to-many transfer learning), for
the recent IE corpora that cover a diverse set of languages. Specifically, we
first determine the correlation between single-transfer performance and a wide
range of linguistic-based distances. From the obtained insights, a combined
language distance metric can be developed that is not only highly correlated
but also robust across different tasks and model scales. Next, we investigate
the more general zero-shot multi-lingual transfer settings where multiple
languages are involved in the training and evaluation processes. Language
clustering based on the newly defined distance can provide directions for
achieving the optimal cost-performance trade-off in data (languages) selection
problem. Finally, a relational-transfer setting is proposed to further
incorporate multi-lingual unlabeled data based on adversarial training using
the relation induced from the above linguistic distance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Perspective Stance Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subjective NLP tasks usually rely on human annotations provided by multiple
annotators, whose judgments may vary due to their diverse backgrounds and life
experiences. Traditional methods often aggregate multiple annotations into a
single ground truth, disregarding the diversity in perspectives that arises
from annotator disagreement. In this preliminary study, we examine the effect
of including multiple annotations on model accuracy in classification. Our
methodology investigates the performance of perspective-aware classification
models in stance detection task and further inspects if annotator disagreement
affects the model confidence. The results show that multi-perspective approach
yields better classification performance outperforming the baseline which uses
the single label. This entails that designing more inclusive perspective-aware
AI models is not only an essential first step in implementing responsible and
ethical AI, but it can also achieve superior results than using the traditional
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separating Tongue from <span class="highlight-title">Thought</span>: Activation Patching Reveals
  Language-Agnostic Concept Representations in <span class="highlight-title">Transformer</span>s <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central question in multilingual language modeling is whether large
language models (LLMs) develop a universal concept representation, disentangled
from specific languages. In this paper, we address this question by analyzing
latent representations (latents) during a word translation task in
transformer-based LLMs. We strategically extract latents from a source
translation prompt and insert them into the forward pass on a target
translation prompt. By doing so, we find that the output language is encoded in
the latent at an earlier layer than the concept to be translated. Building on
this insight, we conduct two key experiments. First, we demonstrate that we can
change the concept without changing the language and vice versa through
activation patching alone. Second, we show that patching with the mean over
latents across different languages does not impair and instead improves the
models' performance in translating the concept. Our results provide evidence
for the existence of language-agnostic concept representations within the
investigated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, previously published under the title "How Do
  Llamas Process Multilingual Text? A Latent Exploration through Activation
  Patching" at the ICML 2024 mechanistic interpretability workshop
  https://openreview.net/forum?id=0ku2hIm4BS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks
  with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingdong Wang, Mingyu Cui, Dongchao Yang, Xueyuan Chen, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of Speech Large Language Models (Speech LLMs), there has been
growing interest in discrete speech tokens for their ability to integrate with
text-based tokens seamlessly. Compared to most studies that focus on continuous
speech features, although discrete-token based LLMs have shown promising
results on certain tasks, the performance gap between these two paradigms is
rarely explored. In this paper, we present a fair and thorough comparison
between discrete and continuous features across a variety of semantic-related
tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that
continuous features generally outperform discrete tokens, particularly in tasks
requiring fine-grained semantic understanding. Moreover, this study goes beyond
surface-level comparison by identifying key factors behind the
under-performance of discrete tokens, such as limited token granularity and
inefficient information retention. To enhance the performance of discrete
tokens, we explore potential aspects based on our analysis. We hope our results
can offer new insights into the opportunities for advancing discrete speech
tokens in Speech LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Rewarding with <span class="highlight-title">Prompt</span> Optimization Enables Tuning-free
  Self-Alignment of <span class="highlight-title">Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Models (LLMs) traditionally relies on costly training
and human preference annotations. Self-alignment seeks to reduce these expenses
by enabling models to align themselves. To further lower costs and achieve
alignment without any expensive tuning or annotations, we introduce a new
tuning-free approach for self-alignment, Dynamic Rewarding with Prompt
Optimization (\ours). Our approach leverages a search-based optimization
framework that allows LLMs to iteratively self-improve and craft the optimal
alignment instructions, all without additional training or human intervention.
The core of \ours is a dynamic rewarding mechanism, which identifies and
rectifies model-specific alignment weaknesses, allowing LLMs to adapt
efficiently to diverse alignment challenges. Empirical evaluations on eight
recent LLMs, both open- and closed-sourced, demonstrate that \ours
significantly enhances alignment performance, with base models outperforming
their SFT/RLHF-tuned counterparts. Moreover, the prompts automatically
optimized by \ours surpass those curated by human experts, further validating
the effectiveness of our approach. Our findings highlight the great potential
of current LLMs to achieve adaptive self-alignment through inference-time
optimization, complementing tuning-based alignment methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyst Reports and Stock Performance: Evidence from the Chinese Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Jiayou Liang, Haolong Chen, Yujia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article applies natural language processing (NLP) to extract and
quantify textual information to predict stock performance. Using an extensive
dataset of Chinese analyst reports and employing a customized BERT deep
learning model for Chinese text, this study categorizes the sentiment of the
reports as positive, neutral, or negative. The findings underscore the
predictive capacity of this sentiment indicator for stock volatility, excess
returns, and trading volume. Specifically, analyst reports with strong positive
sentiment will increase excess return and intraday volatility, and vice versa,
reports with strong negative sentiment also increase volatility and trading
volume, but decrease future excess return. The magnitude of this effect is
greater for positive sentiment reports than for negative sentiment reports.
This article contributes to the empirical literature on sentiment analysis and
the response of the stock market to news in the Chinese stock market.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Triggers Needed for Document-Level <span class="highlight-title">Event</span> Extraction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaden Shaar, Wayne Chen, Maitreyi Chatterjee, Barry Wang, Wenting Zhao, Claire Cardie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing work on event extraction has focused on sentence-level texts
and presumes the identification of a trigger-span -- a word or phrase in the
input that evokes the occurrence of an event of interest. Event arguments are
then extracted with respect to the trigger. Indeed, triggers are treated as
integral to, and trigger detection as an essential component of, event
extraction. In this paper, we provide the first investigation of the role of
triggers for the more difficult and much less studied task of document-level
event extraction. We analyze their usefulness in multiple end-to-end and
pipelined neural event extraction models for three document-level event
extraction datasets, measuring performance using triggers of varying quality
(human-annotated, LLM-generated, keyword-based, and random). Our research shows
that trigger effectiveness varies based on the extraction task's
characteristics and data quality, with basic, automatically-generated triggers
serving as a viable alternative to human-annotated ones. Furthermore, providing
detailed event descriptions to the extraction model helps maintain robust
performance even when trigger quality degrades. Perhaps surprisingly, we also
find that the mere existence of trigger input, even random ones, is important
for prompt-based LLM approaches to the task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Analysis of Byte-Pair Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        László Kozma, Johannes Voderholzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte-Pair Encoding (BPE) is a widely used method for subword tokenization,
with origins in grammar-based text compression. It is employed in a variety of
language processing tasks such as machine translation or large language model
(LLM) pretraining, to create a token dictionary of a prescribed size. Most
evaluations of BPE to date are empirical, and the reasons for its good
practical performance are not well understood.
  In this paper we focus on the optimization problem underlying BPE: finding a
pair encoding that achieves optimal compression utility. We show that this
problem is APX-complete, indicating that it is unlikely to admit a
polynomial-time approximation scheme. This answers, in a stronger form, a
question recently raised by Zouhar et al.
  On the positive side, we show that BPE approximates the compression utility
of the optimal pair encoding to a worst-case factor between $0.333$ and
$0.625$. Our results aim to explain the ongoing success of BPE and are, to our
knowledge, the first rigorous guarantees on its compression utility that hold
for all inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Subset Tuning: Expanding the Operational Range of
  Parameter-Efficient Training for <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Stahlberg, Jared Lichtarge, Shankar Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel parameter-efficient training (PET) method for large
language models that adapts models to downstream tasks by optimizing a small
subset of the existing model parameters. Unlike prior methods, this subset is
not fixed in location but rather which parameters are modified evolves over the
course of training. This dynamic parameter selection can yield good performance
with many fewer parameters than extant methods. Our method enables a seamless
scaling of the subset size across an arbitrary proportion of the total model
size, while popular PET approaches like prompt tuning and LoRA cover only a
small part of this spectrum. We match or outperform prompt tuning and LoRA in
most cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given
parameter budget across different model families and sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Adaptive Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the challenges of large language model performance in natural
language to SQL tasks, we introduce XiYan-SQL, an innovative framework that
employs a multi-generator ensemble strategy to improve candidate generation. We
introduce M-Schema, a semi-structured schema representation method designed to
enhance the understanding of database structures. To enhance the quality and
diversity of generated candidate SQL queries, XiYan-SQL integrates the
significant potential of in-context learning (ICL) with the precise control of
supervised fine-tuning. On one hand, we propose a series of training strategies
to fine-tune models to generate high-quality candidates with diverse
preferences. On the other hand, we implement the ICL approach with an example
selection method based on named entity recognition to prevent overemphasis on
entities. The refiner optimizes each candidate by correcting logical or
syntactical errors. To address the challenge of identifying the best candidate,
we fine-tune a selection model to distinguish nuances of candidate SQL queries.
The experimental results on multiple dialect datasets demonstrate the
robustness of XiYan-SQL in addressing challenges across different scenarios.
Overall, our proposed XiYan-SQL achieves the state-of-the-art execution
accuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on
NL2GQL, and a competitive score of 72.23% on the Bird development benchmark.
The proposed framework not only enhances the quality and diversity of SQL
queries but also outperforms previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorrSynth -- A Correlated Sampling Method for Diverse <span class="highlight-title">Dataset</span> Generation
  from <span class="highlight-title">LLM</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhas S Kowshik, Abhishek Divekar, Vijit Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance in
diverse tasks using zero-shot and few-shot prompting. Even though their
capabilities of data synthesis have been studied well in recent years, the
generated data suffers from a lack of diversity, less adherence to the prompt,
and potential biases that creep into the data from the generator model. In this
work, we tackle the challenge of generating datasets with high diversity, upon
which a student model is trained for downstream tasks. Taking the route of
decoding-time guidance-based approaches, we propose CorrSynth, which generates
data that is more diverse and faithful to the input prompt using a correlated
sampling strategy. Further, our method overcomes the complexity drawbacks of
some other guidance-based techniques like classifier-based guidance. With
extensive experiments, we show the effectiveness of our approach and
substantiate our claims. In particular, we perform intrinsic evaluation to show
the improvements in diversity. Our experiments show that CorrSynth improves
both student metrics and intrinsic metrics upon competitive baselines across
four datasets, showing the innate advantage of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at EMNLP 2024; First two authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Topic Modeling with <span class="highlight-title">Large Language Model</span>s in the Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Yang, He Zhao, Weijie Xu, Yuanyuan Qi, Jueqing Lu, Dinh Phung, Lan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a fundamental task in natural language processing, allowing
the discovery of latent thematic structures in text corpora. While Large
Language Models (LLMs) have demonstrated promising capabilities in topic
discovery, their direct application to topic modeling suffers from issues such
as incomplete topic coverage, misalignment of topics, and inefficiency. To
address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop
framework that integrates LLMs with many existing Neural Topic Models (NTMs).
In LLM-ITL, global topics and document representations are learned through the
NTM, while an LLM refines the topics via a confidence-weighted Optimal
Transport (OT)-based alignment objective. This process enhances the
interpretability and coherence of the learned topics, while maintaining the
efficiency of NTMs. Extensive experiments demonstrate that LLM-ITL can help
NTMs significantly improve their topic interpretability while maintaining the
quality of document representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree-of-Table: Unleashing the Power of <span class="highlight-title">LLM</span>s for Enhanced Large-Scale
  Table Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deyi Ji, Lanyun Zhu, Siqi Gao, Peng Xu, Hongtao Lu, Jieping Ye, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquity and value of tables as semi-structured data across various
domains necessitate advanced methods for understanding their complexity and
vast amounts of information. Despite the impressive capabilities of large
language models (LLMs) in advancing the natural language understanding
frontier, their application to large-scale tabular data presents significant
challenges, specifically regarding table size and complex intricate
relationships. Existing works have shown promise with small-scale tables but
often flounder when tasked with the complex reasoning required by larger,
interconnected tables found in real-world scenarios. To address this gap, we
introduce "Tree-of-Table", a novel approach designed to enhance LLMs' reasoning
capabilities over large and complex tables. Our method employs Table
Condensation and Decomposition to distill and reorganize relevant data into a
manageable format, followed by the construction of a hierarchical Table-Tree
that facilitates tree-structured reasoning. Through a meticulous Table-Tree
Execution process, we systematically unravel the tree-structured reasoning
chain to derive the solutions. Experiments across diverse datasets, including
WikiTQ, TableFact, FeTaQA, and BIRD, demonstrate that Tree-of-Table sets a new
benchmark with superior performance, showcasing remarkable efficiency and
generalization capabilities in large-scale table reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information Theoretic Approach to Operationalize Right to Data
  Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Java, Simra Shahid, Chirag Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread practice of indiscriminate data scraping to fine-tune language
models (LMs) raises significant legal and ethical concerns, particularly
regarding compliance with data protection laws such as the General Data
Protection Regulation (GDPR). This practice often results in the unauthorized
use of personal information, prompting growing debate within the academic and
regulatory communities. Recent works have introduced the concept of generating
unlearnable datasets (by adding imperceptible noise to the clean data), such
that the underlying model achieves lower loss during training but fails to
generalize to the unseen test setting. Though somewhat effective, these
approaches are predominantly designed for images and are limited by several
practical constraints like requiring knowledge of the target model. To this
end, we introduce RegText, a framework that injects imperceptible spurious
correlations into natural language datasets, effectively rendering them
unlearnable without affecting semantic content. We demonstrate RegText's
utility through rigorous empirical analysis of small and large LMs. Notably,
RegText can restrict newer models like GPT-4o and Llama from learning on our
generated data, resulting in a drop in their test accuracy compared to their
zero-shot performance and paving the way for generating unlearnable text to
protect public data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Objective and Unbiased Decision Assessments with <span class="highlight-title">LLM</span>-Enhanced
  Hierarchical Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Kwan Hui Lim, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How objective and unbiased are we while making decisions? This work
investigates cognitive bias identification in high-stake decision making
process by human experts, questioning its effectiveness in real-world settings,
such as candidates assessments for university admission. We begin with a
statistical analysis assessing correlations among different decision points
among in the current process, which discovers discrepancies that imply
cognitive bias and inconsistency in decisions. This motivates our exploration
of bias-aware AI-augmented workflow that surpass human judgment. We propose
BGM-HAN, a hierarchical attention network enhanced by byte-pair encoding,
multi-head attention and gated residual connection. Using it as backbone model,
we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow, which
simulate real-world decision-making. In our experiments, both the proposed
model and the agentic workflow significantly improves on both human judgment
and alternative models, validated with real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Evaluating <span class="highlight-title">Large Language Model</span>s for Graph Query Generation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siraj Munir, Alessandro Aldini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted and will be presented at CSCI2024 in December 2024,
  Later will be published at Springer LNCS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One STEP at a time: Language <span class="highlight-title">Agent</span>s are Stepwise Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Nguyen, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have shown promising adaptability in dynamic environments to
perform complex tasks. However, despite the versatile knowledge embedded in
large language models, these agents still fall short when it comes to tasks
that require planning. We introduce STEP, a novel framework designed to
efficiently learn from previous experiences to enhance the planning
capabilities of language agents in future steps. Concretely, STEP functions
through four interconnected components. First, the Planner takes on the task,
breaks it down into subtasks and provides relevant insights. Then the Executor
generates action candidates, while the Evaluator ensures the actions align with
learned rules from previous experiences. Lastly, Memory stores experiences to
inform future decisions. In the ScienceWorld benchmark, our results show that
STEP consistently outperforms state-of-the-art models, achieving an overall
score of 67.4 and successfully completing 12 out of 18 tasks. These findings
highlight STEP's potential as a framework for enhancing planning capabilities
in language agents, paving the way for more sophisticated task-solving in
dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLaSP: Learning Concepts for Time-Series Signals from Natural Language
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoi Ito, Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a foundation model called "CLaSP" that can search time
series signals using natural language that describes the characteristics of the
signals as queries. Previous efforts to represent time series signal data in
natural language have had challenges in designing a conventional class of time
series signal characteristics, formulating their quantification, and creating a
dictionary of synonyms. To overcome these limitations, the proposed method
introduces a neural network based on contrastive learning. This network is
first trained using the datasets TRUCE and SUSHI, which consist of time series
signals and their corresponding natural language descriptions. Previous studies
have proposed vocabularies that data analysts use to describe signal
characteristics, and SUSHI was designed to cover these terms. We believe that a
neural network trained on these datasets will enable data analysts to search
using natural language vocabulary. Furthermore, our method does not require a
dictionary of predefined synonyms, and it leverages common sense knowledge
embedded in a large-scale language model (LLM). Experimental results
demonstrate that CLaSP enables natural language search of time series signal
data and can accurately learn the points at which signal data changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Syntactic Representations Enable Hierarchical Word Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biraj Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed representations currently used are dense and uninterpretable,
leading to interpretations that themselves are relative, overcomplete, and hard
to interpret. We propose a method that transforms these word vectors into
reduced syntactic representations. The resulting representations are compact
and interpretable allowing better visualization and comparison of the word
vectors and we successively demonstrate that the drawn interpretations are in
line with human judgment. The syntactic representations are then used to create
hierarchical word vectors using an incremental learning approach similar to the
hierarchical aspect of human learning. As these representations are drawn from
pre-trained vectors, the generation process and learning approach are
computationally efficient. Most importantly, we find out that syntactic
representations provide a plausible interpretation of the vectors and
subsequent hierarchical vectors outperform the original vectors in benchmark
tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Translations with <span class="highlight-title">LLM</span>s: A Constraint-Aware Iterative <span class="highlight-title">Prompt</span>ing
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangfeng Chen, Xiayang Shi, Pu Li, Yinlin Li, Jingjing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable proficiency in
machine translation (MT), even without specific training on the languages in
question. However, translating rare words in low-resource or domain-specific
contexts remains challenging for LLMs. To address this issue, we propose a
multi-step prompt chain that enhances translation faithfulness by prioritizing
key terms crucial for semantic accuracy. Our method first identifies these
keywords and retrieves their translations from a bilingual dictionary,
integrating them into the LLM's context using Retrieval-Augmented Generation
(RAG). We further mitigate potential output hallucinations caused by long
prompts through an iterative self-checking mechanism, where the LLM refines its
translations based on lexical and semantic constraints. Experiments using Llama
and Qwen as base models on the FLORES-200 and WMT datasets demonstrate
significant improvements over baselines, highlighting the effectiveness of our
approach in enhancing translation faithfulness and robustness, particularly in
low-resource scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Chinese Multi-label Affective Computing <span class="highlight-title">Dataset</span> Based on Social Media
  Network Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhou, Senlin Luo, Haofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion and personality are central elements in understanding human
psychological states. Emotions reflect an individual subjective experiences,
while personality reveals relatively stable behavioral and cognitive patterns.
Existing affective computing datasets often annotate emotion and personality
traits separately, lacking fine-grained labeling of micro-emotions and emotion
intensity in both single-label and multi-label classifications. Chinese emotion
datasets are extremely scarce, and datasets capturing Chinese user personality
traits are even more limited. To address these gaps, this study collected data
from the major social media platform Weibo, screening 11,338 valid users from
over 50,000 individuals with diverse MBTI personality labels and acquiring
566,900 posts along with the user MBTI personality tags. Using the EQN method,
we compiled a multi-label Chinese affective computing dataset that integrates
the same user's personality traits with six emotions and micro-emotions, each
annotated with intensity levels. Validation results across multiple NLP
classification models demonstrate the dataset strong utility. This dataset is
designed to advance machine recognition of complex human emotions and provide
data support for research in psychology, education, marketing, finance, and
politics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bangla Grammatical Error Detection Leveraging <span class="highlight-title">Transformer</span>-based Token
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayekh Bin Islam, Ridwanul Hasan Tanvir, Sihat Afnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bangla is the seventh most spoken language by a total number of speakers in
the world, and yet the development of an automated grammar checker in this
language is an understudied problem. Bangla grammatical error detection is a
task of detecting sub-strings of a Bangla text that contain grammatical,
punctuation, or spelling errors, which is crucial for developing an automated
Bangla typing assistant. Our approach involves breaking down the task as a
token classification problem and utilizing state-of-the-art transformer-based
models. Finally, we combine the output of these models and apply rule-based
post-processing to generate a more reliable and comprehensive result. Our
system is evaluated on a dataset consisting of over 25,000 texts from various
sources. Our best model achieves a Levenshtein distance score of 1.04. Finally,
we provide a detailed analysis of different components of our system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are <span class="highlight-title">LLM</span>s Prescient? A Continuous Evaluation using Daily News as the
  Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Dai, Ryan Teehan, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict "future" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R3HF: Reward Redistribution for Enhancing Reinforcement Learning from
  Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Li, Tai-wei Chang, Fengda Zhang, Kun Kuang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) provides a paradigm for
aligning large language models (LLMs) with human preferences. This involves the
initial training of a reward model based on pairwise human feedback. The reward
model is subsequently utilized in reinforcement learning to assess the scores
of each generated sentence as a whole, further guiding the optimization of
LLMs. However, current approaches have a significant shortcoming: \emph{They
allocate a single, sparse, and delayed reward to an entire sequence of output}.
This may overlook some significant individual contributions of each token
towards the desired outcome. To overcome this limitation, our paper proposes a
novel reward redistribution method called R3HF, which facilitates a more
fine-grained, token-level reward allocation. Specifically, our method treats
the reward prediction task of the reward model as a regression problem. As a
result, the redistributed rewards are computed by evaluating the specific
contribution of each token to the reward model's output. This detailed approach
improves the model's understanding of language nuances, leading to more precise
enhancements in its performance. Our method is crafted to integrate seamlessly
with most current techniques while incurring minimal computational costs.
Through comprehensive experiments across diverse datasets and tasks, we have
verified the effectiveness and superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Bases in Support of <span class="highlight-title">Large Language Model</span>s for Processing Web
  News 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Zhang, Nabin Pakka, Nian-feng Tzeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have received considerable interest in wide
applications lately. During pre-training via massive datasets, such a model
implicitly memorizes the factual knowledge of trained datasets in its hidden
parameters. However, knowledge held implicitly in parameters often makes its
use by downstream applications ineffective due to the lack of common-sense
reasoning. In this article, we introduce a general framework that permits to
build knowledge bases with an aid of LLMs, tailored for processing Web news.
The framework applies a rule-based News Information Extractor (NewsIE) to news
items for extracting their relational tuples, referred to as knowledge bases,
which are then graph-convoluted with the implicit knowledge facts of news items
obtained by LLMs, for their classification. It involves two lightweight
components: 1) NewsIE: for extracting the structural information of every news
item, in the form of relational tuples; 2) BERTGraph: for graph convoluting the
implicit knowledge facts with relational tuples extracted by NewsIE. We have
evaluated our framework under different news-related datasets for news category
classification, with promising experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Study of Relevance Assessments with <span class="highlight-title">Large Language Model</span>s:
  An Initial Look 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of large language models to provide relevance assessments
presents exciting opportunities to advance information retrieval, natural
language processing, and beyond, but to date many unknowns remain. This paper
reports on the results of a large-scale evaluation (the TREC 2024 RAG Track)
where four different relevance assessment approaches were deployed in situ: the
"standard" fully manual process that NIST has implemented for decades and three
different alternatives that take advantage of LLMs to different extents using
the open-source UMBRELA tool. This setup allows us to correlate system rankings
induced by the different approaches to characterize tradeoffs between cost and
quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system
rankings induced by automatically generated relevance assessments from UMBRELA
correlate highly with those induced by fully manual assessments across a
diverse set of 77 runs from 19 teams. Our results suggest that automatically
generated UMBRELA judgments can replace fully manual judgments to accurately
capture run-level effectiveness. Surprisingly, we find that LLM assistance does
not appear to increase correlation with fully manual assessments, suggesting
that costs associated with human-in-the-loop processes do not bring obvious
tangible benefits. Overall, human assessors appear to be stricter than UMBRELA
in applying relevance criteria. Our work validates the use of LLMs in academic
TREC-style evaluations and provides the foundation for future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code-mixed <span class="highlight-title">LLM</span>: Improve <span class="highlight-title">Large Language Model</span>s' Capability to Handle
  Code-Mixing through Reinforcement Learning from AI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Zhang, Aditya Majumdar, Amulya Yadav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of
linguistic units from two or more languages during the conversation or
sometimes even a single utterance. Code-mixing introduces unique challenges in
daily life, such as syntactic mismatches and semantic blending, that are rarely
encountered in monolingual settings. Large language models (LLMs) have
revolutionized the field of natural language processing (NLP) by offering
unprecedented capabilities in understanding human languages. However, the
effectiveness of current state-of-the-art multilingual LLMs has not yet been
fully explored in the CM scenario. To fill this gap, we first benchmark the
performance of multilingual LLMs on various code-mixing NLP tasks. Then we
propose to improve the multilingual LLMs' ability to understand code-mixing
through reinforcement learning from human feedback (RLHF) and code-mixed
machine translation tasks. Given the high-cost and time-consuming preference
labeling procedure, we improve this by utilizing LLMs as annotators to perform
the reinforcement learning from AI feedback (RLAIF). The experiments show the
effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>initial version: 5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Visual Gap: Fine-Tuning Multimodal Models with
  Knowledge-Adapted Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Yanuka, Assaf Ben Kish, Yonatan Bitton, Idan Szpektor, Raja Giryes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research increasingly focuses on training vision-language models
(VLMs) with long, detailed image captions. However, small-scale VLMs often
struggle to balance the richness of these captions with the risk of
hallucinating content during fine-tuning. In this paper, we explore how well
VLMs adapt to such captions. To quantify caption quality, we propose Decomposed
NLI (DNLI), an evaluation framework that breaks down generated captions into
individual propositions, assessing each in isolation. This fine-grained
analysis reveals a critical balance between capturing descriptive details and
preventing hallucinations. Our findings show that simply reducing caption
complexity or employing standard data curation techniques does not effectively
resolve this issue. To tackle this challenge, we introduce Knowledge Adapted
(KnowAda) fine-tuning, a data-centric approach that automatically adapts
training data with the model's existing knowledge and visual understanding.
KnowAda minimizes hallucinations while preserving high descriptiveness. We
validate this approach across several small-scale VLMs (up to 7B parameters)
and dense caption datasets, demonstrating that KnowAda effectively balances
hallucination reduction and descriptiveness. Our results show that KnowAda
outperforms various baselines in both automatic metrics and human evaluations.
We will release our code and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cut Your Losses in Large-Vocabulary <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models grow ever larger, so do their vocabularies. This has
shifted the memory footprint of LLMs during training disproportionately to one
single layer: the cross-entropy in the loss computation. Cross-entropy builds
up a logit matrix with entries for each pair of input tokens and vocabulary
items and, for small models, consumes an order of magnitude more memory than
the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that
computes the cross-entropy loss without materializing the logits for all tokens
into global memory. Rather, CCE only computes the logit for the correct token
and evaluates the log-sum-exp over all logits on the fly. We implement a custom
kernel that performs the matrix multiplications and the log-sum-exp reduction
over the vocabulary in flash memory, making global memory consumption for the
cross-entropy computation negligible. This has a dramatic effect. Taking the
Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss
computation from 24 GB to 1 MB, and the total training-time memory consumption
of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we
leverage the inherent sparsity of softmax and propose to skip elements of the
gradient computation that have a negligible (i.e., below numerical precision)
contribution to the gradient. Experiments demonstrate that the dramatic
reduction in memory consumption is accomplished without sacrificing training
speed or convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/apple/ml-cross-entropy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refusal in <span class="highlight-title">LLM</span>s is an Affine Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Marshall, Adam Scherlis, Nora Belrose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose affine concept editing (ACE) as an approach for steering language
models' behavior by intervening directly in activations. We begin with an
affine decomposition of model activation vectors and show that prior methods
for steering model behavior correspond to subsets of terms of this
decomposition. We then provide a derivation of ACE and test it on refusal using
Llama 3 8B and Hermes Eagle RWKV v5. ACE ultimately combines affine subspace
projection and activation addition to reliably control the model's refusal
responses across prompt types. We evaluate the results using LLM-based scoring
on a collection of harmful and harmless prompts. Our experiments demonstrate
that ACE consistently achieves more precise control over model behavior and
generalizes to models where directional ablation via affine subspace projection
alone produces incoherent outputs. Code for reproducing our results is
available at https://github.com/EleutherAI/steering-llama3 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoP: Enhancing Text Classification with <span class="highlight-title">LLM</span> through Code Completion
  <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental task in natural language processing
(NLP), and large language models (LLMs) have demonstrated their capability to
perform this task across various domains. However, the performance of LLMs
heavily depends on the quality of their input prompts. Recent studies have also
shown that LLMs exhibit remarkable results in code-related tasks. To leverage
the capabilities of LLMs in text classification, we propose the Code Completion
Prompt (CoCoP) method, which transforms the text classification problem into a
code completion task. CoCoP significantly improves text classification
performance across diverse datasets by utilizing LLMs' code-completion
capability. For instance, CoCoP enhances the accuracy of the SST2 dataset by
more than 20%. Moreover, when CoCoP integrated with LLMs specifically designed
for code-related tasks (code models), such as CodeLLaMA, this method
demonstrates better or comparable performance to few-shot learning techniques
while using only one-tenth of the model size. The source code of our proposed
method will be available to the public upon the acceptance of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness and Confounders in the Demographic Alignment of <span class="highlight-title">LLM</span>s with
  Human Perceptions of Offensiveness <span class="chip">ACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Alipour, Indira Sen, Mattia Samory, Tanushree Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are known to exhibit demographic biases, yet few
studies systematically evaluate these biases across multiple datasets or
account for confounding factors. In this work, we examine LLM alignment with
human annotations in five offensive language datasets, comprising approximately
220K annotations. Our findings reveal that while demographic traits,
particularly race, influence alignment, these effects are inconsistent across
datasets and often entangled with other factors. Confounders -- such as
document difficulty, annotator sensitivity, and within-group agreement --
account for more variation in alignment patterns than demographic traits alone.
Specifically, alignment increases with higher annotator sensitivity and group
agreement, while greater document difficulty corresponds to reduced alignment.
Our results underscore the importance of multi-dataset analyses and
confounder-aware methodologies in developing robust measures of demographic
bias in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, ACL'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Upcycling: Inference Inefficient Finetuning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasha Doubov, Nikhil Sardana, Vitaliy Chiley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small, highly trained, open-source large language models are widely used due
to their inference efficiency, but further improving their quality remains a
challenge. Sparse upcycling is a promising approach that transforms a
pretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing
the model's parameter count and quality. In this work, we compare the
effectiveness of sparse upcycling against continued pretraining (CPT) across
different model sizes, compute budgets, and pretraining durations. Our
experiments show that sparse upcycling can achieve better quality, with
improvements of over 20% relative to CPT in certain scenarios. However, this
comes with a significant inference cost, leading to 40% slowdowns in
high-demand inference settings for larger models. Our findings highlight the
trade-off between model quality and inference efficiency, offering insights for
practitioners seeking to balance model quality and deployment constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, To appear in the 4th NeurIPS Workshop on
  Efficient Natural Language and Speech Processing (ENLSP), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Single <span class="highlight-title">Transformer</span> for Scalable Vision-<span class="highlight-title">Language Model</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Xingyao Wang, Hao Peng, <span class="highlight-author">Heng Ji</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.
Current large vision-language models (LVLMs) such as LLaVA mostly employ
heterogeneous architectures that connect pre-trained visual encoders with large
language models (LLMs) to facilitate visual recognition and complex reasoning.
Although achieving remarkable performance with relatively lightweight training,
we identify four primary scalability limitations: (1) The visual capacity is
constrained by pre-trained visual encoders, which are typically an order of
magnitude smaller than LLMs. (2) The heterogeneous architecture complicates the
use of established hardware and software infrastructure. (3) Study of scaling
laws on such architecture must consider three separate components - visual
encoder, connector, and LLMs, which complicates the analysis. (4) The use of
existing visual encoders typically requires following a pre-defined
specification of image inputs pre-processing, for example, by reshaping inputs
to fixed-resolution square images, which presents difficulties in processing
and training on high-resolution images or those with unusual aspect ratio. A
unified single Transformer architecture, like SOLO, effectively addresses these
scalability concerns in LVLMs; however, its limited adoption in the modern
context likely stems from the absence of reliable training recipes that balance
both modalities and ensure stable training for billion-scale models. In this
paper, we introduce the first open-source training recipe for developing SOLO,
an open-source 7B LVLM using moderate academic resources. The training recipe
involves initializing from LLMs, sequential pre-training on ImageNet and
web-scale data, and instruction fine-tuning on our curated high-quality
datasets. On extensive evaluation, SOLO demonstrates performance comparable to
LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MILU: A Multi-task Indic Language Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) in low-resource and linguistically
diverse languages remains a significant challenge in NLP, particularly for
languages using non-Latin scripts like those spoken in India. Existing
benchmarks predominantly focus on English, leaving substantial gaps in
assessing LLM capabilities in these languages. We introduce MILU, a Multi task
Indic Language Understanding Benchmark, a comprehensive evaluation benchmark
designed to address this gap. MILU spans 8 domains and 42 subjects across 11
Indic languages, reflecting both general and culturally specific knowledge.
With an India-centric design, incorporates material from regional and
state-level examinations, covering topics such as local history, arts,
festivals, and laws, alongside standard subjects like science and mathematics.
We evaluate over 45 LLMs, and find that current LLMs struggle with MILU, with
GPT-4o achieving the highest average accuracy at 72 percent. Open multilingual
models outperform language-specific fine-tuned models, which perform only
slightly better than random baselines. Models also perform better in high
resource languages as compared to low resource ones. Domain-wise analysis
indicates that models perform poorly in culturally relevant areas like Arts and
Humanities, Law and Governance compared to general fields like STEM. To the
best of our knowledge, MILU is the first of its kind benchmark focused on Indic
languages, serving as a crucial step towards comprehensive cultural evaluation.
All code, benchmarks, and artifacts are publicly available to foster open
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from over-reliance on unimodal biases (e.g., language bias
and vision bias), leading to incorrect answers or hallucinations in complex
multimodal tasks. To investigate this issue, we propose a causal framework to
interpret the biases in Visual Question Answering (VQA) problems. Within this
framework, we conduct an in-depth causal analysis to assess the causal effect
of these biases on MLLM predictions. Based on the analysis, we introduce 1) a
novel MORE dataset with 12,000 challenging VQA instances requiring multi-hop
reasoning and overcoming unimodal biases. 2) a causality-enhanced agent
framework CAVE that guides models to comprehensively integrate information from
different modalities and mitigate biases. Our experiments show that MLLMs
perform poorly on MORE, indicating strong unimodal biases and limited semantic
understanding. However, when integrated with our CAVE, promising improvements
in reasoning and bias mitigation can be seen. These findings provide important
insights for the development of more robust MLLMs and contribute to the broader
goal of advancing multimodal AI systems capable of deeper understanding and
reasoning. Our project page is at https://github.com/OpenCausaLab/MORE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Uncertainty of <span class="highlight-title">Thought</span>s: Uncertainty-Aware Planning Enhances Information
  Seeking in <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03271v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03271v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, <span class="highlight-author">Junxian He</span>, Pang Wei Koh, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the face of uncertainty, the ability to *seek information* is of
fundamental importance. In many practical applications, such as medical
diagnosis and troubleshooting, the information needed to solve the task is not
initially given and has to be actively sought by asking follow-up questions
(for example, a doctor asking a patient for more details about their symptoms).
In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to
augment large language models with the ability to actively seek information by
asking effective questions. UoT combines 1) an *uncertainty-aware simulation
approach* which enables the model to simulate possible future scenarios and how
likely they are to occur, 2) *uncertainty-based rewards* motivated by
information gain which incentivizes the model to seek information, and 3) a
*reward propagation scheme* to select the optimal question to ask in a way that
maximizes the expected reward. In experiments on medical diagnosis,
troubleshooting, and the `20 Questions` game, UoT achieves an average
performance improvement of 38.1% in the rate of successful task completion
across multiple LLMs compared with direct prompting and also improves
efficiency (i.e., the number of questions needed to complete the task). Our
code has been released [here](https://github.com/zhiyuanhubj/UoT)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New LLM evaluation benchmarks are important to align with the rapid
development of Large Language Models (LLMs). In this work, we present Chinese
SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality
ability of language models to answer short questions, and Chinese SimpleQA
mainly has five properties (i.e., Chinese, Diverse, High-quality, Static,
Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6
major topics with 99 diverse subtopics. Second, we conduct a comprehensive
quality control process to achieve high-quality questions and answers, where
the reference answers are static and cannot be changed over time. Third,
following SimpleQA, the questions and answers are very short, and the grading
process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we
perform a comprehensive evaluation on the factuality abilities of existing
LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to
better understand the Chinese factuality abilities of their models and
facilitate the growth of foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting
  Volunteer Content Moderators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07879v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07879v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Trista Cao, Lovely-Frances Domingo, Sarah Ann Gilbert, Michelle Mazurek, Katie Shilton, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive efforts in automated approaches for content moderation have been
focused on developing models to identify toxic, offensive, and hateful content
with the aim of lightening the load for moderators. Yet, it remains uncertain
whether improvements on those tasks have truly addressed moderators' needs in
accomplishing their work. In this paper, we surface gaps between past research
efforts that have aimed to provide automation for aspects of content moderation
and the needs of volunteer content moderators, regarding identifying violations
of various moderation rules. To do so, we conduct a model review on Hugging
Face to reveal the availability of models to cover various moderation rules and
guidelines from three exemplar forums. We further put state-of-the-art LLMs to
the test, evaluating how well these models perform in flagging violations of
platform rules from one particular forum. Finally, we conduct a user survey
study with volunteer moderators to gain insight into their perspectives on
useful moderation models. Overall, we observe a non-trivial gap, as missing
developed models and LLMs exhibit moderate to low performance on a significant
portion of the rules. Moderators' reports provide guides for future work on
developing moderation assistant models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Economists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melissa Dell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning provides powerful methods to impute structured information from
large-scale, unstructured text and image datasets. For example, economists
might wish to detect the presence of economic activity in satellite images, or
to measure the topics or entities mentioned in social media, the congressional
record, or firm filings. This review introduces deep neural networks, covering
methods such as classifiers, regression models, generative AI, and embedding
models. Applications include classification, document digitization, record
linkage, and methods for data exploration in massive scale text and image
corpora. When suitable methods are used, deep learning models can be cheap to
tune and can scale affordably to problems involving millions or billions of
data points.. The review is accompanied by a companion website, EconDL, with
user-friendly demo notebooks, software resources, and a knowledge base that
provides technical details and additional applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Free Lunch in <span class="highlight-title">LLM</span> Watermarking: Trade-offs in Watermarking Design
  Choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in generative models have made it possible for AI-generated text,
code, and images to mirror human-generated content in many applications.
Watermarking, a technique that aims to embed information in the output of a
model to verify its source, is useful for mitigating the misuse of such
AI-generated content. However, we show that common design choices in LLM
watermarking schemes make the resulting systems surprisingly susceptible to
attack -- leading to fundamental trade-offs in robustness, utility, and
usability. To navigate these trade-offs, we rigorously study a set of simple
yet effective attacks on common watermarking systems, and propose guidelines
and defenses for LLM watermarking in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General <span class="highlight-title">LLM</span>s as Instructors for Domain-Specific <span class="highlight-title">LLM</span>s: A Sequential
  Fusion Method to Integrate Extraction and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The substantial interest in updating Large Language Models (LLMs) without
retraining from scratch is accompanied by several challenges. This is
particularly true when updating LLMs with datasets that necessitate
domain-expert reasoning across extensive texts, despite limited samples. We
termed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs
(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval
Augmented Generation (RAG) are inadequate for addressing this critical issue,
particularly evident in our exploration of a specific medical dataset that
epitomizes the distinct needs of FDoR-UL. To tackle this challenge, we
introduce a Sequential Fusion method to integrate knowledge from complex
contexts into LLMs. This method employs a two-stage framework: initially
leveraging general LLMs to perform relation extraction for knowledge
acquisition from complex texts, followed by updating domain-specific LLMs
through Knowledge Editing (KE). Employing our method, domain-specific LLMs
achieved a 71.7% accuracy (an average gain of 39.1%) in question-answering
tasks. Furthermore, we expanded our evaluation to a novel economics-management
dataset we developed, where our method achieved a 75.0% accuracy (an average
gain of 45.0%). These findings underscore the effectiveness and flexibility of
our approach in FDoR-UL across various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Moral Self-correction An Innate Capability of <span class="highlight-title">Large Language Model</span>s?
  A Mechanistic Analysis to Self-correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimo Qi, Guangliang Liu, Kristen Marie Johnson, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though intensive attentions to the self-correction capability of Large
Language Models (LLMs), the underlying mechanism of this capability is still
under-explored. In this paper, we aim to answer two fundamental questions for
moral self-correction: (1) how different components in self-correction, such as
Chain-of-Thought (CoT) reasoning, external feedback, and instructional prompts,
interact to enable moral self-correction; and (2) is the self-correction one of
LLMs' innate capabilities? To answer the first question, we examine how
different self-correction components interact to intervene the embedded
morality within hidden states, therefore contributing to different performance.
For the second question, we (i) evaluate the robustness of moral
self-correction by introducing natural language interventions of weak evidence
into prompts; (ii) propose a validation framework, self-distinguish, that
requires effective self-correction to enable LLMs to distinguish between
desirable and undesirable outputs. Our experimental results indicate that there
is no universally optimal self-correction method for the tasks considered,
although external feedback and CoT can contribute to additional performance
gains. However, our mechanistic analysis reveals negative interactions among
instructional prompts, CoT, and external feedback, suggesting a conflict
between internal knowledge and external feedback. The self-distinguish
experiments demonstrate that while LLMs can self-correct their responses, they
are unable to reliably distinguish between desired and undesired outputs. With
our empirical evidence, we can conclude that moral self-correction is not an
innate capability of LLMs acquired during pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Post-Hoc Attributions in Long Document Comprehension via
  Coarse Grained Answer Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately attributing answer text to its source document is crucial for
developing a reliable question-answering system. However, attribution for long
documents remains largely unexplored. Post-hoc attribution systems are designed
to map answer text back to the source document, yet the granularity of this
mapping has not been addressed. Furthermore, a critical question arises: What
exactly should be attributed? This involves identifying the specific
information units within an answer that require grounding. In this paper, we
propose and investigate a novel approach to the factual decomposition of
generated answers for attribution, employing template-based in-context
learning. To accomplish this, we utilize the question and integrate negative
sampling during few-shot in-context learning for decomposition. This approach
enhances the semantic understanding of both abstractive and extractive answers.
We examine the impact of answer decomposition by providing a thorough
examination of various attribution approaches, ranging from retrieval-based
techniques to LLM-based attributors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are <span class="highlight-title">Large Language Model</span>s Table-based Fact-Checkers? <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table-based Fact Verification (TFV) aims to extract the entailment relation
between statements and structured tables. Existing TFV methods based on
small-scaled models suffer from insufficient labeled data and weak zero-shot
ability. Recently, the appearance of Large Language Models (LLMs) has gained
lots of attraction in research fields. They have shown powerful zero-shot and
in-context learning abilities on several NLP tasks, but their potential on TFV
is still unknown. In this work, we implement a preliminary study about whether
LLMs are table-based fact-checkers. In detail, we design diverse prompts to
explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and
few-shot TFV capability. Besides, we carefully design and construct TFV
instructions to study the performance gain brought by the instruction tuning of
LLMs. Experimental results demonstrate that LLMs can achieve acceptable results
on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning
can stimulate the TFV capability significantly. We also make some valuable
findings about the format of zero-shot prompts and the number of in-context
examples. Finally, we analyze some possible directions to promote the accuracy
of TFV via LLMs, which is beneficial to further research of table reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CSCWD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target-driven Attack for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhang, Mingyu Jin, Dong Shu, Taowen Wang, Dongfang Liu, Xiaobo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current large language models (LLM) provide a strong foundation for
large-scale user-oriented natural language tasks. Many users can easily inject
adversarial text or instructions through the user interface, thus causing LLM
model security challenges like the language model not giving the correct
answer. Although there is currently a large amount of research on black-box
attacks, most of these black-box attacks use random and heuristic strategies.
It is unclear how these strategies relate to the success rate of attacks and
thus effectively improve model robustness. To solve this problem, we propose
our target-driven black-box attack method to maximize the KL divergence between
the conditional probabilities of the clean text and the attack text to redefine
the attack's goal. We transform the distance maximization problem into two
convex optimization problems based on the attack goal to solve the attack text
and estimate the covariance. Furthermore, the projected gradient descent
algorithm solves the vector corresponding to the attack text. Our target-driven
black-box attack approach includes two attack strategies: token manipulation
and misinformation attack. Experimental results on multiple Large Language
Models and datasets demonstrate the effectiveness of our attack method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures. This work is an extension of the
  arXiv:2404.07234 work. We propose new methods. 27th European Conference on
  Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthesizRR: Generating Diverse <span class="highlight-title">Dataset</span>s with Retrieval Augmentation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Divekar, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is often desirable to distill the capabilities of large language models
(LLMs) into smaller student models due to compute and memory constraints. One
way to do this for classification tasks is via dataset synthesis, which can be
accomplished by generating examples of each label from the LLM. Prior
approaches to synthesis use few-shot prompting, which relies on the LLM's
parametric knowledge to generate usable examples. However, this leads to issues
of repetition, bias towards popular entities, and stylistic differences from
human text. In this work, we propose Synthesize by Retrieval and Refinement
(SynthesizRR), which uses retrieval augmentation to introduce variety into the
dataset synthesis process: as retrieved passages vary, the LLM is seeded with
different content to generate its examples. We empirically study the synthesis
of six datasets, covering topic classification, sentiment analysis, tone
detection, and humor, requiring complex synthesis strategies. We find that
SynthesizRR greatly improves lexical and semantic diversity, similarity to
human-written text, and distillation performance, when compared to 32-shot
prompting and four prior approaches. We release our code to perform all steps
at https://github.com/amazon-science/synthesizrr
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at EMNLP 2024. Code available at
  https://github.com/amazon-science/synthesizrr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vikhr: Constructing a State-of-the-art Bilingual Open-Source
  Instruction-Following <span class="highlight-title">Large Language Model</span> for Russian <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13929v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13929v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Nikolich, Konstantin Korolev, Sergei Bratchikov, Igor Kiselev, Artem Shelmanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a surge in developing various Large Language Models (LLMs).
However, text generation for languages other than English often faces
significant challenges, including poor generation quality and reduced
computational performance due to the disproportionate representation of tokens
in the model's vocabulary. In this work, we address these issues by developing
a pipeline for adapting English-oriented pre-trained models to other languages
and constructing efficient bilingual LLMs. Using this pipeline, we construct
Vikhr, a state-of-the-art bilingual open-source instruction-following LLM
designed specifically for the Russian language. "Vikhr" refers to the name of
the Mistral LLM series and means a "strong gust of wind." Unlike previous
Russian-language models that typically rely on LoRA adapters on top of
English-oriented models, sacrificing performance for lower training costs,
Vikhr features an adapted tokenizer vocabulary and undergoes continued
pre-training and instruction tuning of all weights. This not only enhances the
model's performance but also significantly improves its computational and
contextual efficiency. The remarkable performance of Vikhr across various
Russian-language benchmarks can also be attributed to our efforts in expanding
instruction datasets and corpora for continued pre-training. Vikhr not only
sets a new state of the art among open-source LLMs for Russian but even
outperforms some proprietary closed-source models on certain benchmarks. The
model weights, instruction sets, and code are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WMRL @ EMNLP-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaffold-BPE: Enhancing Byte Pair Encoding for <span class="highlight-title">Large Language Model</span>s
  with Simple and Effective Scaffold Token Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lian, Yizhe Xiong, Jianwei Niu, Shasha Mo, Zhenpeng Su, Zijia Lin, Hui Chen, Peng Liu, Jungong Han, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte Pair Encoding (BPE) serves as a foundation method for text tokenization
in the Natural Language Processing (NLP) field. Despite its wide adoption, the
original BPE algorithm harbors an inherent flaw: it inadvertently introduces a
frequency imbalance for tokens in the text corpus. Since BPE iteratively merges
the most frequent token pair in the text corpus to generate a new token and
keeps all generated tokens in the vocabulary, it unavoidably holds tokens that
primarily act as components of a longer token and appear infrequently on their
own. We term such tokens as Scaffold Tokens. Due to their infrequent
occurrences in the text corpus, Scaffold Tokens pose a learning imbalance
issue. To address that issue, we propose Scaffold-BPE, which incorporates a
dynamic scaffold token removal mechanism by parameter-free, computation-light,
and easy-to-implement modifications to the original BPE method. This novel
approach ensures the exclusion of low-frequency Scaffold Tokens from the token
representations for given texts, thereby mitigating the issue of frequency
imbalance and facilitating model training. On extensive experiments across
language modeling and even machine translation, Scaffold-BPE consistently
outperforms the original BPE, well demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spin glass model of in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Li, Ruoran Bai, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models show a surprising in-context learning ability -- being
able to use a prompt to form a prediction for a query, yet without additional
training, in stark contrast to old-fashioned supervised learning. Providing a
mechanistic interpretation and linking the empirical phenomenon to physics are
thus challenging and remain unsolved. We study a simple yet expressive
transformer with linear attention and map this structure to a spin glass model
with real-valued spins, where the couplings and fields explain the intrinsic
disorder in data. The spin glass model explains how the weight parameters
interact with each other during pre-training, and further clarifies why an
unseen function can be predicted by providing only a prompt yet without further
training. Our theory reveals that for single-instance learning, increasing the
task diversity leads to the emergence of in-context learning, by allowing the
Boltzmann distribution to converge to a unique correct solution of weight
parameters. Therefore the pre-trained transformer displays a prediction power
in a novel prompt setting. The proposed analytically tractable model thus
offers a promising avenue for thinking about how to interpret many intriguing
but puzzling properties of large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating AI-Generated Essays with GRE Analytical Writing Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhong, Jiangang Hao, Michael Fauss, Chen Li, Yuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent revolutionary advance in generative AI enables the generation of
realistic and coherent texts by large language models (LLMs). Despite many
existing evaluation metrics on the quality of the generated texts, there is
still a lack of rigorous assessment of how well LLMs perform in complex and
demanding writing assessments. This study examines essays generated by ten
leading LLMs for the analytical writing assessment of the Graduate Record Exam
(GRE). We assessed these essays using both human raters and the e-rater
automated scoring engine as used in the GRE scoring pipeline. Notably, the
top-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,
respectively, falling between "generally thoughtful, well-developed analysis of
the issue and conveys meaning clearly" and "presents a competent analysis of
the issue and conveys meaning with acceptable clarity" according to the GRE
scoring guideline. We also evaluated the detection accuracy of these essays,
with detectors trained on essays generated by the same and different LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-IF: Benchmarking <span class="highlight-title">LLM</span>s on Multi-Turn and Multilingual Instructions
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, Sinong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
various tasks, including instruction following, which is crucial for aligning
model outputs with user expectations. However, evaluating LLMs' ability to
follow instructions remains challenging due to the complexity and subjectivity
of human language. Current benchmarks primarily focus on single-turn,
monolingual instructions, which do not adequately reflect the complexities of
real-world applications that require handling multi-turn and multilingual
interactions. To address this gap, we introduce Multi-IF, a new benchmark
designed to assess LLMs' proficiency in following multi-turn and multilingual
instructions. Multi-IF, which utilizes a hybrid framework combining LLM and
human annotators, expands upon the IFEval by incorporating multi-turn sequences
and translating the English prompts into another 7 languages, resulting in a
dataset of 4,501 multilingual conversations, where each has three turns. Our
evaluation of 14 state-of-the-art LLMs on Multi-IF reveals that it presents a
significantly more challenging task than existing benchmarks. All the models
tested showed a higher rate of failure in executing instructions correctly with
each additional turn. For example, o1-preview drops from 0.877 at the first
turn to 0.707 at the third turn in terms of average accuracy over all
languages. Moreover, languages with non-Latin scripts (Hindi, Russian, and
Chinese) generally exhibit higher error rates, suggesting potential limitations
in the models' multilingual capabilities. We release Multi-IF prompts and the
evaluation code base to encourage further research in this critical area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experiences from Creating a Benchmark for Sentiment Classification for
  Varieties of English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipankar Srirag, Jordan Painter, Aditya Joshi, Diptesh Kanojia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks often fail to account for linguistic diversity, like
language variants of English. In this paper, we share our experiences from our
ongoing project of building a sentiment classification benchmark for three
variants of English: Australian (en-AU), Indian (en-IN), and British (en-UK)
English. Using Google Places reviews, we explore the effects of various
sampling techniques based on label semantics, review length, and sentiment
proportion and report performances on three fine-tuned BERT-based models. Our
initial evaluation reveals significant performance variations influenced by
sample characteristics, label semantics, and language variety, highlighting the
need for nuanced benchmark design. We offer actionable insights for researchers
to create robust benchmarks, emphasising the importance of diverse sampling,
careful label definition, and comprehensive evaluation across linguistic
varieties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair <span class="highlight-title">Summarization</span>: Bridging Quality and Diversity in Extractive
  Summaries <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in multi-document summarization of user-generated content remains a
critical challenge in natural language processing (NLP). Existing summarization
methods often fail to ensure equitable representation across different social
groups, leading to biased outputs. In this paper, we introduce two novel
methods for fair extractive summarization: FairExtract, a clustering-based
approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.
We evaluate these methods using Divsumm summarization dataset of White-aligned,
Hispanic, and African-American dialect tweets and compare them against relevant
baselines. The results obtained using a comprehensive set of summarization
quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well
as a fairness metric F, demonstrate that FairExtract and FairGPT achieve
superior fairness while maintaining competitive summarization quality.
Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that
integrate quality and fairness into a single evaluation framework, offering a
more nuanced understanding of the trade-offs between these objectives. This
work highlights the importance of fairness in summarization and sets a
benchmark for future research in fairness-aware NLP models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Algorithmic Fairness through the Lens of Metrics and
  Evaluation Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretability Needs a New Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Madsen, Himabindu Lakkaraju, Siva Reddy, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is the study of explaining models in understandable terms to
humans. At present, interpretability is divided into two paradigms: the
intrinsic paradigm, which believes that only models designed to be explained
can be explained, and the post-hoc paradigm, which believes that black-box
models can be explained. At the core of this debate is how each paradigm
ensures its explanations are faithful, i.e., true to the model's behavior. This
is important, as false but convincing explanations lead to unsupported
confidence in artificial intelligence (AI), which can be dangerous. This
paper's position is that we should think about new paradigms while staying
vigilant regarding faithfulness. First, by examining the history of paradigms
in science, we see that paradigms are constantly evolving. Then, by examining
the current paradigms, we can understand their underlying beliefs, the value
they bring, and their limitations. Finally, this paper presents 3 emerging
paradigms for interpretability. The first paradigm designs models such that
faithfulness can be easily measured. Another optimizes models such that
explanations become faithful. The last paradigm proposes to develop models that
produce both a prediction and an explanation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Prep-Kit: getting your data ready for <span class="highlight-title">LLM</span> application development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wood, Boris Lublinsky, Alexy Roytman, Shivdeep Singh, Constantin Adam, Abdulhamid Adebayo, Sungeun An, Yuan Chi Chang, Xuan-Hong Dang, Nirmit Desai, Michele Dolfi, Hajar Emami-Gohari, Revital Eres, Takuya Goto, Dhiraj Joshi, Yan Koyfman, Mohammad Nassar, Hima Patel, Paramesvaran Selvam, Yousaf Shah, Saptha Surendran, Daiki Tsuzuku, Petros Zerfos, Shahrokh Daijavad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trustful <span class="highlight-title">LLM</span>s: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Jaya Krishna Mandivarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MDCure: A Scalable Pipeline for Multi-Document Instruction-Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document (MD) processing is crucial for LLMs to handle real-world tasks
such as summarization and question-answering across large sets of documents.
While LLMs have improved at processing long inputs, MD contexts still present
challenges, such as managing inter-document dependencies, redundancy, and
incoherent structures. We introduce MDCure, a scalable and effective
fine-tuning pipeline to enhance the MD capabilities of LLMs without the
computational cost of pre-training or reliance on human annotated data. MDCure
is based on generation of high-quality synthetic MD instruction data from sets
of related articles via targeted prompts. We further introduce MDCureRM, a
multi-objective reward model which filters generated data based on their
training utility for MD settings. With MDCure, we fine-tune a variety of LLMs,
from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in
size. Extensive evaluations on a wide range of MD and long-context benchmarks
spanning various tasks show MDCure consistently improves performance over
pre-trained baselines and over corresponding base models by up to 75.5%. Our
code, datasets, and models are available at https://github.com/yale-nlp/MDCure.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking negative sampling in content-based news recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Ângelo Rebelo, João Vinagre, Ivo Pereira, Álvaro Figueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News recommender systems are hindered by the brief lifespan of articles, as
they undergo rapid relevance decay. Recent studies have demonstrated the
potential of content-based neural techniques in tackling this problem. However,
these models often involve complex neural architectures and often lack
consideration for negative examples. In this study, we posit that the careful
sampling of negative examples has a big impact on the model's outcome. We
devise a negative sampling technique that not only improves the accuracy of the
model but also facilitates the decentralization of the recommendation system.
The experimental results obtained using the MIND dataset demonstrate that the
accuracy of the method under consideration can compete with that of
State-of-the-Art models. The utilization of the sampling technique is essential
in reducing model complexity and accelerating the training process, while
maintaining a high level of accuracy. Finally, we discuss how decentralized
models can help improve privacy and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scholarly Wikidata: Population and Exploration of Conference Data in
  Wikidata using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandana Mihindukulasooriya, Sanju Tiwari, Daniil Dobriy, Finn Årup Nielsen, Tek Raj Chhetri, Axel Polleres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several initiatives have been undertaken to conceptually model the domain of
scholarly data using ontologies and to create respective Knowledge Graphs. Yet,
the full potential seems unleashed, as automated means for automatic population
of said ontologies are lacking, and respective initiatives from the Semantic
Web community are not necessarily connected: we propose to make scholarly data
more sustainably accessible by leveraging Wikidata's infrastructure and
automating its population in a sustainable manner through LLMs by tapping into
unstructured sources like conference Web sites and proceedings texts as well as
already existing structured conference datasets. While an initial analysis
shows that Semantic Web conferences are only minimally represented in Wikidata,
we argue that our methodology can help to populate, evolve and maintain
scholarly data as a community within Wikidata. Our main contributions include
(a) an analysis of ontologies for representing scholarly data to identify gaps
and relevant entities/properties in Wikidata, (b) semi-automated extraction --
requiring (minimal) manual validation -- of conference metadata (e.g.,
acceptance rates, organizer roles, programme committee members, best paper
awards, keynotes, and sponsors) from websites and proceedings texts using LLMs.
Finally, we discuss (c) extensions to visualization tools in the Wikidata
context for data exploration of the generated scholarly data. Our study focuses
on data from 105 Semantic Web-related conferences and extends/adds more than
6000 entities in Wikidata. It is important to note that the method can be more
generally applicable beyond Semantic Web-related conferences for enhancing
Wikidata's utility as a comprehensive scholarly resource.
  Source Repository: https://github.com/scholarly-wikidata/
  DOI: https://doi.org/10.5281/zenodo.10989709
  License: Creative Commons CC0 (Data), MIT (Code)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted at EKAW-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Corrective Machine Unranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingrui Hou, Axel Finke, Georgina Cosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning in neural information retrieval (IR) systems requires
removing specific data whilst maintaining model performance. Applying existing
machine unlearning methods to IR may compromise retrieval effectiveness or
inadvertently expose unlearning actions due to the removal of particular items
from the retrieved results presented to users. We formalise corrective
unranking, which extends machine unlearning in (neural) IR context by
integrating substitute documents to preserve ranking integrity, and propose a
novel teacher-student framework, Corrective unRanking Distillation (CuRD), for
this task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR
model such that its output relevance scores of to-be-forgotten samples mimic
those of low-ranking, non-retrievable samples; (2) enables correction by
fine-tuning the relevance scores for the substitute samples to match those of
corresponding to-be-forgotten samples closely; (3) seeks to preserve
performance on samples that are not targeted for forgetting. We evaluate CuRD
on four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and
TREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the
training dataset demonstrate that CuRD outperforms seven state-of-the-art
baselines in terms of forgetting and correction while maintaining model
retention and generalisation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Information Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multimodal Query Representation via Visual Dialogues for
  End-to-End Knowledge Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multimodal retrieval systems often rely on disjointed models for
image comprehension, such as object detectors and caption generators, leading
to cumbersome implementations and training processes. To overcome this
limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a
text retriever with the ability to understand multimodal queries via dynamic
modality interaction. Ret-XKnow leverages a partial convolution mechanism to
focus on visual information relevant to the given textual query, thereby
enhancing multimodal query representations. To effectively learn multimodal
interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset
automatically constructed from visual dialogue datasets. Our dataset
construction process ensures that the dialogues are transformed into suitable
information retrieval tasks using a text retriever. We demonstrate that our
approach not only significantly improves retrieval performance in zero-shot
settings but also achieves substantial improvements in fine-tuning scenarios.
Our code is publicly available: https://github.com/yeongjoonJu/Ret_XKnow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Study of Relevance Assessments with <span class="highlight-title">Large Language Model</span>s:
  An Initial Look 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of large language models to provide relevance assessments
presents exciting opportunities to advance information retrieval, natural
language processing, and beyond, but to date many unknowns remain. This paper
reports on the results of a large-scale evaluation (the TREC 2024 RAG Track)
where four different relevance assessment approaches were deployed in situ: the
"standard" fully manual process that NIST has implemented for decades and three
different alternatives that take advantage of LLMs to different extents using
the open-source UMBRELA tool. This setup allows us to correlate system rankings
induced by the different approaches to characterize tradeoffs between cost and
quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system
rankings induced by automatically generated relevance assessments from UMBRELA
correlate highly with those induced by fully manual assessments across a
diverse set of 77 runs from 19 teams. Our results suggest that automatically
generated UMBRELA judgments can replace fully manual judgments to accurately
capture run-level effectiveness. Surprisingly, we find that LLM assistance does
not appear to increase correlation with fully manual assessments, suggesting
that costs associated with human-in-the-loop processes do not bring obvious
tangible benefits. Overall, human assessors appear to be stricter than UMBRELA
in applying relevance criteria. Our work validates the use of LLMs in academic
TREC-style evaluations and provides the foundation for future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Model Prior Overcomes Cold-Start Items 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Wang, Hao Ding, Yupeng Gu, Sergul Aydore, Kousha Kalantari, Branislav Kveton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of recommender systems (RecSys) is driven by digitization and the
need for personalized content in areas such as e-commerce and video streaming.
The content in these systems often changes rapidly and therefore they
constantly face the ongoing cold-start problem, where new items lack
interaction data and are hard to value. Existing solutions for the cold-start
problem, such as content-based recommenders and hybrid methods, leverage item
metadata to determine item similarities. The main challenge with these methods
is their reliance on structured and informative metadata to capture detailed
item similarities, which may not always be available. This paper introduces a
novel approach for cold-start item recommendation that utilizes the language
model (LM) to estimate item similarities, which are further integrated as a
Bayesian prior with classic recommender systems. This approach is generic and
able to boost the performance of various recommenders. Specifically, our
experiments integrate it with both sequential and collaborative filtering-based
recommender and evaluate it on two real-world datasets, demonstrating the
enhanced performance of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is dedicated to cold-start item recommendation using
  language-model priors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Need in Metaverse Recordings - A Field Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Steinert, Jan Mischkies, Stefan Wagenpfeil, Ingo Frommholz, Matthias L. Hemmje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse Recordings (MVRs) represent an emerging and underexplored media
type within the field of Multimedia Information Retrieval (MMIR). This paper
presents findings from a field study aimed at understanding the users
information needs and search behaviors specific to MVR retrieval. By conducting
and analyzing expert interviews, the study identifies application scenarios and
highlights challenges in retrieving multimedia content from the metaverse. The
results reveal existing application scenarios of MVRs and confirm the relevance
of capturing time-series data from the graphical rendering process and related
input-output devices, which are also highly relevant to user needs.
Furthermore, the study provides a foundation for developing retrieval systems
tailored to MVRs by defining use cases, user stereotypes, and specific
requirements for MVR Retrieval systems. The findings contribute to a better
understanding of information search behaviors in MVR Retrieval and pave the way
for future research and system design in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Need in Metaverse Recordings -- A Field Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Steinert, Jan Mischkies, Stefan Wagenpfeil, Ingo Frommholz, Matthias L. Hemmje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse Recordings (MVRs) represent an emerging and underexplored media
type within the field of Multimedia Information Retrieval (MMIR). This paper
presents findings from a field study aimed at understanding the users
information needs and search behaviors specific to MVR retrieval. By conducting
and analyzing expert interviews, the study identifies application scenarios and
highlights challenges in retrieving multimedia content from the metaverse. The
results reveal existing application scenarios of MVRs and confirm the relevance
of capturing time-series data from the graphical rendering process and related
input-output devices, which are also highly relevant to user needs.
Furthermore, the study provides a foundation for developing retrieval systems
tailored to MVRs by defining use cases, user stereotypes, and specific
requirements for MVR Retrieval systems. The findings contribute to a better
understanding of information search behaviors in MVR Retrieval and pave the way
for future research and system design in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Enrichment-Driven GrAph Reasoner (EDGAR) for Large Knowledge
  Graphs with Applications in Drug Repurposing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olawumi Olasunkanmi, Evan Morris, Yaphet Kebede, Harlin Lee, Stanley Ahalt, Alexander Tropsha, Chris Bizon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) represent connections and relationships between
real-world entities. We propose a link prediction framework for KGs named
Enrichment-Driven GrAph Reasoner (EDGAR), which infers new edges by mining
entity-local rules. This approach leverages enrichment analysis, a
well-established statistical method used to identify mechanisms common to sets
of differentially expressed genes. EDGAR's inference results are inherently
explainable and rankable, with p-values indicating the statistical significance
of each enrichment-based rule.
  We demonstrate the framework's effectiveness on a large-scale biomedical KG,
ROBOKOP, focusing on drug repurposing for Alzheimer disease (AD) as a case
study. Initially, we extracted 14 known drugs from the KG and identified 20
contextual biomarkers through enrichment analysis, revealing functional
pathways relevant to shared drug efficacy for AD. Subsequently, using the top
1000 enrichment results, our system identified 1246 additional drug candidates
for AD treatment. The top 10 candidates were validated using evidence from
medical literature.
  EDGAR is deployed within ROBOKOP, complete with a web user interface. This is
the first study to apply enrichment analysis to large graph completion and drug
repurposing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM4Steal: <span class="highlight-title">Diffusion</span> Model For Link Stealing Attack On Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Haonan Ma, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph has become increasingly integral to the advancement of recommendation
systems, particularly with the fast development of graph neural network(GNN).
By exploring the virtue of rich node features and link information, GNN is
designed to provide personalized and accurate suggestions. Meanwhile, the
privacy leakage of GNN in such contexts has also captured special attention.
Prior work has revealed that a malicious user can utilize auxiliary knowledge
to extract sensitive link data of the target graph, integral to recommendation
systems, via the decision made by the target GNN model. This poses a
significant risk to the integrity and confidentiality of data used in
recommendation system. Though important, previous works on GNN's privacy
leakage are still challenged in three aspects, i.e., limited stealing attack
scenarios, sub-optimal attack performance, and adaptation against defense. To
address these issues, we propose a diffusion model based link stealing attack,
named DM4Steal. It differs previous work from three critical aspects. (i)
Generality: aiming at six attack scenarios with limited auxiliary knowledge, we
propose a novel training strategy for diffusion models so that DM4Steal is
transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from
the retention of semantic structure in the diffusion model during the training
process, DM4Steal is capable to learn the precise topology of the target graph
through the GNN decision process. (iii) Adaptation: when GNN is defensive
(e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling
the score model multiple times to keep performance degradation to a minimum,
thus DM4Steal implements successful adaptive attack on defensive GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We found that there were critical problems in our paper, and we
  needed to redo the experiment, which was incomplete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Interaction Fusion Self-<span class="highlight-title">Distill</span>ation Network For CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sang, Qiuze Ru, Honghao Li, Yiwen Zhang, Qian Cao, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction plays a vital role in recommender
systems, online advertising, and search engines. Most of the current approaches
model feature interactions through stacked or parallel structures, with some
employing knowledge distillation for model compression. However, we observe
some limitations with these approaches: (1) In parallel structure models, the
explicit and implicit components are executed independently and simultaneously,
which leads to insufficient information sharing within the feature set. (2) The
introduction of knowledge distillation technology brings about the problems of
complex teacher-student framework design and low knowledge transfer efficiency.
(3) The dataset and the process of constructing high-order feature interactions
contain significant noise, which limits the model's effectiveness. To address
these limitations, we propose FSDNet, a CTR prediction framework incorporating
a plug-and-play fusion self-distillation module. Specifically, FSDNet forms
connections between explicit and implicit feature interactions at each layer,
enhancing the sharing of information between different features. The deepest
fusion layer is then used as the teacher model, utilizing self-distillation to
guide the training of shallow layers. Empirical evaluation across four
benchmark datasets validates the framework's efficacy and generalization
capabilities. The code is available on
https://anonymous.4open.science/r/FSDNet.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">175</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Short Note on Evaluating RepNet for <span class="highlight-title">Temporal</span> Repetition Counting in
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss some consistent issues on how RepNet has been evaluated in various
papers. As a way to mitigate these issues, we report RepNet performance results
on different datasets, and release evaluation code and the RepNet checkpoint to
obtain these results. Code URL:
https://github.com/google-research/google-research/blob/master/repnet/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Limited Impact of Medical Adaptation of Large Language and
  Vision-<span class="highlight-title">Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare ten
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting and supervised fine-tuning regimes for medical question-answering
(QA). For instance, across all tasks and model pairs we consider in the 3-shot
setting, medical LLMs only outperform their base models in 22.7% of cases,
reach a (statistical) tie in 36.8% of cases, and are significantly worse than
their base models in the remaining 40.5% of cases. Our conclusions are based on
(i) comparing each medical model head-to-head, directly against the
corresponding base model; (ii) optimizing the prompts for each model separately
in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty
in comparisons. While these basic practices are not consistently adopted in the
literature, our ablations show that they substantially impact conclusions.
Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs
can show performance improvements, but the benefits do not carry over to tasks
based on clinical notes. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes
  additional results on clinical note QA tasks and supervised fine-tuning
  evaluations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Parameter-free Outlier Detection using HDBSCAN* Outlier
  Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushankur Ghosh, Murilo Coelho Naldi, Jörg Sander, Euijin Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning and data mining, outliers are data points that
significantly differ from the dataset and often introduce irrelevant
information that can induce bias in its statistics and models. Therefore,
unsupervised methods are crucial to detect outliers if there is limited or no
information about them. Global-Local Outlier Scores based on Hierarchies
(GLOSH) is an unsupervised outlier detection method within HDBSCAN*, a
state-of-the-art hierarchical clustering method. GLOSH estimates outlier scores
for each data point by comparing its density to the highest density of the
region they reside in the HDBSCAN* hierarchy. GLOSH may be sensitive to
HDBSCAN*'s minpts parameter that influences density estimation. With limited
knowledge about the data, choosing an appropriate minpts value beforehand is
challenging as one or some minpts values may better represent the underlying
cluster structure than others. Additionally, in the process of searching for
``potential outliers'', one has to define the number of outliers n a dataset
has, which may be impractical and is often unknown. In this paper, we propose
an unsupervised strategy to find the ``best'' minpts value, leveraging the
range of GLOSH scores across minpts values to identify the value for which
GLOSH scores can best identify outliers from the rest of the dataset. Moreover,
we propose an unsupervised strategy to estimate a threshold for classifying
points into inliers and (potential) outliers without the need to pre-define any
value. Our experiments show that our strategies can automatically find the
minpts value and threshold that yield the best or near best outlier detection
results using GLOSH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Conference on Big Data, IEEE BigData
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>Stinger: Jailbreaking <span class="highlight-title">LLM</span>s using RL fine-tuned <span class="highlight-title">LLM</span>s <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Jha, Arnav Arora, Vijay Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLMStinger, a novel approach that leverages Large Language
Models (LLMs) to automatically generate adversarial suffixes for jailbreak
attacks. Unlike traditional methods, which require complex prompt engineering
or white-box access, LLMStinger uses a reinforcement learning (RL) loop to
fine-tune an attacker LLM, generating new suffixes based on existing attacks
for harmful questions from the HarmBench benchmark. Our method significantly
outperforms existing red-teaming approaches (we compared against 15 of the
latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on
LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for
their extensive safety measures. Additionally, we achieved a 94.97% ASR on
GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability
of LLMStinger across open and closed-source models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interaction Testing in Variation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drago Plecko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relationships of cause and effect are of prime importance for explaining
scientific phenomena. Often, rather than just understanding the effects of
causes, researchers also wish to understand how a cause $X$ affects an outcome
$Y$ mechanistically -- i.e., what are the causal pathways that are activated
between $X$ and $Y$. For analyzing such questions, a range of methods has been
developed over decades under the rubric of causal mediation analysis.
Traditional mediation analysis focuses on decomposing the average treatment
effect (ATE) into direct and indirect effects, and therefore focuses on the ATE
as the central quantity. This corresponds to providing explanations for
associations in the interventional regime, such as when the treatment $X$ is
randomized. Commonly, however, it is of interest to explain associations in the
observational regime, and not just in the interventional regime. In this paper,
we introduce \text{variation analysis}, an extension of mediation analysis that
focuses on the total variation (TV) measure between $X$ and $Y$, written as
$\mathrm{E}[Y \mid X=x_1] - \mathrm{E}[Y \mid X=x_0]$. The TV measure
encompasses both causal and confounded effects, as opposed to the ATE which
only encompasses causal (direct and mediated) variations. In this way, the TV
measure is suitable for providing explanations in the natural regime and
answering questions such as ``why is $X$ associated with $Y$?''. Our focus is
on decomposing the TV measure, in a way that explicitly includes direct,
indirect, and confounded variations. Furthermore, we also decompose the TV
measure to include interaction terms between these different pathways.
Subsequently, interaction testing is introduced, involving hypothesis tests to
determine if interaction terms are significantly different from zero. If
interactions are not significant, more parsimonious decompositions of the TV
measure can be used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oblique Bayesian additive regression trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul-Hieu V. Nguyen, Ryan Yee, Sameer K. Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current implementations of Bayesian Additive Regression Trees (BART) are
based on axis-aligned decision rules that recursively partition the feature
space using a single feature at a time. Several authors have demonstrated that
oblique trees, whose decision rules are based on linear combinations of
features, can sometimes yield better predictions than axis-aligned trees and
exhibit excellent theoretical properties. We develop an oblique version of BART
that leverages a data-adaptive decision rule prior that recursively partitions
the feature space along random hyperplanes. Using several synthetic and
real-world benchmark datasets, we systematically compared our oblique BART
implementation to axis-aligned BART and other tree ensemble methods, finding
that oblique BART was competitive with -- and sometimes much better than --
those methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Adaptation of Quadruped Locomotion using <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reece O'Mahoney, Alexander L. Mitchell, Wanming Yu, Ingmar Posner, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a diffusion-based approach to quadrupedal locomotion that
simultaneously addresses the limitations of learning and interpolating between
multiple skills and of (modes) offline adapting to new locomotion behaviours
after training. This is the first framework to apply classifier-free guided
diffusion to quadruped locomotion and demonstrate its efficacy by extracting
goal-conditioned behaviour from an originally unlabelled dataset. We show that
these capabilities are compatible with a multi-skill policy and can be applied
with little modification and minimal compute overhead, i.e., running entirely
on the robots onboard CPU. We verify the validity of our approach with hardware
experiments on the ANYmal quadruped platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model agnostic local variable importance for locally dependent
  relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelvyn K. Bladen, Adele Cutler, D. Richard Cutler, Kevin R. Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global variable importance measures are commonly used to interpret machine
learning model results. Local variable importance techniques assess how
variables contribute to individual observations rather than the entire dataset.
Current methods typically fail to accurately reflect locally dependent
relationships between variables and instead focus on marginal importance
values. Additionally, they are not natively adapted for multi-class
classification problems. We propose a new model-agnostic method for calculating
local variable importance, CLIQUE, that captures locally dependent
relationships, contains improvements over permutation-based methods, and can be
directly applied to multi-class classification problems. Simulated and
real-world examples show that CLIQUE emphasizes locally dependent information
and properly reduces bias in regions where variables do not affect the
response.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process-aware Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zheng, Petros Papapanagiotou, Jacques D. Fleuriot, Jane Hillston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally follow distinct patterns when conducting their daily
activities, which are driven by established practices and processes, such as
production workflows, social norms and daily routines. Human activity
recognition (HAR) algorithms usually use neural networks or machine learning
techniques to analyse inherent relationships within the data. However, these
approaches often overlook the contextual information in which the data are
generated, potentially limiting their effectiveness. We propose a novel
approach that incorporates process information from context to enhance the HAR
performance. Specifically, we align probabilistic events generated by machine
learning models with process models derived from contextual information. This
alignment adaptively weighs these two sources of information to optimise HAR
accuracy. Our experiments demonstrate that our approach achieves better
accuracy and Macro F1-score compared to baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinRobot: AI <span class="highlight-title">Agent</span> for Equity Research and Valuation with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zhou, Pinqiao Wang, Yilin Wu, Hongyang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As financial markets grow increasingly complex, there is a rising need for
automated tools that can effectively assist human analysts in equity research,
particularly within sell-side research. While Generative AI (GenAI) has
attracted significant attention in this field, existing AI solutions often fall
short due to their narrow focus on technical factors and limited capacity for
discretionary judgment. These limitations hinder their ability to adapt to new
data in real-time and accurately assess risks, which diminishes their practical
value for investors.
  This paper presents FinRobot, the first AI agent framework specifically
designed for equity research. FinRobot employs a multi-agent Chain of Thought
(CoT) system, integrating both quantitative and qualitative analyses to emulate
the comprehensive reasoning of a human analyst. The system is structured around
three specialized agents: the Data-CoT Agent, which aggregates diverse data
sources for robust financial integration; the Concept-CoT Agent, which mimics
an analysts reasoning to generate actionable insights; and the Thesis-CoT
Agent, which synthesizes these insights into a coherent investment thesis and
report. FinRobot provides thorough company analysis supported by precise
numerical data, industry-appropriate valuation metrics, and realistic risk
assessments. Its dynamically updatable data pipeline ensures that research
remains timely and relevant, adapting seamlessly to new financial information.
Unlike existing automated research tools, such as CapitalCube and Wright
Reports, FinRobot delivers insights comparable to those produced by major
brokerage firms and fundamental research vendors. We open-source FinRobot at
\url{https://github. com/AI4Finance-Foundation/FinRobot}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st Workshop on LLMs and Generative AI for Finance, ICAIF 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Accelerated Quantum Transport Simulations in
  Nanoelectronics: From Break Junctions to Field-Effect Transistors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jijie Zou, Zhanghao Zhouyin, Dongying Lin, Linfeng Zhang, Shimin Hou, Qiangqiang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum transport calculations are essential for understanding and designing
nanoelectronic devices, yet the trade-off between accuracy and computational
efficiency has long limited their practical applications. We present a general
framework that combines the deep learning tight-binding Hamiltonian (DeePTB)
approach with the non-equilibrium Green's Function (NEGF) method, enabling
efficient quantum transport calculations while maintaining first-principles
accuracy. We demonstrate the capabilities of the DeePTB-NEGF framework through
two representative applications: comprehensive simulation of break junction
systems, where conductance histograms show good agreement with experimental
measurements in both metallic contact and single-molecule junction cases; and
simulation of carbon nanotube field effect transistors through self-consistent
NEGF-Poisson calculations, capturing essential physics including the
electrostatic potential and transfer characteristic curves under finite bias
conditions. This framework bridges the gap between first-principles accuracy
and computational efficiency, providing a powerful tool for high-throughput
quantum transport simulations across different scales in nanoelectronics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity
  and Directional Convergence <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berfin Simsek, Amire Bendjeddou, Daniel Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on the gradient flow dynamics of a neural network model
that uses correlation loss to approximate a multi-index function on
high-dimensional standard Gaussian data. Specifically, the multi-index function
we consider is a sum of neurons $f^*(x) \!=\! \sum_{j=1}^k \! \sigma^*(v_j^T
x)$ where $v_1, \dots, v_k$ are unit vectors, and $\sigma^*$ lacks the first
and second Hermite polynomials in its Hermite expansion. It is known that, for
the single-index case ($k\!=\!1$), overcoming the search phase requires
polynomial time complexity. We first generalize this result to multi-index
functions characterized by vectors in arbitrary directions. After the search
phase, it is not clear whether the network neurons converge to the index
vectors, or get stuck at a sub-optimal solution. When the index vectors are
orthogonal, we give a complete characterization of the fixed points and prove
that neurons converge to the nearest index vectors. Therefore, using $n \!
\asymp \! k \log k$ neurons ensures finding the full set of index vectors with
gradient flow with high probability over random initialization. When $ v_i^T
v_j \!=\! \beta \! \geq \! 0$ for all $i \neq j$, we prove the existence of a
sharp threshold $\beta_c \!=\! c/(c+k)$ at which the fixed point that computes
the average of the index vectors transitions from a saddle point to a minimum.
Numerical simulations show that using a correlation loss and a mild
overparameterization suffices to learn all of the index vectors when they are
nearly orthogonal, however, the correlation loss fails when the dot product
between the index vectors exceeds a certain threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, under review by AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Private Sampling with Public Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnoosh Zamanlooy, Mario Diaz, Shahab Asoodeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local differential privacy (LDP) is increasingly employed in
privacy-preserving machine learning to protect user data before sharing it with
an untrusted aggregator. Most LDP methods assume that users possess only a
single data record, which is a significant limitation since users often gather
extensive datasets (e.g., images, text, time-series data) and frequently have
access to public datasets. To address this limitation, we propose a locally
private sampling framework that leverages both the private and public datasets
of each user. Specifically, we assume each user has two distributions: $p$ and
$q$ that represent their private dataset and the public dataset, respectively.
The objective is to design a mechanism that generates a private sample
approximating $p$ while simultaneously preserving $q$. We frame this objective
as a minimax optimization problem using $f$-divergence as the utility measure.
We fully characterize the minimax optimal mechanisms for general
$f$-divergences provided that $p$ and $q$ are discrete distributions.
Remarkably, we demonstrate that this optimal mechanism is universal across all
$f$-divergences. Experiments validate the effectiveness of our minimax optimal
sampler compared to the state-of-the-art locally private sampler.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can sparse autoencoders be used to decompose and interpret steering
  vectors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Mayne, Yushi Yang, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steering vectors are a promising approach to control the behaviour of large
language models. However, their underlying mechanisms remain poorly understood.
While sparse autoencoders (SAEs) may offer a potential method to interpret
steering vectors, recent findings show that SAE-reconstructed vectors often
lack the steering properties of the original vectors. This paper investigates
why directly applying SAEs to steering vectors yields misleading
decompositions, identifying two reasons: (1) steering vectors fall outside the
input distribution for which SAEs are designed, and (2) steering vectors can
have meaningful negative projections in feature directions, which SAEs are not
designed to accommodate. These limitations hinder the direct use of SAEs for
interpreting steering vectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shabarish Chenakkod, Michał Dereziński, Xiaoyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An oblivious subspace embedding is a random $m\times n$ matrix $\Pi$ such
that, for any $d$-dimensional subspace, with high probability $\Pi$ preserves
the norms of all vectors in that subspace within a $1\pm\epsilon$ factor. In
this work, we give an oblivious subspace embedding with the optimal dimension
$m=\Theta(d/\epsilon^2)$ that has a near-optimal sparsity of $\tilde
O(1/\epsilon)$ non-zero entries per column of $\Pi$. This is the first result
to nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the
best sparsity attainable by an optimal oblivious subspace embedding, improving
on a prior bound of $\tilde O(1/\epsilon^6)$ non-zeros per column [Chenakkod et
al., STOC 2024]. We further extend our approach to the non-oblivious setting,
proposing a new family of Leverage Score Sparsified embeddings with Independent
Columns, which yield faster runtimes for matrix approximation and regression
tasks.
  In our analysis, we develop a new method which uses a decoupling argument
together with the cumulant method for bounding the edge universality error of
isotropic random matrices. To achieve near-optimal sparsity, we combine this
general-purpose approach with new traces inequalities that leverage the
specific structure of our subspace embedding construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Methane -- The Impact of Dairy Farm Practices on Emissions
  Through Satellite Data and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Bi, Suresh Neethirajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the correlation between dairy farm characteristics
and methane concentrations as derived from satellite observations in Eastern
Canada. Utilizing data from 11 dairy farms collected between January 2020 and
December 2022, we integrated Sentinel-5P satellite methane data with critical
farm-level attributes, including herd genetics, feeding practices, and
management strategies. Initial analyses revealed significant correlations with
methane concentrations, leading to the application of Variance Inflation Factor
(VIF) and Principal Component Analysis (PCA) to address multicollinearity and
enhance model stability. Subsequently, machine learning models - specifically
Random Forest and Neural Networks - were employed to evaluate feature
importance and predict methane emissions. Our findings indicate a strong
negative correlation between the Estimated Breeding Value (EBV) for protein
percentage and methane concentrations, suggesting that genetic selection for
higher milk protein content could be an effective strategy for emissions
reduction. The integration of atmospheric transport models with satellite data
further refined our emission estimates, significantly enhancing accuracy and
spatial resolution. This research underscores the potential of advanced
satellite monitoring, machine learning techniques, and atmospheric modeling in
improving methane emission assessments within the dairy sector. It emphasizes
the critical role of farm-specific characteristics in developing effective
mitigation strategies. Future investigations should focus on expanding the
dataset and incorporating inversion modeling for more precise emission
quantification. Balancing ecological impacts with economic viability will be
essential for fostering sustainable dairy farming practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow reconstruction in time-varying geometries using graph neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bogdan A. Danciu, Vito A. Pagone, Benjamin Böhm, Marius Schmidt, Christos E. Frouzakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a Graph Attention Convolutional Network (GACN) for flow
reconstruction from very sparse data in time-varying geometries. The model
incorporates a feature propagation algorithm as a preprocessing step to handle
extremely sparse inputs, leveraging information from neighboring nodes to
initialize missing features. In addition, a binary indicator is introduced as a
validity mask to distinguish between the original and propagated data points,
enabling more effective learning from sparse inputs. Trained on a unique data
set of Direct Numerical Simulations (DNS) of a motored engine at a technically
relevant operating condition, the GACN shows robust performance across
different resolutions and domain sizes and can effectively handle unstructured
data and variable input sizes. The model is tested on previously unseen DNS
data as well as on an experimental data set from Particle Image Velocimetry
(PIV) measurements that were not considered during training. A comparative
analysis shows that the GACN consistently outperforms both a conventional
Convolutional Neural Network (CNN) and cubic interpolation methods on the DNS
and PIV test sets by achieving lower reconstruction errors and better capturing
fine-scale turbulent structures. In particular, the GACN effectively
reconstructs flow fields from domains up to 14 times larger than those observed
during training, with the performance advantage increasing for larger domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Dissipation Preserving Physics Informed Neural Network for
  Allen-Cahn Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Kütük, Hamdullah Yücel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a numerical solution of Allen-Cahn equation with
constant and degenerate mobility, with polynomial and logarithmic energy
functionals, with deterministic and random initial functions, and with
advective term in one, two, and three spatial dimensions, based on the
physics-informed neural network (PINN). To improve the learning capacity of the
PINN, we incorporate the energy dissipation property of the Allen-Cahn equation
as a penalty term into the loss function of the network. To facilitate the
learning process of random initials, we employ a continuous analogue of the
initial random condition by utilizing the Fourier series expansion. Adaptive
methods from traditional numerical analysis are also integrated to enhance the
effectiveness of the proposed PINN. Numerical results indicate a consistent
decrease in the discrete energy, while also revealing phenomena such as phase
separation and metastability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaleNet: Scale Invariance Learning in Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Jiang, Chengjia Wang, Michael Lones, Wei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have advanced relational data analysis but lack
invariance learning techniques common in image classification. In node
classification with GNNs, it is actually the ego-graph of the center node that
is classified. This research extends the scale invariance concept to node
classification by drawing an analogy to image processing: just as scale
invariance being used in image classification to capture multi-scale features,
we propose the concept of ``scaled ego-graphs''. Scaled ego-graphs generalize
traditional ego-graphs by replacing undirected single-edges with
``scaled-edges'', which are ordered sequences of multiple directed edges. We
empirically assess the performance of the proposed scale invariance in graphs
on seven benchmark datasets, across both homophilic and heterophilic
structures. Our scale-invariance-based graph learning outperforms inception
models derived from random walks by being simpler, faster, and more accurate.
The scale invariance explains inception models' success on homophilic graphs
and limitations on heterophilic graphs. To ensure applicability of inception
model to heterophilic graphs as well, we further present ScaleNet, an
architecture that leverages multi-scaled features. ScaleNet achieves
state-of-the-art results on five out of seven datasets (four homophilic and one
heterophilic) and matches top performance on the remaining two, demonstrating
its excellent applicability. This represents a significant advance in graph
learning, offering a unified framework that enhances node classification across
various graph types. Our code is available at
https://github.com/Qin87/ScaleNet/tree/July25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Scale invariance in node classification is demonstrated and applied
  in graph transformation to develop ScaleNet, which achieves state-of-the-art
  performance on both homophilic and heterophilic directed graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised Anomaly Detection in Surveillance Videos Based on
  Two-Stream I3D Convolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sareh Soltani Nejad, Anwar Haque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread implementation of urban surveillance systems has necessitated
more sophisticated techniques for anomaly detection to ensure enhanced public
safety. This paper presents a significant advancement in the field of anomaly
detection through the application of Two-Stream Inflated 3D (I3D) Convolutional
Networks. These networks substantially outperform traditional 3D Convolutional
Networks (C3D) by more effectively extracting spatial and temporal features
from surveillance videos, thus improving the precision of anomaly detection.
Our research advances the field by implementing a weakly supervised learning
framework based on Multiple Instance Learning (MIL), which uniquely
conceptualizes surveillance videos as collections of 'bags' that contain
instances (video clips). Each instance is innovatively processed through a
ranking mechanism that prioritizes clips based on their potential to display
anomalies. This novel strategy not only enhances the accuracy and precision of
anomaly detection but also significantly diminishes the dependency on extensive
manual annotations. Moreover, through meticulous optimization of model
settings, including the choice of optimizer, our approach not only establishes
new benchmarks in the performance of anomaly detection systems but also offers
a scalable and efficient solution for real-world surveillance applications.
This paper contributes significantly to the field of computer vision by
delivering a more adaptable, efficient, and context-aware anomaly detection
system, which is poised to redefine practices in urban surveillance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Transport-Based Displacement Interpolation with Data
  Augmentation for Reduced Order Modeling of Nonlinear Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moaad Khamlich, Federico Pichi, Michele Girfoglio, Annalisa Quaini, Gianluigi Rozza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel reduced-order Model (ROM) that leverages optimal transport
(OT) theory and displacement interpolation to enhance the representation of
nonlinear dynamics in complex systems. While traditional ROM techniques face
challenges in this scenario, especially when data (i.e., observational
snapshots) is limited, our method addresses these issues by introducing a data
augmentation strategy based on OT principles. The proposed framework generates
interpolated solutions tracing geodesic paths in the space of probability
distributions, enriching the training dataset for the ROM. A key feature of our
approach is its ability to provide a continuous representation of the
solution's dynamics by exploiting a virtual-to-real time mapping. This enables
the reconstruction of solutions at finer temporal scales than those provided by
the original data. To further improve prediction accuracy, we employ Gaussian
Process Regression to learn the residual and correct the representation between
the interpolated snapshots and the physical solution. We demonstrate the
effectiveness of our methodology with atmospheric mesoscale benchmarks
characterized by highly nonlinear, advection-dominated dynamics. Our results
show improved accuracy and efficiency in predicting complex system behaviors,
indicating the potential of this approach for a wide range of applications in
computational physics and engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Comparisons Between Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko H. Schütt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Which neural networks are similar is a fundamental question for both machine
learning and neuroscience. Our novel method compares representations based on
Bayesian statistics about linear readouts from the representations. Concretely,
we suggest to use the total variation distance or Jensen-Shannon distance
between prior predictive distributions to compare representations. The prior
predictive distribution is a full description of the inductive bias and
generalization of a model in Bayesian statistics, making it a great basis for
comparisons. As Jensen-Shannon distance and total variation distance are
metrics our dissimilarity measures are pseudo-metrics for representations. For
a linear readout, our metrics just depend on the linear kernel matrix of the
representations. Thus, our metrics connects linear read-out based comparisons
to kernel based metrics like centered kernel alignment and representational
similarity analysis. We apply our new metrics to deep neural networks trained
on ImageNet-1k. Our new metrics can be computed efficiently including a
stochastic gradient without dimensionality reductions of the representations.
It broadly agrees with existing metrics, but is more stringent. It varies less
across different random image samples, and it measures how well two
representations could be distinguished based on a linear read out. Thus our
metric nicely extends our toolkit for comparing representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender systems and reinforcement learning for building control and
  occupant interaction: A text-mining driven <span class="highlight-title">review</span> of scientific literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhang, Matias Quintana, Clayton Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The indoor environment greatly affects health and well-being; enhancing
health and reducing energy use in these settings is a key research focus. With
advancing Information and Communication Technology (ICT), recommendation
systems and reinforcement learning have emerged as promising methods to induce
behavioral changes that improve indoor environments and building energy
efficiency. This study employs text-mining and Natural Language Processing
(NLP) to examine these approaches in building control and occupant interaction.
Analyzing approximately 27,000 articles from the ScienceDirect database, we
found extensive use of recommendation systems and reinforcement learning for
space optimization, location recommendations, and personalized control
suggestions. Despite broad applications, their use in optimizing indoor
environments and energy efficiency is limited. Traditional recommendation
algorithms are commonly used, but optimizing indoor conditions and energy
efficiency often requires advanced machine learning techniques like
reinforcement and deep learning. This review highlights the potential for
expanding recommender systems and reinforcement learning applications in
buildings and indoor environments. Areas for innovation include predictive
maintenance, building-related product recommendations, and optimizing
environments for specific needs like sleep and productivity enhancements based
on user feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching Latent Program Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Bonnet, Matthew V Macfarlane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis methods aim to automatically generate programs restricted
to a language that can explain a given specification of input-output pairs.
While purely symbolic approaches suffer from a combinatorial search space,
recent methods leverage neural networks to learn distributions over program
structures to narrow this search space significantly, enabling more efficient
search. However, for challenging problems, it remains difficult to train models
to perform program synthesis in one shot, making test-time search essential.
Most neural methods lack structured search mechanisms during inference, relying
instead on stochastic sampling or gradient updates, which can be inefficient.
In this work, we propose the Latent Program Network (LPN), a general algorithm
for program induction that learns a distribution over latent programs in a
continuous space, enabling efficient search and test-time adaptation. We
explore how to train these networks to optimize for test-time computation and
demonstrate the use of gradient-based search both during training and at test
time. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluates
performance by generalizing programs to new inputs rather than explaining the
underlying specification. We show that LPN can generalize beyond its training
distribution and adapt to unseen tasks by utilizing test-time computation,
outperforming algorithms without test-time adaptation mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/clement-bonnet/lpn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Cong, Zhiling Sang, Hongwei Liu, Haoran Luo, Xin Wang, Hong Liang, Jie Hao, Xiaohui Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distinct characteristics of multiomics data, including complex
interactions within and across biological layers and disease heterogeneity
(e.g., heterogeneity in etiology and clinical symptoms), drive us to develop
novel designs to address unique challenges in multiomics prediction. In this
paper, we propose the multi-view knowledge transfer learning (MVKTrans)
framework, which transfers intra- and inter-omics knowledge in an adaptive
manner by reviewing data heterogeneity and suppressing bias transfer, thereby
enhancing classification performance. Specifically, we design a graph
contrastive module that is trained on unlabeled data to effectively learn and
transfer the underlying intra-omics patterns to the supervised task. This
unsupervised pretraining promotes learning general and unbiased representations
for each modality, regardless of the downstream tasks. In light of the varying
discriminative capacities of modalities across different diseases and/or
samples, we introduce an adaptive and bi-directional cross-omics distillation
module. This module automatically identifies richer modalities and facilitates
dynamic knowledge transfer from more informative to less informative omics,
thereby enabling a more robust and generalized integration. Extensive
experiments on four real biomedical datasets demonstrate the superior
performance and robustness of MVKTrans compared to the state-of-the-art. Code
and data are available at https://github.com/Yaolab-fantastic/MVKTrans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRACE: <span class="highlight-title">Transformer</span>-based Risk Assessment for Clinical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dionysis Christopoulos, Sotiris Spanos, Valsamis Ntouskos, Konstantinos Karantzalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),
a novel method for clinical risk assessment based on clinical data, leveraging
the self-attention mechanism for enhanced feature interaction and result
interpretation. Our approach is able to handle different data modalities,
including continuous, categorical and multiple-choice (checkbox) attributes.
The proposed architecture features a shared representation of the clinical data
obtained by integrating specialized embeddings of each data modality, enabling
the detection of high-risk individuals using Transformer encoder layers. To
assess the effectiveness of the proposed method, a strong baseline based on
non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method
outperforms various baselines widely used in the domain of clinical risk
assessment, while effectively handling missing values. In terms of
explainability, our Transformer-based method offers easily interpretable
results via attention weights, further enhancing the clinicians'
decision-making process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking negative sampling in content-based news recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Ângelo Rebelo, João Vinagre, Ivo Pereira, Álvaro Figueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News recommender systems are hindered by the brief lifespan of articles, as
they undergo rapid relevance decay. Recent studies have demonstrated the
potential of content-based neural techniques in tackling this problem. However,
these models often involve complex neural architectures and often lack
consideration for negative examples. In this study, we posit that the careful
sampling of negative examples has a big impact on the model's outcome. We
devise a negative sampling technique that not only improves the accuracy of the
model but also facilitates the decentralization of the recommendation system.
The experimental results obtained using the MIND dataset demonstrate that the
accuracy of the method under consideration can compete with that of
State-of-the-Art models. The utilization of the sampling technique is essential
in reducing model complexity and accelerating the training process, while
maintaining a high level of accuracy. Finally, we discuss how decentralized
models can help improve privacy and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSub: Introducing class-aware Subnetworks Fusion to Enhance
  Personalized Federated Learning in Ubiquitous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized Federated Learning is essential in AI-driven ubiquitous systems,
supporting the distributed development of models able to adapt to diverse and
evolving user behaviors while safeguarding privacy. Despite addressing
heterogeneous user data distributions in collaborative model training, existing
methods often face limitations balancing personalization and generalization,
oversimplifying user similarities, or relying heavily on global models. In this
paper, we propose FedSub, a novel federated approach designed to enhance
personalization through the use of class-aware prototypes and model
subnetworks. Prototypes serve as compact representations of user data,
clustered on the server to identify similarities based on specific label
patterns. Concurrently, subnetworks -- model components necessary to process
each class -- are extracted locally and fused by the server according to these
clusters, producing highly tailored model updates for each user. This
fine-grained, class-specific aggregation of clients' models allows FedSub to
capture the unique characteristics of individual user data patterns. The
effectiveness of FedSub is validated in three real-world scenarios
characterized by high data heterogeneity, derived from human activity
recognition and mobile health applications. Experimental evaluations
demonstrate FedSub's performance improvements with respect to the
state-of-the-art and significant advancements in personalization for ubiquitous
systems based on personal mobile and wearable devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Proceedings of the ACM on Interactive, Mobile, Wearable
  and Ubiquitous Technologies (IMWUT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring similarity between embedding spaces using induced neighborhood
  graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago F. Tavares, Fabio Ayres, Paris Smaragdis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning techniques have excelled at generating embedding spaces that
capture semantic similarities between items. Often these representations are
paired, enabling experiments with analogies (pairs within the same domain) and
cross-modality (pairs across domains). These experiments are based on specific
assumptions about the geometry of embedding spaces, which allow finding paired
items by extrapolating the positional relationships between embedding pairs in
the training dataset, allowing for tasks such as finding new analogies, and
multimodal zero-shot classification. In this work, we propose a metric to
evaluate the similarity between paired item representations. Our proposal is
built from the structural similarity between the nearest-neighbors induced
graphs of each representation, and can be configured to compare spaces based on
different distance metrics and on different neighborhood sizes. We demonstrate
that our proposal can be used to identify similar structures at different
scales, which is hard to achieve with kernel methods such as Centered Kernel
Alignment (CKA). We further illustrate our method with two case studies: an
analogy task using GloVe embeddings, and zero-shot classification in the
CIFAR-100 dataset using CLIP embeddings. Our results show that accuracy in both
analogy and zero-shot classification tasks correlates with the embedding
similarity. These findings can help explain performance differences in these
tasks, and may lead to improved design of paired-embedding models in the
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniMat: Unifying Materials Embeddings through Multi-modal Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janghoon Ock, Joseph Montoya, Daniel Schweigert, Linda Hung, Santosh K. Suram, Weike Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Materials science datasets are inherently heterogeneous and are available in
different modalities such as characterization spectra, atomic structures,
microscopic images, and text-based synthesis conditions. The advancements in
multi-modal learning, particularly in vision and language models, have opened
new avenues for integrating data in different forms. In this work, we evaluate
common techniques in multi-modal learning (alignment and fusion) in unifying
some of the most important modalities in materials science: atomic structure,
X-ray diffraction patterns (XRD), and composition. We show that structure graph
modality can be enhanced by aligning with XRD patterns. Additionally, we show
that aligning and fusing more experimentally accessible data formats, such as
XRD patterns and compositions, can create more robust joint embeddings than
individual modalities across various tasks. This lays the groundwork for future
studies aiming to exploit the full potential of multi-modal data in materials
science, facilitating more informed decision-making in materials design and
discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Quasi-Static Time Series Simulations with Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alban Puech, François Mirallès, Jonas Weiss, Vincent Mai, Alexandre Blondin Massé, Martin de Montigny, Thomas Brunschwiler, Hendrik F. Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quasi-static time series (QSTS) simulations have great potential for
evaluating the grid's ability to accommodate the large-scale integration of
distributed energy resources. However, as grids expand and operate closer to
their limits, iterative power flow solvers, central to QSTS simulations, become
computationally prohibitive and face increasing convergence issues. Neural
power flow solvers provide a promising alternative, speeding up power flow
computations by 3 to 4 orders of magnitude, though they are costly to train. In
this paper, we envision how recently introduced grid foundation models could
improve the economic viability of neural power flow solvers. Conceptually,
these models amortize training costs by serving as a foundation for a range of
grid operation and planning tasks beyond power flow solving, with only minimal
fine-tuning required. We call for collaboration between the AI and power grid
communities to develop and open-source these models, enabling all operators,
even those with limited resources, to benefit from AI without building
solutions from scratch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contributors: A.P. and F.M.; Lead contact: A.P</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating unknown parameters in differential equations with a
  reinforcement learning based PSO method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkui Sun, Xiaoya Fan, Lijuan Jia, Tinyi Chu, Shing-Tung Yau, Rongling Wu, Zhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential equations offer a foundational yet powerful framework for
modeling interactions within complex dynamic systems and are widely applied
across numerous scientific fields. One common challenge in this area is
estimating the unknown parameters of these dynamic relationships. However,
traditional numerical optimization methods rely on the selection of initial
parameter values, making them prone to local optima. Meanwhile, deep learning
and Bayesian methods require training models on specific differential
equations, resulting in poor versatility. This paper reformulates the parameter
estimation problem of differential equations as an optimization problem by
introducing the concept of particles from the particle swarm optimization
algorithm. Building on reinforcement learning-based particle swarm optimization
(RLLPSO), this paper proposes a novel method, DERLPSO, for estimating unknown
parameters of differential equations. We compared its performance on three
typical ordinary differential equations with the state-of-the-art methods,
including the RLLPSO algorithm, traditional numerical methods, deep learning
approaches, and Bayesian methods. The experimental results demonstrate that our
DERLPSO consistently outperforms other methods in terms of performance,
achieving an average Mean Square Error of 1.13e-05, which reduces the error by
approximately 4 orders of magnitude compared to other methods. Apart from
ordinary differential equations, our DERLPSO also show great promise for
estimating unknown parameters of partial differential equations. The DERLPSO
method proposed in this paper has high accuracy, is independent of initial
parameter values, and possesses strong versatility and stability. This work
provides new insights into unknown parameter estimation for differential
equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Secure Intelligent O-RAN Architecture: Vulnerabilities, Threats
  and Promising Technical Solutions using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojdeh Karbalaee Motalleb, Chafika Benzaid, Tarik Taleb, Marcos Katz, Vahid Shah-Mansouri, JaeSeung Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of wireless communication systems will be fundamentally
impacted by an open radio access network (O-RAN), a new concept defining an
intelligent architecture with enhanced flexibility, openness, and the ability
to slice services more efficiently. For all its promises, and like any
technological advancement, O-RAN is not without risks that need to be carefully
assessed and properly addressed to accelerate its wide adoption in future
mobile networks. In this paper, we present an in-depth security analysis of the
O-RAN architecture, discussing the potential threats that may arise in the
different O-RAN architecture layers and their impact on the Confidentiality,
Integrity, and Availability (CIA) triad. We also promote the potential of zero
trust, Moving Target Defense (MTD), blockchain, and large language models(LLM)
technologies in fortifying O-RAN's security posture. Furthermore, we
numerically demonstrate the effectiveness of MTD in empowering robust deep
reinforcement learning methods for dynamic network slice admission control in
the O-RAN architecture. Moreover, we examine the effect of explainable AI (XAI)
based on LLMs in securing the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Mixture Models Based Augmentation Enhances GNN Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Amine Mohamed Aboussalah, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown great promise in tasks like node and
graph classification, but they often struggle to generalize, particularly to
unseen or out-of-distribution (OOD) data. These challenges are exacerbated when
training data is limited in size or diversity. To address these issues, we
introduce a theoretical framework using Rademacher complexity to compute a
regret bound on the generalization error and then characterize the effect of
data augmentation. This framework informs the design of GMM-GDA, an efficient
graph data augmentation (GDA) algorithm leveraging the capability of Gaussian
Mixture Models (GMMs) to approximate any distribution. Our approach not only
outperforms existing augmentation techniques in terms of generalization but
also offers improved time complexity, making it highly suitable for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot See, Robot Do: Imitation Reward for Noisy Financial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sven Goluža, Tomislav Kovačević, Stjepan Begušić, Zvonko Kostanjčar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sequential nature of decision-making in financial asset trading aligns
naturally with the reinforcement learning (RL) framework, making RL a common
approach in this domain. However, the low signal-to-noise ratio in financial
markets results in noisy estimates of environment components, including the
reward function, which hinders effective policy learning by RL agents. Given
the critical importance of reward function design in RL problems, this paper
introduces a novel and more robust reward function by leveraging imitation
learning, where a trend labeling algorithm acts as an expert. We integrate
imitation (expert's) feedback with reinforcement (agent's) feedback in a
model-free RL algorithm, effectively embedding the imitation learning problem
within the RL paradigm to handle the stochasticity of reward signals. Empirical
results demonstrate that this novel approach improves financial performance
metrics compared to traditional benchmarks and RL agents trained solely using
reinforcement feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep <span class="highlight-title">Generative</span> Demand Learning for Newsvendor and Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijin Gong, Huihang Liu, Xinyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider data-driven inventory and pricing decisions in the feature-based
newsvendor problem, where demand is influenced by both price and contextual
features and is modeled without any structural assumptions. The unknown demand
distribution results in a challenging conditional stochastic optimization
problem, further complicated by decision-dependent uncertainty and the
integration of features. Inspired by recent advances in deep generative
learning, we propose a novel approach leveraging conditional deep generative
models (cDGMs) to address these challenges. cDGMs learn the demand distribution
and generate probabilistic demand forecasts conditioned on price and features.
This generative approach enables accurate profit estimation and supports the
design of algorithms for two key objectives: (1) optimizing inventory for
arbitrary prices, and (2) jointly determining optimal pricing and inventory
levels. We provide theoretical guarantees for our approach, including the
consistency of profit estimation and convergence of our decisions to the
optimal solution. Extensive simulations-ranging from simple to complex
scenarios, including one involving textual features-and a real-world case study
demonstrate the effectiveness of our approach. Our method opens a new paradigm
in management science and operations research, is adaptable to extensions of
the newsvendor and pricing problems, and holds potential for solving other
conditional stochastic optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Subset Tuning: Expanding the Operational Range of
  Parameter-Efficient Training for <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Stahlberg, Jared Lichtarge, Shankar Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel parameter-efficient training (PET) method for large
language models that adapts models to downstream tasks by optimizing a small
subset of the existing model parameters. Unlike prior methods, this subset is
not fixed in location but rather which parameters are modified evolves over the
course of training. This dynamic parameter selection can yield good performance
with many fewer parameters than extant methods. Our method enables a seamless
scaling of the subset size across an arbitrary proportion of the total model
size, while popular PET approaches like prompt tuning and LoRA cover only a
small part of this spectrum. We match or outperform prompt tuning and LoRA in
most cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given
parameter budget across different model families and sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Adaptive Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the challenges of large language model performance in natural
language to SQL tasks, we introduce XiYan-SQL, an innovative framework that
employs a multi-generator ensemble strategy to improve candidate generation. We
introduce M-Schema, a semi-structured schema representation method designed to
enhance the understanding of database structures. To enhance the quality and
diversity of generated candidate SQL queries, XiYan-SQL integrates the
significant potential of in-context learning (ICL) with the precise control of
supervised fine-tuning. On one hand, we propose a series of training strategies
to fine-tune models to generate high-quality candidates with diverse
preferences. On the other hand, we implement the ICL approach with an example
selection method based on named entity recognition to prevent overemphasis on
entities. The refiner optimizes each candidate by correcting logical or
syntactical errors. To address the challenge of identifying the best candidate,
we fine-tune a selection model to distinguish nuances of candidate SQL queries.
The experimental results on multiple dialect datasets demonstrate the
robustness of XiYan-SQL in addressing challenges across different scenarios.
Overall, our proposed XiYan-SQL achieves the state-of-the-art execution
accuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on
NL2GQL, and a competitive score of 72.23% on the Bird development benchmark.
The proposed framework not only enhances the quality and diversity of SQL
queries but also outperforms previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hopfield-Fenchel-Young Networks: A Unified Framework for Associative
  Memory Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saul Santos, Vlad Niculae, Daniel McNamee, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Associative memory models, such as Hopfield networks and their modern
variants, have garnered renewed interest due to advancements in memory capacity
and connections with self-attention in transformers. In this work, we introduce
a unified framework-Hopfield-Fenchel-Young networks-which generalizes these
models to a broader family of energy functions. Our energies are formulated as
the difference between two Fenchel-Young losses: one, parameterized by a
generalized entropy, defines the Hopfield scoring mechanism, while the other
applies a post-transformation to the Hopfield output. By utilizing Tsallis and
norm entropies, we derive end-to-end differentiable update rules that enable
sparse transformations, uncovering new connections between loss margins,
sparsity, and exact retrieval of single memory patterns. We further extend this
framework to structured Hopfield networks using the SparseMAP transformation,
allowing the retrieval of pattern associations rather than a single pattern.
Our framework unifies and extends traditional and modern Hopfield networks and
provides an energy minimization perspective for widely used
post-transformations like $\ell_2$-normalization and layer normalization-all
through suitable choices of Fenchel-Young losses and by using convex analysis
as a building block. Finally, we validate our Hopfield-Fenchel-Young networks
on diverse memory recall tasks, including free and sequential recall.
Experiments on simulated data, image retrieval, multiple instance learning, and
text rationalization demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 14 figures. arXiv admin note: text overlap with
  arXiv:2402.13725</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepUQ: Assessing the Aleatoric Uncertainties from two Deep Learning
  Methods <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca Nevin, Aleksandra Ćiprijanović, Brian D. Nord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the quality of aleatoric uncertainty estimates from uncertainty
quantification (UQ) deep learning methods is important in scientific contexts,
where uncertainty is physically meaningful and important to characterize and
interpret exactly. We systematically compare aleatoric uncertainty measured by
two UQ techniques, Deep Ensembles (DE) and Deep Evidential Regression (DER).
Our method focuses on both zero-dimensional (0D) and two-dimensional (2D) data,
to explore how the UQ methods function for different data dimensionalities. We
investigate uncertainty injected on the input and output variables and include
a method to propagate uncertainty in the case of input uncertainty so that we
can compare the predicted aleatoric uncertainty to the known values. We
experiment with three levels of noise. The aleatoric uncertainty predicted
across all models and experiments scales with the injected noise level.
However, the predicted uncertainty is miscalibrated to $\rm{std}(\sigma_{\rm
al})$ with the true uncertainty for half of the DE experiments and almost all
of the DER experiments. The predicted uncertainty is the least accurate for
both UQ methods for the 2D input uncertainty experiment and the high-noise
level. While these results do not apply to more complex data, they highlight
that further research on post-facto calibration for these methods would be
beneficial, particularly for high-noise and high-dimensional settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Machine Learning for Physical Sciences workshop at
  NeurIPS 2024; 11 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Algorithms For Signature Diagnostics Of Three-Phase Motors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stepan Svirin, Artem Ryzhikov, Saraa Ali, Denis Derkach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining state of the art algorithms with a novel unsupervised anomaly
generation methodology that takes into account physics model of the engine.
This hybrid approach leverages the strengths of both supervised ML and
unsupervised signature analysis, achieving superior diagnostic accuracy and
reliability along with a wide industrial application. Our experimental results
demonstrate that this method significantly outperforms existing ML and non-ML
state-of-the-art approaches while retaining the practical advantages of an
unsupervised methodology. The findings highlight the potential of our approach
to significantly contribute to the field of engine diagnostics, offering a
robust and efficient solution for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grammarization-Based Grasping with Deep Multi-Autoencoder Latent Space
  Exploration by Reinforcement Learning <span class="highlight-title">Agent</span> <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Askianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping by a robot in unstructured environments is deemed a critical
challenge because of the requirement for effective adaptation to a wide
variation in object geometries, material properties, and other environmental
factors. In this paper, we propose a novel framework for robotic grasping based
on the idea of compressing high-dimensional target and gripper features in a
common latent space using a set of autoencoders. Our approach simplifies
grasping by using three autoencoders dedicated to the target, the gripper, and
a third one that fuses their latent representations. This allows the RL agent
to achieve higher learning rates at the initial stages of exploration of a new
environment, as well as at non-zero shot grasp attempts. The agent explores the
latent space of the third autoencoder for better quality grasp without explicit
reconstruction of objects. By implementing the PoWER algorithm into the RL
training process, updates on the agent's policy will be made through the
perturbation in the reward-weighted latent space. The successful exploration
efficiently constrains both position and pose integrity for feasible executions
of grasps. We evaluate our system on a diverse set of objects, demonstrating
the high success rate in grasping with minimum computational overhead. We found
that approach enhances the adaptation of the RL agent by more than 35 \% in
simulation experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review at IEEE ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Locally Adaptive Metrics that Enhance Structural Representation
  with $\texttt{LAMINAR}$ <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Kleiber, William H. Oliver, Tobias Buck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present $\texttt{LAMINAR}$, a novel unsupervised machine learning pipeline
designed to enhance the representation of structure within data via producing a
more-informative distance metric. Analysis methods in the physical sciences
often rely on standard metrics to define geometric relationships in data, which
may fail to capture the underlying structure of complex data sets.
$\texttt{LAMINAR}$ addresses this by using a continuous-normalising-flow and
inverse-transform-sampling to define a Riemannian manifold in the data space
without the need for the user to specify a metric over the data a-priori. The
result is a locally-adaptive-metric that produces structurally-informative
density-based distances. We demonstrate the utility of $\texttt{LAMINAR}$ by
comparing its output to the Euclidean metric for structured data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the NeurIPS 2024 Machine Learning and the Physical
  Sciences workshop. 6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">Pre-Train</span>ed Neural Networks to Enhance Machine Learning with
  Variational Quantum Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Qi, Chao-Han Yang, Samuel Yen-Chi Chen, Pin-Yu Chen, Hector Zenil, Jesper Tegner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) offers tremendous potential but is currently
limited by the availability of qubits. We introduce an innovative approach that
utilizes pre-trained neural networks to enhance Variational Quantum Circuits
(VQC). This technique effectively separates approximation error from qubit
count and removes the need for restrictive conditions, making QML more viable
for real-world applications. Our method significantly improves parameter
optimization for VQC while delivering notable gains in representation and
generalization capabilities, as evidenced by rigorous theoretical analysis and
extensive empirical testing on quantum dot classification tasks. Moreover, our
results extend to applications such as human genome analysis, demonstrating the
broad applicability of our approach. By addressing the constraints of current
quantum hardware, our work paves the way for a new era of advanced QML
applications, unlocking the full potential of quantum computing in fields such
as machine learning, materials science, medicine, mimetics, and various
interdisciplinary areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks in Supply Chain Analytics and Optimization:
  Concepts, Perspectives, <span class="highlight-title">Dataset</span> and Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib, Mahathir Mohammad Bappy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have recently gained traction in transportation,
bioinformatics, language and image processing, but research on their
application to supply chain management remains limited. Supply chains are
inherently graph-like, making them ideal for GNN methodologies, which can
optimize and solve complex problems. The barriers include a lack of proper
conceptual foundations, familiarity with graph applications in SCM, and
real-world benchmark datasets for GNN-based supply chain research. To address
this, we discuss and connect supply chains with graph structures for effective
GNN application, providing detailed formulations, examples, mathematical
definitions, and task guidelines. Additionally, we present a multi-perspective
real-world benchmark dataset from a leading FMCG company in Bangladesh,
focusing on supply chain planning. We discuss various supply chain tasks using
GNNs and benchmark several state-of-the-art models on homogeneous and
heterogeneous graphs across six supply chain analytics tasks. Our analysis
shows that GNN-based models consistently outperform statistical Machine
Learning and other Deep Learning models by around 10-30% in regression, 10-30%
in classification and detection tasks, and 15-40% in anomaly detection tasks on
designated metrics. With this work, we lay the groundwork for solving supply
chain problems using GNNs, supported by conceptual discussions, methodological
insights, and a comprehensive dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 Pages. Extended journal version of SupplyGraph (arXiv:2401.15299).
  In Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLV$^2$-Net: Rater-Based Majority-Label Voting for Consistent Meningeal
  Lymphatic Vessel Segmentation <span class="chip">ML4H 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Bongratz, Markus Karmann, Adrian Holz, Moritz Bonhoeffer, Viktor Neumaier, Sarah Deli, Benita Schmitz-Koep, Claus Zimmer, Christian Sorg, Melissa Thalhammer, Dennis M Hedderich, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meningeal lymphatic vessels (MLVs) are responsible for the drainage of waste
products from the human brain. An impairment in their functionality has been
associated with aging as well as brain disorders like multiple sclerosis and
Alzheimer's disease. However, MLVs have only recently been described for the
first time in magnetic resonance imaging (MRI), and their ramified structure
renders manual segmentation particularly difficult. Further, as there is no
consistent notion of their appearance, human-annotated MLV structures contain a
high inter-rater variability that most automatic segmentation methods cannot
take into account. In this work, we propose a new rater-aware training scheme
for the popular nnU-Net model, and we explore rater-based ensembling strategies
for accurate and consistent segmentation of MLVs. This enables us to boost
nnU-Net's performance while obtaining explicit predictions in different
annotation styles and a rater-based uncertainty estimation. Our final model,
MLV$^2$-Net, achieves a Dice similarity coefficient of 0.806 with respect to
the human reference standard. The model further matches the human inter-rater
reliability and replicates age-related associations with MLV volume.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ML4H 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Whole Slide Image Classification through Fisher Vector
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Kant Gupta, Dadi Dharani, Shambhavi Shanker, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of digital pathology, particularly through computational
analysis of whole slide images (WSI), is poised to significantly enhance
diagnostic precision and efficiency. However, the large size and complexity of
WSIs make it difficult to analyze and classify them using computers. This study
introduces a novel method for WSI classification by automating the
identification and examination of the most informative patches, thus
eliminating the need to process the entire slide. Our method involves
two-stages: firstly, it extracts only a few patches from the WSIs based on
their pathological significance; and secondly, it employs Fisher vectors (FVs)
for representing features extracted from these patches, which is known for its
robustness in capturing fine-grained details. This approach not only
accentuates key pathological features within the WSI representation but also
significantly reduces computational overhead, thus making the process more
efficient and scalable. We have rigorously evaluated the proposed method across
multiple datasets to benchmark its performance against comprehensive WSI
analysis and contemporary weakly-supervised learning methodologies. The
empirical results indicate that our focused analysis of select patches,
combined with Fisher vector representation, not only aligns with, but at times
surpasses, the classification accuracy of standard practices. Moreover, this
strategy notably diminishes computational load and resource expenditure,
thereby establishing an efficient and precise framework for WSI analysis in the
realm of digital pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SAD-TIME: a Spatio<span class="highlight-title">temporal</span>-fused network for depression detection with
  Automated multi-scale Depth-wise and TIME-interval-related common feature
  extractor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Guang Wang, Hui-Rang Hou, Li-C<span class="highlight-author">heng Ji</span>n, Chen-Yang Xu, Zhong-Yi Zhang, Qing-Hao Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and Objective: Depression is a severe mental disorder, and
accurate diagnosis is pivotal to the cure and rehabilitation of people with
depression. However, the current questionnaire-based diagnostic methods could
bring subjective biases and may be denied by subjects. In search of a more
objective means of diagnosis, researchers have begun to experiment with deep
learning-based methods for identifying depressive disorders in recent years.
Methods: In this study, a novel Spatiotemporal-fused network with Automated
multi-scale Depth-wise and TIME-interval-related common feature extractor
(SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common
features extractor (CFE), a spatial sector (SpS), a modified temporal sector
(TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale
depth-wise 1D-convolutional neural network and a time-interval embedding
generator, where the unique information of each channel is preserved. The SpS
fuses the functional connectivity with the distance-based connectivity
containing spatial position of EEG electrodes. A multi-head-attention graph
convolutional network is also applied in the SpS to fuse the features from
different EEG channels. The TeS is based on long short-term memory and graph
transformer networks, where the temporal information of different time-windows
is fused. Moreover, the DAL is used after the SpS to obtain the
domain-invariant feature. Results: Experimental results under tenfold
cross-validation show that the proposed SAD-TIME method achieves 92.00% and
94.00% depression classification accuracies on two datasets, respectively, in
cross-subject mode. Conclusion: SAD-TIME is a robust depression detection
model, where the automatedly-generated features, the SpS and the TeS assist the
classification performance with the fusion of the innate spatiotemporal
information in the EEG signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information Theoretic Approach to Operationalize Right to Data
  Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Java, Simra Shahid, Chirag Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread practice of indiscriminate data scraping to fine-tune language
models (LMs) raises significant legal and ethical concerns, particularly
regarding compliance with data protection laws such as the General Data
Protection Regulation (GDPR). This practice often results in the unauthorized
use of personal information, prompting growing debate within the academic and
regulatory communities. Recent works have introduced the concept of generating
unlearnable datasets (by adding imperceptible noise to the clean data), such
that the underlying model achieves lower loss during training but fails to
generalize to the unseen test setting. Though somewhat effective, these
approaches are predominantly designed for images and are limited by several
practical constraints like requiring knowledge of the target model. To this
end, we introduce RegText, a framework that injects imperceptible spurious
correlations into natural language datasets, effectively rendering them
unlearnable without affecting semantic content. We demonstrate RegText's
utility through rigorous empirical analysis of small and large LMs. Notably,
RegText can restrict newer models like GPT-4o and Llama from learning on our
generated data, resulting in a drop in their test accuracy compared to their
zero-shot performance and paving the way for generating unlearnable text to
protect public data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methodology for a Statistical Analysis of Influencing Factors on 3D
  Object Detection Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kuznietsov, Dirk Schweickard, Steven Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, object detection is an essential task to perceive the
environment by localizing and classifying objects. Most object detection
algorithms rely on deep learning for their superior performance. However, their
black box nature makes it challenging to ensure safety. In this paper, we
propose a first-of-its-kind methodology for statistical analysis of the
influence of various factors related to the objects to detect or the
environment on the detection performance of both LiDAR- and camera-based 3D
object detectors. We perform a univariate analysis between each of the factors
and the detection error in order to compare the strength of influence. To
better identify potential sources of detection errors, we also analyze the
performance in dependency of the influencing factors and examine the
interdependencies between the different influencing factors. Recognizing the
factors that influence detection performance helps identify robustness issues
in the trained object detector and supports the safety approval of object
detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Model Agnostic Explanations via Constraint Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Koriche, Jean-Marie Lagniez, Stefan Mengel, Chi Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretable Machine Learning faces a recurring challenge of explaining the
predictions made by opaque classifiers such as ensemble models, kernel methods,
or neural networks in terms that are understandable to humans. When the model
is viewed as a black box, the objective is to identify a small set of features
that jointly determine the black box response with minimal error. However,
finding such model-agnostic explanations is computationally demanding, as the
problem is intractable even for binary classifiers. In this paper, the task is
framed as a Constraint Optimization Problem, where the constraint solver seeks
an explanation of minimum error and bounded size for an input data instance and
a set of samples generated by the black box. From a theoretical perspective,
this constraint programming approach offers PAC-style guarantees for the output
explanation. We evaluate the approach empirically on various datasets and show
that it statistically outperforms the state-of-the-art heuristic Anchors
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Trap-MID: Trapdoor-based Defense against Model Inversion Attacks <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen-<span class="highlight-author">Ting Liu</span>, Shang-Tse Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion (MI) attacks pose a significant threat to the privacy of Deep
Neural Networks by recovering training data distribution from well-trained
models. While existing defenses often rely on regularization techniques to
reduce information leakage, they remain vulnerable to recent attacks. In this
paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to
mislead MI attacks. A trapdoor is integrated into the model to predict a
specific label when the input is injected with the corresponding trigger.
Consequently, this trapdoor information serves as the "shortcut" for MI
attacks, leading them to extract trapdoor triggers rather than private data. We
provide theoretical insights into the impacts of trapdoor's effectiveness and
naturalness on deceiving MI attacks. In addition, empirical experiments
demonstrate the state-of-the-art defense performance of Trap-MID against
various MI attacks without the requirements for extra data or large
computational overhead. Our source code is publicly available at
https://github.com/ntuaislab/Trap-MID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neural Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Unlearning on <span class="highlight-title">Pre-train</span>ed Models by Residual Feature Alignment
  Using LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laiqiao Qin, Tianqing Zhu, Linlin Wang, Wanlei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is new emerged technology that removes a subset of the
training data from a trained model without affecting the model performance on
the remaining data. This topic is becoming increasingly important in protecting
user privacy and eliminating harmful or outdated data. The key challenge lies
in effectively and efficiently unlearning specific information without
compromising the model's utility on the retained data. For the pre-trained
models, fine-tuning is an important way to achieve the unlearning target.
Previous work typically fine-tuned the entire model's parameters, which incurs
significant computation costs. In addition, the fine-tuning process may cause
shifts in the intermediate layer features, affecting the model's overall
utility. In this work, we propose a novel and efficient machine unlearning
method on pre-trained models. We term the method as Residual Feature Alignment
Unlearning. Specifically, we leverage LoRA (Low-Rank Adaptation) to decompose
the model's intermediate features into pre-trained features and residual
features. By adjusting the residual features, we align the unlearned model with
the pre-trained model at the intermediate feature level to achieve both
unlearning and remaining targets. The method aims to learn the zero residuals
on the retained set and shifted residuals on the unlearning set. Extensive
experiments on numerous datasets validate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One STEP at a time: Language <span class="highlight-title">Agent</span>s are Stepwise Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Nguyen, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have shown promising adaptability in dynamic environments to
perform complex tasks. However, despite the versatile knowledge embedded in
large language models, these agents still fall short when it comes to tasks
that require planning. We introduce STEP, a novel framework designed to
efficiently learn from previous experiences to enhance the planning
capabilities of language agents in future steps. Concretely, STEP functions
through four interconnected components. First, the Planner takes on the task,
breaks it down into subtasks and provides relevant insights. Then the Executor
generates action candidates, while the Evaluator ensures the actions align with
learned rules from previous experiences. Lastly, Memory stores experiences to
inform future decisions. In the ScienceWorld benchmark, our results show that
STEP consistently outperforms state-of-the-art models, achieving an overall
score of 67.4 and successfully completing 12 out of 18 tasks. These findings
highlight STEP's potential as a framework for enhancing planning capabilities
in language agents, paving the way for more sophisticated task-solving in
dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Properties of fairness measures in the context of varying class
  imbalance and protected group ratios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dariusz Brzezinski, Julia Stachowiak, Jerzy Stefanowski, Izabela Szczech, Robert Susmaga, Sofya Aksenyuk, Uladzimir Ivashka, Oleksandr Yasinskyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Society is increasingly relying on predictive models in fields like criminal
justice, credit risk management, or hiring. To prevent such automated systems
from discriminating against people belonging to certain groups, fairness
measures have become a crucial component in socially relevant applications of
machine learning. However, existing fairness measures have been designed to
assess the bias between predictions for protected groups without considering
the imbalance in the classes of the target variable. Current research on the
potential effect of class imbalance on fairness focuses on practical
applications rather than dataset-independent measure properties. In this paper,
we study the general properties of fairness measures for changing class and
protected group proportions. For this purpose, we analyze the probability mass
functions of six of the most popular group fairness measures. We also measure
how the probability of achieving perfect fairness changes for varying class
imbalance ratios. Moreover, we relate the dataset-independent properties of
fairness measures described in this paper to classifier fairness in real-life
tasks. Our results show that measures such as Equal Opportunity and Positive
Predictive Parity are more sensitive to changes in class imbalance than
Accuracy Equality. These findings can help guide researchers and practitioners
in choosing the most appropriate fairness measures for their classification
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Material Property Prediction with Element Attribute Knowledge Graphs and
  Multimodal Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Chunyan Chen, Ling Shi, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has become a crucial tool for predicting the properties of
crystalline materials. However, existing methods primarily represent material
information by constructing multi-edge graphs of crystal structures, often
overlooking the chemical and physical properties of elements (such as atomic
radius, electronegativity, melting point, and ionization energy), which have a
significant impact on material performance. To address this limitation, we
first constructed an element property knowledge graph and utilized an embedding
model to encode the element attributes within the knowledge graph. Furthermore,
we propose a multimodal fusion framework, ESNet, which integrates element
property features with crystal structure features to generate joint multimodal
representations. This provides a more comprehensive perspective for predicting
the performance of crystalline materials, enabling the model to consider both
microstructural composition and chemical characteristics of the materials. We
conducted experiments on the Materials Project benchmark dataset, which showed
leading performance in the bandgap prediction task and achieved results on a
par with existing benchmarks in the formation energy prediction task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Qualitative Insights: Leveraging <span class="highlight-title">LLM</span>s to Market Predict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyoung Lee, Youngsoo Choi, Yuhee Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have the potential to
transform financial analytics by integrating numerical and textual data.
However, challenges such as insufficient context when fusing multimodal
information and the difficulty in measuring the utility of qualitative outputs,
which LLMs generate as text, have limited their effectiveness in tasks such as
financial forecasting. This study addresses these challenges by leveraging
daily reports from securities firms to create high-quality contextual
information. The reports are segmented into text-based key factors and combined
with numerical data, such as price information, to form context sets. By
dynamically updating few-shot examples based on the query time, the sets
incorporate the latest information, forming a highly relevant set closely
aligned with the query point. Additionally, a crafted prompt is designed to
assign scores to the key factors, converting qualitative insights into
quantitative results. The derived scores undergo a scaling process,
transforming them into real-world values that are used for prediction. Our
experiments demonstrate that LLMs outperform time-series models in market
forecasting, though challenges such as imperfect reproducibility and limited
explainability remain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLaSP: Learning Concepts for Time-Series Signals from Natural Language
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoi Ito, Kota Dohi, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a foundation model called "CLaSP" that can search time
series signals using natural language that describes the characteristics of the
signals as queries. Previous efforts to represent time series signal data in
natural language have had challenges in designing a conventional class of time
series signal characteristics, formulating their quantification, and creating a
dictionary of synonyms. To overcome these limitations, the proposed method
introduces a neural network based on contrastive learning. This network is
first trained using the datasets TRUCE and SUSHI, which consist of time series
signals and their corresponding natural language descriptions. Previous studies
have proposed vocabularies that data analysts use to describe signal
characteristics, and SUSHI was designed to cover these terms. We believe that a
neural network trained on these datasets will enable data analysts to search
using natural language vocabulary. Furthermore, our method does not require a
dictionary of predefined synonyms, and it leverages common sense knowledge
embedded in a large-scale language model (LLM). Experimental results
demonstrate that CLaSP enables natural language search of time series signal
data and can accurately learn the points at which signal data changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Syntactic Representations Enable Hierarchical Word Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biraj Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed representations currently used are dense and uninterpretable,
leading to interpretations that themselves are relative, overcomplete, and hard
to interpret. We propose a method that transforms these word vectors into
reduced syntactic representations. The resulting representations are compact
and interpretable allowing better visualization and comparison of the word
vectors and we successively demonstrate that the drawn interpretations are in
line with human judgment. The syntactic representations are then used to create
hierarchical word vectors using an incremental learning approach similar to the
hierarchical aspect of human learning. As these representations are drawn from
pre-trained vectors, the generation process and learning approach are
computationally efficient. Most importantly, we find out that syntactic
representations provide a plausible interpretation of the vectors and
subsequent hierarchical vectors outperform the original vectors in benchmark
tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics Informed <span class="highlight-title">Distill</span>ation for <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhananjaya Nagaraja Gowda, Chanwoo Kim, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently emerged as a potent tool in generative
modeling. However, their inherent iterative nature often results in sluggish
image generation due to the requirement for multiple model evaluations. Recent
progress has unveiled the intrinsic link between diffusion models and
Probability Flow Ordinary Differential Equations (ODEs), thus enabling us to
conceptualize diffusion models as ODE systems. Simultaneously, Physics Informed
Neural Networks (PINNs) have substantiated their effectiveness in solving
intricate differential equations through implicit modeling of their solutions.
Building upon these foundational insights, we introduce Physics Informed
Distillation (PID), which employs a student model to represent the solution of
the ODE system corresponding to the teacher diffusion model, akin to the
principles employed in PINNs. Through experiments on CIFAR 10 and ImageNet
64x64, we observe that PID achieves performance comparable to recent
distillation methods. Notably, it demonstrates predictable trends concerning
method-specific hyperparameters and eliminates the need for synthetic dataset
generation during the distillation process. Both of which contribute to its
easy-to-use nature as a distillation approach for Diffusion Models. Our code
and pre-trained checkpoint are publicly available at:
https://github.com/pantheon5100/pid_diffusion.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Graph Learning with Graphless Clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingbo Fu, Song Wang, Yushun Dong, Binchi Zhang, Chen Chen, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) is tasked with training machine learning
models, such as Graph Neural Networks (GNNs), for multiple clients, each with
its own graph data. Existing methods usually assume that each client has both
node features and graph structure of its graph data. In real-world scenarios,
however, there exist federated systems where only a part of the clients have
such data while other clients (i.e. graphless clients) may only have node
features. This naturally leads to a novel problem in FGL: how to jointly train
a model over distributed graph data with graphless clients? In this paper, we
propose a novel framework FedGLS to tackle the problem in FGL with graphless
clients. In FedGLS, we devise a local graph learner on each graphless client
which learns the local graph structure with the structure knowledge transferred
from other clients. To enable structure knowledge transfer, we design a GNN
model and a feature encoder on each client. During local training, the feature
encoder retains the local graph structure knowledge together with the GNN model
via knowledge distillation, and the structure knowledge is transferred among
clients in global update. Our extensive experiments demonstrate the superiority
of the proposed FedGLS over five baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surprisingly Popular Voting for Concentric Rank-Order Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Hosseini, Debmalya Mandal, Amrit Puhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important problem on social information sites is the recovery of ground
truth from individual reports when the experts are in the minority. The wisdom
of the crowd, i.e. the collective opinion of a group of individuals fails in
such a scenario. However, the surprisingly popular (SP)
algorithm~\cite{prelec2017solution} can recover the ground truth even when the
experts are in the minority, by asking the individuals to report additional
prediction reports--their beliefs about the reports of others. Several recent
works have extended the surprisingly popular algorithm to an equivalent voting
rule (SP-voting) to recover the ground truth ranking over a set of $m$
alternatives. However, we are yet to fully understand when SP-voting can
recover the ground truth ranking, and if so, how many samples (votes and
predictions) it needs. We answer this question by proposing two rank-order
models and analyzing the sample complexity of SP-voting under these models. In
particular, we propose concentric mixtures of Mallows and Plackett-Luce models
with $G (\ge 2)$ groups. Our models generalize previously proposed concentric
mixtures of Mallows models with $2$ groups, and we highlight the importance of
$G > 2$ groups by identifying three distinct groups (expert, intermediate, and
non-expert) from existing datasets. Next, we provide conditions on the
parameters of the underlying models so that SP-voting can recover ground-truth
rankings with high probability, and also derive sample complexities under the
same. We complement the theoretical results by evaluating SP-voting on
simulated and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coverage Analysis for Digital Cousin Selection -- Improving
  Multi-Environment Q-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talha Bozkus, Tara Javidi, Urbashi Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Q-learning is widely employed for optimizing various large-dimensional
networks with unknown system dynamics. Recent advancements include
multi-environment mixed Q-learning (MEMQ) algorithms, which utilize multiple
independent Q-learning algorithms across multiple, structurally related but
distinct environments and outperform several state-of-the-art Q-learning
algorithms in terms of accuracy, complexity, and robustness. We herein conduct
a comprehensive probabilistic coverage analysis to ensure optimal data coverage
conditions for MEMQ algorithms. First, we derive upper and lower bounds on the
expectation and variance of different coverage coefficients (CC) for MEMQ
algorithms. Leveraging these bounds, we develop a simple way of comparing the
utilities of multiple environments in MEMQ algorithms. This approach appears to
be near optimal versus our previously proposed partial ordering approach. We
also present a novel CC-based MEMQ algorithm to improve the accuracy and
complexity of existing MEMQ algorithms. Numerical experiments are conducted
using random network graphs with four different graph properties. Our algorithm
can reduce the average policy error (APE) by 65% compared to partial ordering
and is 95% faster than the exhaustive search. It also achieves 60% less APE
than several state-of-the-art reinforcement learning and prior MEMQ algorithms.
Additionally, we numerically verify the theoretical results and show their
scalability with the action-space size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication Efficient Decentralization for Smoothed Online Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the multi-agent Smoothed Online Convex Optimization (SOCO) problem,
where $N$ agents interact through a communication graph. In each round, each
agent $i$ receives a strongly convex hitting cost function $f^i_t$ in an online
fashion and selects an action $x^i_t \in \mathbb{R}^d$. The objective is to
minimize the global cumulative cost, which includes the sum of individual
hitting costs $f^i_t(x^i_t)$, a temporal "switching cost" for changing
decisions, and a spatial "dissimilarity cost" that penalizes deviations in
decisions among neighboring agents. We propose the first decentralized
algorithm for multi-agent SOCO and prove its asymptotic optimality. Our
approach allows each agent to operate using only local information from its
immediate neighbors in the graph. For finite-time performance, we establish
that the optimality gap in competitive ratio decreases with the time horizon
$T$ and can be conveniently tuned based on the per-round computation available
to each agent. Moreover, our results hold even when the communication graph
changes arbitrarily and adaptively over time. Finally, we establish that the
computational complexity per round depends only logarithmically on the number
of agents and almost linearly on their degree within the graph, ensuring
scalability for large-system implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bangla Grammatical Error Detection Leveraging <span class="highlight-title">Transformer</span>-based Token
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayekh Bin Islam, Ridwanul Hasan Tanvir, Sihat Afnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bangla is the seventh most spoken language by a total number of speakers in
the world, and yet the development of an automated grammar checker in this
language is an understudied problem. Bangla grammatical error detection is a
task of detecting sub-strings of a Bangla text that contain grammatical,
punctuation, or spelling errors, which is crucial for developing an automated
Bangla typing assistant. Our approach involves breaking down the task as a
token classification problem and utilizing state-of-the-art transformer-based
models. Finally, we combine the output of these models and apply rule-based
post-processing to generate a more reliable and comprehensive result. Our
system is evaluated on a dataset consisting of over 25,000 texts from various
sources. Our best model achieves a Levenshtein distance score of 1.04. Finally,
we provide a detailed analysis of different components of our system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Augmented Algorithms for Online Concave Packing and Convex
  Covering Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Grigorescu, Young-San Lin, Maoyuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-augmented algorithms have been extensively studied across the
computer science community in the recent years, driven by advances in machine
learning predictors, which can provide additional information to augment
classical algorithms. Such predictions are especially powerful in the context
of online problems, where decisions have to be made without knowledge of the
future, and which traditionally exhibits impossibility results bounding the
performance of any online algorithm. The study of learning-augmented algorithms
thus aims to use external advice prudently, to overcome classical impossibility
results when the advice is accurate, and still perform comparably to the
state-of-the-art online algorithms even when the advice is inaccurate.
  In this paper, we present learning-augmented algorithmic frameworks for two
fundamental optimizations settings, extending and generalizing prior works. For
online packing with concave objectives, we present a simple but overarching
strategy that switches between the advice and the state-of-the-art online
algorithm. For online covering with convex objectives, we greatly extend
primal-dual methods for online convex covering programs by Azar et al. (FOCS
2016) and previous learning-augmented framework for online covering linear
programs from the literature, to many new applications. We show that our
algorithms break impossibility results when the advice is accurate, while
maintaining comparable performance with state-of-the-art classical online
algorithms even when the advice is erroneous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages. In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Conjugate Flows: Physics-informed architectures with flow
  structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Bizzi, Lucas Nissenbaum, João M. Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Neural Conjugate Flows (NCF), a class of neural network
architectures equipped with exact flow structure. By leveraging topological
conjugation, we prove that these networks are not only naturally isomorphic to
a continuous group, but are also universal approximators for flows of ordinary
differential equation (ODEs). Furthermore, topological properties of these
flows can be enforced by the architecture in an interpretable manner. We
demonstrate in numerical experiments how this topological group structure leads
to concrete computational gains over other physics informed neural networks in
estimating and extrapolating latent dynamics of ODEs, while training up to five
times faster than other flow-based architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are <span class="highlight-title">LLM</span>s Prescient? A Continuous Evaluation using Daily News as the
  Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Dai, Ryan Teehan, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict "future" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Variable Flow Matching: Transforming Conditional Densities
  with Amortized Conditional Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam P. Generale, Andreas E. Robertson, Surya R. Kalidindi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting stochastic nonlinear dynamical systems under the influence of
conditioning variables is a fundamental challenge repeatedly encountered across
the biological and physical sciences. While flow-based models can impressively
predict the temporal evolution of probability distributions representing
possible outcomes of a specific process, existing frameworks cannot
satisfactorily account for the impact of conditioning variables on these
dynamics. Amongst several limitations, existing methods require training data
with paired conditions and are developed for discrete conditioning variables.
We propose Conditional Variable Flow Matching (CVFM), a framework for learning
flows transforming conditional distributions with amortization across
continuous conditioning variables - permitting predictions across the
conditional density manifold. This is accomplished through several novel
advances, in particular, simultaneous sample conditioned flows over the main
and conditioning variables, alongside a conditional Wasserstein distance and
kernel facilitating conditional optimal transport. Collectively, these advances
allow for learning system dynamics provided measurement data whose states and
conditioning variables are not in correspondence. We demonstrate CVFM on a
suite of increasingly challenging problems, including discrete and continuous
conditional mapping benchmarks, image-to-image domain transfer, and modeling
the temporal evolution of materials internal structure during manufacturing
processes. We observe that CVFM results in improved performance and convergence
characteristics over alternative conditional variants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDDBench: A Benchmark for Synthesizable Drug Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songtao Liu, Zhengkai Tu, Hanjun Dai, Peng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant challenge in wet lab experiments with current drug design
generative models is the trade-off between pharmacological properties and
synthesizability. Molecules predicted to have highly desirable properties are
often difficult to synthesize, while those that are easily synthesizable tend
to exhibit less favorable properties. As a result, evaluating the
synthesizability of molecules in general drug design scenarios remains a
significant challenge in the field of drug discovery. The commonly used
synthetic accessibility (SA) score aims to evaluate the ease of synthesizing
generated molecules, but it falls short of guaranteeing that synthetic routes
can actually be found. Inspired by recent advances in top-down synthetic route
generation, we propose a new, data-driven metric to evaluate molecule
synthesizability. Our approach directly assesses the feasibility of synthetic
routes for a given molecule through our proposed round-trip score. This novel
metric leverages the synergistic duality between retrosynthetic planners and
reaction predictors, both of which are trained on extensive reaction datasets.
To demonstrate the efficacy of our method, we conduct a comprehensive
evaluation of round-trip scores alongside search success rate across a range of
representative molecule generative models. Code is available at
https://github.com/SongtaoLiu0823/SDDBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TowerDebias: A Novel Debiasing Method based on the Tower Property 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Norman Matloff, Aditya Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making processes have increasingly come to rely on sophisticated
machine learning tools, raising concerns about the fairness of their
predictions with respect to any sensitive groups. The widespread use of
commercial black-box machine learning models necessitates careful consideration
of their legal and ethical implications on consumers. In situations where users
have access to these "black-box" models, a key question emerges: how can we
mitigate or eliminate the influence of sensitive attributes, such as race or
gender? We propose towerDebias (tDB), a novel approach designed to reduce the
influence of sensitive variables in predictions made by black-box models. Using
the Tower Property from probability theory, tDB aims to improve prediction
fairness during the post-processing stage in a manner amenable to the
Fairness-Utility Tradeoff. This method is highly flexible, requiring no prior
knowledge of the original model's internal structure, and can be extended to a
range of different applications. We provide a formal improvement theorem for
tDB and demonstrate its effectiveness in both regression and classification
tasks, underscoring its impact on the fairness-utility tradeoff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to a journal soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RESOLVE: Relational Reasoning with Symbolic and Object-Level Features
  Using Vector Symbolic Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Mejri, Chandramouli Amarnath, Abhijit Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern transformer-based encoder-decoder architectures struggle with
reasoning tasks due to their inability to effectively extract relational
information between input objects (data/tokens). Recent work introduced the
Abstractor module, embedded between transformer layers, to address this gap.
However, the Abstractor layer while excelling at capturing relational
information (pure relational reasoning), faces challenges in tasks that require
both object and relational-level reasoning (partial relational reasoning). To
address this, we propose RESOLVE, a neuro-vector symbolic architecture that
combines object-level features with relational representations in
high-dimensional spaces, using fast and efficient operations such as bundling
(summation) and binding (Hadamard product) allowing both object-level features
and relational representations to coexist within the same structure without
interfering with one another. RESOLVE is driven by a novel attention mechanism
that operates in a bipolar high dimensional space, allowing fast attention
score computation compared to the state-of-the-art. By leveraging this design,
the model achieves both low compute latency and memory efficiency. RESOLVE also
offers better generalizability while achieving higher accuracy in purely
relational reasoning tasks such as sorting as well as partial relational
reasoning tasks such as math problem-solving compared to state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hashing for Protein Structure Similarity Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Han, Wu-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structure similarity search (PSSS), which tries to search proteins
with similar structures, plays a crucial role across diverse domains from drug
design to protein function prediction and molecular evolution. Traditional
alignment-based PSSS methods, which directly calculate alignment on the protein
structures, are highly time-consuming with high memory cost. Recently,
alignment-free methods, which represent protein structures as fixed-length
real-valued vectors, are proposed for PSSS. Although these methods have lower
time and memory cost than alignment-based methods, their time and memory cost
is still too high for large-scale PSSS, and their accuracy is unsatisfactory.
In this paper, we propose a novel method, called
$\underline{\text{p}}$r$\underline{\text{o}}$tein
$\underline{\text{s}}$tructure $\underline{\text{h}}$ashing (POSH), for PSSS.
POSH learns a binary vector representation for each protein structure, which
can dramatically reduce the time and memory cost for PSSS compared with
real-valued vector representation based methods. Furthermore, in POSH we also
propose expressive hand-crafted features and a structure encoder to well model
both node and edge interactions in proteins. Experimental results on real
datasets show that POSH can outperform other methods to achieve
state-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more
than six times and speed improvement of more than four times, compared with
other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Least Squares Training of Quadratic Convolutional Neural Networks with
  Applications to System Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Yetman Van Egmond, Luis Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a least squares formulation for the training of a 2-layer
convolutional neural network using quadratic activation functions, a 2-norm
loss function, and no regularization term. Using this method, an analytic
expression for the globally optimal weights is obtained alongside a quadratic
input-output equation for the network. These properties make the network a
viable tool in system theory by enabling further analysis, such as the
sensitivity of the output to perturbations in the input, which is crucial for
safety-critical systems such as aircraft or autonomous vehicles.The least
squares method is compared to previously proposed strategies for training
quadratic networks and to a back-propagation-trained ReLU network. The proposed
method is applied to a system identification problem and a GPS position
estimation problem. The least squares network is shown to have a significantly
reduced training time with minimal compromises on prediction accuracy alongside
the advantages of having an analytic input-output equation. Although these
results only apply to 2-layer networks, this paper motivates the exploration of
deeper quadratic networks in the context of system theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>ree: Towards Explainable Decision-Making via <span class="highlight-title">LLM</span>-powered Decision
  Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sichao Xiong, Yigit Ihlamur, Fuat Alican, Aaron Ontoyin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional decision tree algorithms are explainable but struggle with
non-linear, high-dimensional data, limiting its applicability in complex
decision-making. Neural networks excel at capturing complex patterns but
sacrifice explainability in the process. In this work, we present GPTree, a
novel framework combining explainability of decision trees with the advanced
reasoning capabilities of LLMs. GPTree eliminates the need for feature
engineering and prompt chaining, requiring only a task-specific prompt and
leveraging a tree-based structure to dynamically split samples. We also
introduce an expert-in-the-loop feedback mechanism to further enhance
performance by enabling human intervention to refine and rebuild decision
paths, emphasizing the harmony between human expertise and machine
intelligence. Our decision tree achieved a 7.8% precision rate for identifying
"unicorn" startups at the inception stage of a startup, surpassing gpt-4o with
few-shot learning as well as the best human decision-makers (3.1% to 5.6%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drone Detection using Deep Neural Networks Trained on Pure Synthetic
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariusz Wisniewski, Zeeshan A. Rana, Ivan Petrunin, Alan Holt, Stephen Harman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone detection has benefited from improvements in deep neural networks, but
like many other applications, suffers from the availability of accurate data
for training. Synthetic data provides a potential for low-cost data generation
and has been shown to improve data availability and quality. However, models
trained on synthetic datasets need to prove their ability to perform on
real-world data, known as the problem of sim-to-real transferability. Here, we
present a drone detection Faster-RCNN model trained on a purely synthetic
dataset that transfers to real-world data. We found that it achieves an AP_50
of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones -
compared with 97.8% for an equivalent model trained on real-world data. Our
results show that using synthetic data for drone detection has the potential to
reduce data collection costs and improve labelling quality. These findings
could be a starting point for more elaborate synthetic drone datasets. For
example, realistic recreations of specific scenarios could de-risk the dataset
generation of safety-critical applications such as the detection of drones at
airports. Further, synthetic data may enable reliable drone detection systems,
which could benefit other areas, such as unmanned traffic management systems.
The code is available
https://github.com/mazqtpopx/cranfield-synthetic-drone-detection alongside the
datasets
https://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code-mixed <span class="highlight-title">LLM</span>: Improve <span class="highlight-title">Large Language Model</span>s' Capability to Handle
  Code-Mixing through Reinforcement Learning from AI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Zhang, Aditya Majumdar, Amulya Yadav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of
linguistic units from two or more languages during the conversation or
sometimes even a single utterance. Code-mixing introduces unique challenges in
daily life, such as syntactic mismatches and semantic blending, that are rarely
encountered in monolingual settings. Large language models (LLMs) have
revolutionized the field of natural language processing (NLP) by offering
unprecedented capabilities in understanding human languages. However, the
effectiveness of current state-of-the-art multilingual LLMs has not yet been
fully explored in the CM scenario. To fill this gap, we first benchmark the
performance of multilingual LLMs on various code-mixing NLP tasks. Then we
propose to improve the multilingual LLMs' ability to understand code-mixing
through reinforcement learning from human feedback (RLHF) and code-mixed
machine translation tasks. Given the high-cost and time-consuming preference
labeling procedure, we improve this by utilizing LLMs as annotators to perform
the reinforcement learning from AI feedback (RLAIF). The experiments show the
effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>initial version: 5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive
  Knowledge Graph Learning <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanggeon Yun, Ryozo Masukawa, William Youngwoo Chung, Minhyoung Na, Nathaniel Bastian, Mohsen Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for robust security solutions across various industries
has made Video Anomaly Detection (VAD) a critical task in applications such as
intelligent surveillance, evidence investigation, and violence detection.
Traditional approaches to VAD often rely on finetuning large pre-trained
models, which can be computationally expensive and impractical for real-time or
resource-constrained environments. To address this, MissionGNN introduced a
more efficient method by training a graph neural network (GNN) using a fixed
knowledge graph (KG) derived from large language models (LLMs) like GPT-4.
While this approach demonstrated significant efficiency in computational power
and memory, it faces limitations in dynamic environments where frequent updates
to the KG are necessary due to evolving behavior trends and shifting data
patterns. These updates typically require cloud-based computation, posing
challenges for edge computing applications. In this paper, we propose a novel
framework that facilitates continuous KG adaptation directly on edge devices,
overcoming the limitations of cloud dependency. Our method dynamically modifies
the KG through a three-phase process: pruning, alternating, and creating nodes,
enabling real-time adaptation to changing data trends. This continuous learning
approach enhances the robustness of anomaly detection models, making them more
suitable for deployment in dynamic and resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Model Prior Overcomes Cold-Start Items 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Wang, Hao Ding, Yupeng Gu, Sergul Aydore, Kousha Kalantari, Branislav Kveton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of recommender systems (RecSys) is driven by digitization and the
need for personalized content in areas such as e-commerce and video streaming.
The content in these systems often changes rapidly and therefore they
constantly face the ongoing cold-start problem, where new items lack
interaction data and are hard to value. Existing solutions for the cold-start
problem, such as content-based recommenders and hybrid methods, leverage item
metadata to determine item similarities. The main challenge with these methods
is their reliance on structured and informative metadata to capture detailed
item similarities, which may not always be available. This paper introduces a
novel approach for cold-start item recommendation that utilizes the language
model (LM) to estimate item similarities, which are further integrated as a
Bayesian prior with classic recommender systems. This approach is generic and
able to boost the performance of various recommenders. Specifically, our
experiments integrate it with both sequential and collaborative filtering-based
recommender and evaluate it on two real-world datasets, demonstrating the
enhanced performance of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is dedicated to cold-start item recommendation using
  language-model priors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Two-Sample Testing under Local Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Mun, Seungwoo Kwak, Ilmun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the trade-off between privacy and statistical utility in private
two-sample testing under local differential privacy (LDP) for both multinomial
and continuous data. We begin by addressing the multinomial case, where we
introduce private permutation tests using practical privacy mechanisms such as
Laplace, discrete Laplace, and Google's RAPPOR. We then extend our multinomial
approach to continuous data via binning and study its uniform separation rates
under LDP over H\"older and Besov smoothness classes. The proposed tests for
both discrete and continuous cases rigorously control the type I error for any
finite sample size, strictly adhere to LDP constraints, and achieve minimax
separation rates under LDP. The attained minimax rates reveal inherent
privacy-utility trade-offs that are unavoidable in private testing. To address
scenarios with unknown smoothness parameters in density testing, we propose an
adaptive test based on a Bonferroni-type approach that ensures robust
performance without prior knowledge of the smoothness parameters. We validate
our theoretical findings with extensive numerical experiments and demonstrate
the practical relevance and effectiveness of our proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimisation Strategies for Ensuring Fairness in Machine Learning: With
  and Without Demographics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring fairness has emerged as one of the primary concerns in AI and its
related algorithms. Over time, the field of machine learning fairness has
evolved to address these issues. This paper provides an extensive overview of
this field and introduces two formal frameworks to tackle open questions in
machine learning fairness.
  In one framework, operator-valued optimisation and min-max objectives are
employed to address unfairness in time-series problems. This approach showcases
state-of-the-art performance on the notorious COMPAS benchmark dataset,
demonstrating its effectiveness in real-world scenarios.
  In the second framework, the challenge of lacking sensitive attributes, such
as gender and race, in commonly used datasets is addressed. This issue is
particularly pressing because existing algorithms in this field predominantly
rely on the availability or estimations of such attributes to assess and
mitigate unfairness. Here, a framework for a group-blind bias-repair is
introduced, aiming to mitigate bias without relying on sensitive attributes.
The efficacy of this approach is showcased through analyses conducted on the
Adult Census Income dataset.
  Additionally, detailed algorithmic analyses for both frameworks are provided,
accompanied by convergence guarantees, ensuring the robustness and reliability
of the proposed methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis. arXiv admin note: text overlap with arXiv:2310.11407</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated
  Machine Learning for Indoor Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Singampalli, Danish Gufran, Sudeep Pasricha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) based indoor localization solutions are critical for
many emerging applications, yet their efficacy is often compromised by
hardware/software variations across mobile devices (i.e., device heterogeneity)
and the threat of ML data poisoning attacks. Conventional methods aimed at
countering these challenges show limited resilience to the uncertainties
created by these phenomena. In response, in this paper, we introduce SAFELOC, a
novel framework that not only minimizes localization errors under these
challenging conditions but also ensures model compactness for efficient mobile
device deployment. Our framework targets a distributed and co-operative
learning environment that uses federated learning (FL) to preserve user data
privacy and assumes heterogeneous mobile devices carried by users (just like in
most real-world scenarios). Within this heterogeneous FL context, SAFELOC
introduces a novel fused neural network architecture that performs data
poisoning detection and localization, with a low model footprint. Additionally,
a dynamic saliency map-based aggregation strategy is designed to adapt based on
the severity of the detected data poisoning scenario. Experimental evaluations
demonstrate that SAFELOC achieves improvements of up to 5.9x in mean
localization error, 7.8x in worst-case localization error, and a 2.1x reduction
in model inference latency compared to state-of-the-art indoor localization
frameworks, across diverse building floorplans, mobile devices, and ML data
poisoning attack scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClevrSkills: Compositional Language and Visual Reasoning in Robotics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Haresh, Daniel Dijkman, Apratim Bhattacharyya, Roland Memisevic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics tasks are highly compositional by nature. For example, to perform a
high-level task like cleaning the table a robot must employ low-level
capabilities of moving the effectors to the objects on the table, pick them up
and then move them off the table one-by-one, while re-evaluating the
consequently dynamic scenario in the process. Given that large vision language
models (VLMs) have shown progress on many tasks that require high level,
human-like reasoning, we ask the question: if the models are taught the
requisite low-level capabilities, can they compose them in novel ways to
achieve interesting high-level tasks like cleaning the table without having to
be explicitly taught so? To this end, we present ClevrSkills - a benchmark
suite for compositional reasoning in robotics. ClevrSkills is an environment
suite developed on top of the ManiSkill2 simulator and an accompanying dataset.
The dataset contains trajectories generated on a range of robotics tasks with
language and visual annotations as well as multi-modal prompts as task
specification. The suite includes a curriculum of tasks with three levels of
compositional understanding, starting with simple tasks requiring basic motor
skills. We benchmark multiple different VLM baselines on ClevrSkills and show
that even after being pre-trained on large numbers of tasks, these models fail
on compositional reasoning in robotics tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024 (D&B track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomaly Detection in Large-Scale Cloud Systems: An Industry Case and
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Saiful Islam, Mohamed Sami Rakha, William Pourmajidi, Janakan Sivaloganathan, John Steinbacher, Andriy Miranskyy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large-Scale Cloud Systems (LCS) become increasingly complex, effective
anomaly detection is critical for ensuring system reliability and performance.
However, there is a shortage of large-scale, real-world datasets available for
benchmarking anomaly detection methods.
  To address this gap, we introduce a new high-dimensional dataset from IBM
Cloud, collected over 4.5 months from the IBM Cloud Console. This dataset
comprises 39,365 rows and 117,448 columns of telemetry data. Additionally, we
demonstrate the application of machine learning models for anomaly detection
and discuss the key challenges faced in this process.
  This study and the accompanying dataset provide a resource for researchers
and practitioners in cloud system monitoring. It facilitates more efficient
testing of anomaly detection methods in real-world data, helping to advance the
development of robust solutions to maintain the health and performance of
large-scale cloud infrastructures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based Time-Series Biomarker Discovery for COPD Diagnosis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Gadgil, Joshua Galanter, Mohammadreza Negahdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Obstructive Pulmonary Disorder (COPD) is an irreversible and
progressive disease which is highly heritable. Clinically, COPD is defined
using the summary measures derived from a spirometry test but these are not
always adequate. Here we show that using the high-dimensional raw spirogram can
provide a richer signal compared to just using the summary measures. We design
a transformer-based deep learning technique to process the raw spirogram values
along with demographic information and predict clinically-relevant endpoints
related to COPD. Our method is able to perform better than prior works while
being more computationally efficient. Using the weights learned by the model,
we make the framework more interpretable by identifying parts of the spirogram
that are important for the model predictions. Pairing up with a board-certified
pulmonologist, we also provide clinical insights into the different aspects of
the spirogram and show that the explanations obtained from the model align with
underlying medical knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Visual Gap: Fine-Tuning Multimodal Models with
  Knowledge-Adapted Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Yanuka, Assaf Ben Kish, Yonatan Bitton, Idan Szpektor, Raja Giryes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research increasingly focuses on training vision-language models
(VLMs) with long, detailed image captions. However, small-scale VLMs often
struggle to balance the richness of these captions with the risk of
hallucinating content during fine-tuning. In this paper, we explore how well
VLMs adapt to such captions. To quantify caption quality, we propose Decomposed
NLI (DNLI), an evaluation framework that breaks down generated captions into
individual propositions, assessing each in isolation. This fine-grained
analysis reveals a critical balance between capturing descriptive details and
preventing hallucinations. Our findings show that simply reducing caption
complexity or employing standard data curation techniques does not effectively
resolve this issue. To tackle this challenge, we introduce Knowledge Adapted
(KnowAda) fine-tuning, a data-centric approach that automatically adapts
training data with the model's existing knowledge and visual understanding.
KnowAda minimizes hallucinations while preserving high descriptiveness. We
validate this approach across several small-scale VLMs (up to 7B parameters)
and dense caption datasets, demonstrating that KnowAda effectively balances
hallucination reduction and descriptiveness. Our results show that KnowAda
outperforms various baselines in both automatic metrics and human evaluations.
We will release our code and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cut Your Losses in Large-Vocabulary <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Krähenbühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models grow ever larger, so do their vocabularies. This has
shifted the memory footprint of LLMs during training disproportionately to one
single layer: the cross-entropy in the loss computation. Cross-entropy builds
up a logit matrix with entries for each pair of input tokens and vocabulary
items and, for small models, consumes an order of magnitude more memory than
the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that
computes the cross-entropy loss without materializing the logits for all tokens
into global memory. Rather, CCE only computes the logit for the correct token
and evaluates the log-sum-exp over all logits on the fly. We implement a custom
kernel that performs the matrix multiplications and the log-sum-exp reduction
over the vocabulary in flash memory, making global memory consumption for the
cross-entropy computation negligible. This has a dramatic effect. Taking the
Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss
computation from 24 GB to 1 MB, and the total training-time memory consumption
of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we
leverage the inherent sparsity of softmax and propose to skip elements of the
gradient computation that have a negligible (i.e., below numerical precision)
contribution to the gradient. Experiments demonstrate that the dramatic
reduction in memory consumption is accomplished without sacrificing training
speed or convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/apple/ml-cross-entropy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refusal in <span class="highlight-title">LLM</span>s is an Affine Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Marshall, Adam Scherlis, Nora Belrose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose affine concept editing (ACE) as an approach for steering language
models' behavior by intervening directly in activations. We begin with an
affine decomposition of model activation vectors and show that prior methods
for steering model behavior correspond to subsets of terms of this
decomposition. We then provide a derivation of ACE and test it on refusal using
Llama 3 8B and Hermes Eagle RWKV v5. ACE ultimately combines affine subspace
projection and activation addition to reliably control the model's refusal
responses across prompt types. We evaluate the results using LLM-based scoring
on a collection of harmful and harmless prompts. Our experiments demonstrate
that ACE consistently achieves more precise control over model behavior and
generalizes to models where directional ablation via affine subspace projection
alone produces incoherent outputs. Code for reproducing our results is
available at https://github.com/EleutherAI/steering-llama3 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Microfoundation Inference for Strategic Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Bracale, Subha Maity, Felipe Maia Polo, Seamus Somerstep, Moulinath Banerjee, Yuekai Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Often in prediction tasks, the predictive model itself can influence the
distribution of the target variable, a phenomenon termed performative
prediction. Generally, this influence stems from strategic actions taken by
stakeholders with a vested interest in predictive models. A key challenge that
hinders the widespread adaptation of performative prediction in machine
learning is that practitioners are generally unaware of the social impacts of
their predictions. To address this gap, we propose a methodology for learning
the distribution map that encapsulates the long-term impacts of predictive
models on the population. Specifically, we model agents' responses as a
cost-adjusted utility maximization problem and propose estimates for said cost.
Our approach leverages optimal transport to align pre-model exposure (ex ante)
and post-model exposure (ex post) distributions. We provide a rate of
convergence for this proposed estimate and assess its quality through empirical
demonstrations on a credit-scoring dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Inference via Differentiable <span class="highlight-title">Diffusion</span> Bridge Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Boserup, Gefan Yang, Michael Lind Severinsen, Christy Anna Hipsley, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a methodology for performing parameter inference in
high-dimensional, non-linear diffusion processes. We illustrate its
applicability for obtaining insights into the evolution of and relationships
between species, including ancestral state reconstruction. Estimation is
performed by utilising score matching to approximate diffusion bridges, which
are subsequently used in an importance sampler to estimate log-likelihoods. The
entire setup is differentiable, allowing gradient ascent on approximated
log-likelihoods. This allows both parameter inference and diffusion mean
estimation. This novel, numerically stable, score matching-based parameter
inference framework is presented and demonstrated on biological two- and
three-dimensional morphometry data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Euclidean High-Order Smooth Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Pablo Contreras, Cristóbal Guzmán, David Martínez-Rubio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop algorithms for the optimization of convex objectives that have
H\"older continuous $q$-th derivatives with respect to a $p$-norm by using a
$q$-th order oracle, for $p, q \geq 1$. We can also optimize other structured
functions. We do this by developing a non-Euclidean inexact accelerated
proximal point method that makes use of an inexact uniformly convex
regularizer. We also provide nearly matching lower bounds for any deterministic
algorithm that interacts with the function via a local oracle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lynx: Enabling Efficient MoE Inference through Dynamic Batch-Aware
  Expert Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vima Gupta, Kartik Sinha, Ada Gavrilovska, Anand Padmanabha Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures have recently gained popularity in
enabling efficient scaling of large language models. However, we uncover a
fundamental tension: while MoEs are designed for selective expert activation,
production serving requires request batching, which forces the activation of
all experts and negates MoE's efficiency benefits during the decode phase. We
present Lynx, a system that enables efficient MoE inference through dynamic,
batch-aware expert selection. Our key insight is that expert importance varies
significantly across tokens and inference phases, creating opportunities for
runtime optimization. Lynx leverages this insight through a lightweight
framework that dynamically reduces active experts while preserving model
accuracy. Our evaluations show that Lynx achieves up to 1.55x reduction in
inference latency while maintaining negligible accuracy loss from baseline
model across complex code generation and mathematical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Upcycling: Inference Inefficient Finetuning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasha Doubov, Nikhil Sardana, Vitaliy Chiley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small, highly trained, open-source large language models are widely used due
to their inference efficiency, but further improving their quality remains a
challenge. Sparse upcycling is a promising approach that transforms a
pretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing
the model's parameter count and quality. In this work, we compare the
effectiveness of sparse upcycling against continued pretraining (CPT) across
different model sizes, compute budgets, and pretraining durations. Our
experiments show that sparse upcycling can achieve better quality, with
improvements of over 20% relative to CPT in certain scenarios. However, this
comes with a significant inference cost, leading to 40% slowdowns in
high-demand inference settings for larger models. Our findings highlight the
trade-off between model quality and inference efficiency, offering insights for
practitioners seeking to balance model quality and deployment constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, To appear in the 4th NeurIPS Workshop on
  Efficient Natural Language and Speech Processing (ENLSP), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply
  Better Samples <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noël Vouitsis, Rasa Hosseinzadeh, Brendan Leigh Ross, Valentin Villecroze, Satya Krishna Gorti, Jesse C. Cresswell, Gabriel Loaiza-Ganem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although diffusion models can generate remarkably high-quality samples, they
are intrinsically bottlenecked by their expensive iterative sampling procedure.
Consistency models (CMs) have recently emerged as a promising diffusion model
distillation method, reducing the cost of sampling by generating high-fidelity
samples in just a few iterations. Consistency model distillation aims to solve
the probability flow ordinary differential equation (ODE) defined by an
existing diffusion model. CMs are not directly trained to minimize error
against an ODE solver, rather they use a more computationally tractable
objective. As a way to study how effectively CMs solve the probability flow
ODE, and the effect that any induced error has on the quality of generated
samples, we introduce Direct CMs, which \textit{directly} minimize this error.
Intriguingly, we find that Direct CMs reduce the ODE solving error compared to
CMs but also result in significantly worse sample quality, calling into
question why exactly CMs work well in the first place. Full code is available
at: https://github.com/layer6ai-labs/direct-cms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 ATTRIB Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regional Style and Color Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13880v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13880v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li, Qingtian Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel contribution to the field of regional style
transfer. Existing methods often suffer from the drawback of applying style
homogeneously across the entire image, leading to stylistic inconsistencies or
foreground object twisted when applied to image with foreground elements such
as person figures. To address this limitation, we propose a new approach that
leverages a segmentation network to precisely isolate foreground objects within
the input image. Subsequently, style transfer is applied exclusively to the
background region. The isolated foreground objects are then carefully
reintegrated into the style-transferred background. To enhance the visual
coherence between foreground and background, a color transfer step is employed
on the foreground elements prior to their rein-corporation. Finally, we utilize
feathering techniques to achieve a seamless amalgamation of foreground and
background, resulting in a visually unified and aesthetically pleasing final
composition. Extensive evaluations demonstrate that our proposed approach
yields significantly more natural stylistic transformations compared to
conventional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Computer Vision,
  Image and Deep Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Single <span class="highlight-title">Transformer</span> for Scalable Vision-<span class="highlight-title">Language Model</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Xingyao Wang, Hao Peng, <span class="highlight-author">Heng Ji</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.
Current large vision-language models (LVLMs) such as LLaVA mostly employ
heterogeneous architectures that connect pre-trained visual encoders with large
language models (LLMs) to facilitate visual recognition and complex reasoning.
Although achieving remarkable performance with relatively lightweight training,
we identify four primary scalability limitations: (1) The visual capacity is
constrained by pre-trained visual encoders, which are typically an order of
magnitude smaller than LLMs. (2) The heterogeneous architecture complicates the
use of established hardware and software infrastructure. (3) Study of scaling
laws on such architecture must consider three separate components - visual
encoder, connector, and LLMs, which complicates the analysis. (4) The use of
existing visual encoders typically requires following a pre-defined
specification of image inputs pre-processing, for example, by reshaping inputs
to fixed-resolution square images, which presents difficulties in processing
and training on high-resolution images or those with unusual aspect ratio. A
unified single Transformer architecture, like SOLO, effectively addresses these
scalability concerns in LVLMs; however, its limited adoption in the modern
context likely stems from the absence of reliable training recipes that balance
both modalities and ensure stable training for billion-scale models. In this
paper, we introduce the first open-source training recipe for developing SOLO,
an open-source 7B LVLM using moderate academic resources. The training recipe
involves initializing from LLMs, sequential pre-training on ImageNet and
web-scale data, and instruction fine-tuning on our curated high-quality
datasets. On extensive evaluation, SOLO demonstrates performance comparable to
LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Geometry-Aware Neural Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiheng Zhong, Hadi Meidani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Engineering design problems often involve solving parametric Partial
Differential Equations (PDEs) under variable PDE parameters and domain
geometry. Recently, neural operators have shown promise in learning PDE
operators and quickly predicting the PDE solutions. However, training these
neural operators typically requires large datasets, the acquisition of which
can be prohibitively expensive. To overcome this, physics-informed training
offers an alternative way of building neural operators, eliminating the high
computational costs associated with Finite Element generation of training data.
Nevertheless, current physics-informed neural operators struggle with
limitations, either in handling varying domain geometries or varying PDE
parameters. In this research, we introduce a novel method, the Physics-Informed
Geometry-Aware Neural Operator (PI-GANO), designed to simultaneously generalize
across both PDE parameters and domain geometries. We adopt a geometry encoder
to capture the domain geometry features, and design a novel pipeline to
integrate this component within the existing DCON architecture. Numerical
results demonstrate the accuracy and efficiency of the proposed method. All the
codes and data related to this work are available on GitHub:
https://github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.13646</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insights and Current Gaps in Open-Source <span class="highlight-title">LLM</span> Vulnerability Scanners: A
  Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Brokman, Omer Hofman, Oren Rachmil, Inderjeet Singh, Rathina Sabapathy Aishvariya Priya, Vikas Pahuja, Amit Giloni, Roman Vainshtein, Hisashi Kojima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents a comparative analysis of open-source vulnerability
scanners for conversational large language models (LLMs). As LLMs become
integral to various applications, they also present potential attack surfaces,
exposed to security risks such as information leakage and jailbreak attacks.
Our study evaluates prominent scanners - Garak, Giskard, PyRIT, and
CyberSecEval - that adapt red-teaming practices to expose these
vulnerabilities. We detail the distinctive features and practical use of these
scanners, outline unifying principles of their design and perform quantitative
evaluations to compare them. These evaluations uncover significant reliability
issues in detecting successful attacks, highlighting a fundamental gap for
future development. Additionally, we contribute a preliminary labelled dataset,
which serves as an initial step to bridge this gap. Based on the above, we
provide strategic recommendations to assist organizations choose the most
suitable scanner for their red-teaming needs, accounting for customizability,
test suite comprehensiveness, and industry-specific use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $π_0$: A Vision-Language-Action Flow Model for General Robot Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot learning holds tremendous promise to unlock the full potential of
flexible, general, and dexterous robot systems, as well as to address some of
the deepest questions in artificial intelligence. However, bringing robot
learning to the level of generality required for effective real-world systems
faces major obstacles in terms of data, generalization, and robustness. In this
paper, we discuss how generalist robot policies (i.e., robot foundation models)
can address these challenges, and how we can design effective generalist robot
policies for complex and highly dexterous tasks. We propose a novel flow
matching architecture built on top of a pre-trained vision-language model (VLM)
to inherit Internet-scale semantic knowledge. We then discuss how this model
can be trained on a large and diverse dataset from multiple dexterous robot
platforms, including single-arm robots, dual-arm robots, and mobile
manipulators. We evaluate our model in terms of its ability to perform tasks in
zero shot after pre-training, follow language instructions from people and from
a high-level VLM policy, and its ability to acquire new skills via fine-tuning.
Our results cover a wide variety of tasks, such as laundry folding, table
cleaning, and assembling boxes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See project website for videos:
  https://physicalintelligence.company/blog/pi0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-informed Discretization-independent Deep Compositional Operator
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13646v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13646v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiheng Zhong, Hadi Meidani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving parametric Partial Differential Equations (PDEs) for a broad range of
parameters is a critical challenge in scientific computing. To this end, neural
operators, which \textcolor{black}{predicts the PDE solution with variable PDE
parameter inputs}, have been successfully used. However, the training of neural
operators typically demands large training datasets, the acquisition of which
can be prohibitively expensive. To address this challenge, physics-informed
training can offer a cost-effective strategy. However, current physics-informed
neural operators face limitations, either in handling irregular domain shapes
or in in generalizing to various discrete representations of PDE parameters. In
this research, we introduce a novel physics-informed model architecture which
can generalize to various discrete representations of PDE parameters and
irregular domain shapes. Particularly, inspired by deep operator neural
networks, our model involves a discretization-independent learning of parameter
embedding repeatedly, and this parameter embedding is integrated with the
response embeddings through multiple compositional layers, for more
expressivity. Numerical results demonstrate the accuracy and efficiency of the
proposed method. All the codes and data related to this work are available on
GitHub: https://github.com/WeihengZ/PI-DCON.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Universal Deep Learning Framework for Materials X-ray Absorption
  Spectra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubha R. Kharel, Fanchen Meng, Xiaohui Qu, Matthew R. Carbone, Deyu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray absorption spectroscopy (XAS) is a powerful characterization technique
for probing the local chemical environment of absorbing atoms. However,
analyzing XAS data presents significant challenges, often requiring extensive,
computationally intensive simulations, as well as significant domain expertise.
These limitations hinder the development of fast, robust XAS analysis pipelines
that are essential in high-throughput studies and for autonomous
experimentation. We address these challenges with OmniXAS, a framework that
contains a suite of transfer learning approaches for XAS prediction, each
contributing to improved accuracy and efficiency, as demonstrated on K-edge
spectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS
framework is built upon three distinct strategies. First, we use M3GNet to
derive latent representations of the local chemical environment of absorption
sites as input for XAS prediction, achieving up to order-of-magnitude
improvements over conventional featurization techniques. Second, we employ a
hierarchical transfer learning strategy, training a universal multi-task model
across elements before fine-tuning for element-specific predictions. Models
based on this cascaded approach after element-wise fine-tuning outperform
element-specific models by up to 69%. Third, we implement cross-fidelity
transfer learning, adapting a universal model to predict spectra generated by
simulation of a different fidelity with a higher computational cost. This
approach improves prediction accuracy by up to 11% over models trained on the
target fidelity alone. Our approach boosts the throughput of XAS modeling by
orders of magnitude versus first-principles simulations and is extendable to
XAS prediction for a broader range of elements. This transfer learning
framework is generalizable to enhance deep-learning models that target other
properties in materials research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main manuscript: 22 pages, 11 figures. Supplemental material (12
  pages, 6 figures) available as a separate file in arXiv ancillary files
  (additional downloadable files)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Uncertainty of <span class="highlight-title">Thought</span>s: Uncertainty-Aware Planning Enhances Information
  Seeking in <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03271v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03271v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, <span class="highlight-author">Junxian He</span>, Pang Wei Koh, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the face of uncertainty, the ability to *seek information* is of
fundamental importance. In many practical applications, such as medical
diagnosis and troubleshooting, the information needed to solve the task is not
initially given and has to be actively sought by asking follow-up questions
(for example, a doctor asking a patient for more details about their symptoms).
In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to
augment large language models with the ability to actively seek information by
asking effective questions. UoT combines 1) an *uncertainty-aware simulation
approach* which enables the model to simulate possible future scenarios and how
likely they are to occur, 2) *uncertainty-based rewards* motivated by
information gain which incentivizes the model to seek information, and 3) a
*reward propagation scheme* to select the optimal question to ask in a way that
maximizes the expected reward. In experiments on medical diagnosis,
troubleshooting, and the `20 Questions` game, UoT achieves an average
performance improvement of 38.1% in the rate of successful task completion
across multiple LLMs compared with direct prompting and also improves
efficiency (i.e., the number of questions needed to complete the task). Our
code has been released [here](https://github.com/zhiyuanhubj/UoT)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Inference Meeting Energy-Efficient Control of Parallel and
  Identical Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the application of active inference in developing
energy-efficient control agents for manufacturing systems. Active inference,
rooted in neuroscience, provides a unified probabilistic framework integrating
perception, learning, and action, with inherent uncertainty quantification
elements. Our study explores deep active inference, an emerging field that
combines deep learning with the active inference decision-making framework.
Leveraging a deep active inference agent, we focus on controlling parallel and
identical machine workstations to enhance energy efficiency. We address
challenges posed by the problem's stochastic nature and delayed policy response
by introducing tailored enhancements to existing agent architectures.
Specifically, we introduce multi-step transition and hybrid horizon methods to
mitigate the need for complex planning. Our experimental results demonstrate
the effectiveness of these enhancements and highlight the potential of the
active inference-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 10th International Conference on Machine Learning,
  Optimization, and Data Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Training Survival Models with Scoring Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Kopper, David Rügamer, Raphael Sonabend, Bernd Bischl, Andreas Bender
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scoring rules are an established way of comparing predictive performances
across model classes. In the context of survival analysis, they require
adaptation in order to accommodate censoring. This work investigates using
scoring rules for model training rather than evaluation. Doing so, we establish
a general framework for training survival models that is model agnostic and can
learn event time distributions parametrically or non-parametrically. In
addition, our framework is not restricted to any specific scoring rule. While
we focus on neural network-based implementations, we also provide
proof-of-concept implementations using gradient boosting, generalized additive
models, and trees. Empirical comparisons on synthetic and real-world data
indicate that scoring rules can be successfully incorporated into model
training and yield competitive predictive performance with established
time-to-event models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CGRclust: Chaos Game Representation for Twin <span class="highlight-title">Contrastive</span> Clustering of
  Unlabelled DNA Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Alipour, Kathleen A. Hill, Lila Kari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes CGRclust, a novel combination of unsupervised twin
contrastive clustering of Chaos Game Representations (CGR) of DNA sequences,
with convolutional neural networks (CNNs). To the best of our knowledge,
CGRclust is the first method to use unsupervised learning for image
classification (herein applied to two-dimensional CGR images) for clustering
datasets of DNA sequences. CGRclust overcomes the limitations of traditional
sequence classification methods by leveraging unsupervised twin contrastive
learning to detect distinctive sequence patterns, without requiring DNA
sequence alignment or biological/taxonomic labels. CGRclust accurately
clustered twenty-five diverse datasets, with sequence lengths ranging from 664
bp to 100 kbp, including mitochondrial genomes of fish, fungi, and protists, as
well as viral whole genome assemblies and synthetic DNA sequences. Compared
with three recent clustering methods for DNA sequences (DeLUCS, iDeLUCS, and
MeShClust v3.0.), CGRclust is the only method that surpasses 81.70% accuracy
across all four taxonomic levels tested for mitochondrial DNA genomes of fish.
Moreover, CGRclust also consistently demonstrates superior performance across
all the viral genomic datasets. The high clustering accuracy of CGRclust on
these twenty-five datasets, which vary significantly in terms of sequence
length, number of genomes, number of clusters, and level of taxonomy,
demonstrates its robustness, scalability, and versatility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal vintage factor analysis with deflation varimax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Bing, Dian Jin, Yuqian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vintage factor analysis is one important type of factor analysis that aims to
first find a low-dimensional representation of the original data, and then to
seek a rotation such that the rotated low-dimensional representation is
scientifically meaningful. The most widely used vintage factor analysis is the
Principal Component Analysis (PCA) followed by the varimax rotation. Despite
its popularity, little theoretical guarantee can be provided to date mainly
because varimax rotation requires to solve a non-convex optimization over the
set of orthogonal matrices.
  In this paper, we propose a deflation varimax procedure that solves each row
of an orthogonal matrix sequentially. In addition to its net computational gain
and flexibility, we are able to fully establish theoretical guarantees for the
proposed procedure in a broader context. Adopting this new deflation varimax as
the second step after PCA, we further analyze this two step procedure under a
general class of factor models. Our results show that it estimates the factor
loading matrix in the minimax optimal rate when the signal-to-noise-ratio (SNR)
is moderate or large. In the low SNR regime, we offer possible improvement over
using PCA and the deflation varimax when the additive noise under the factor
model is structured. The modified procedure is shown to be minimax optimal in
all SNR regimes. Our theory is valid for finite sample and allows the number of
the latent factors to grow with the sample size as well as the ambient
dimension to grow with, or even exceed, the sample size. Extensive simulation
and real data analysis further corroborate our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Effects of Data Scale on UI Control <span class="highlight-title">Agent</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03679v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03679v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, Oriana Riva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents that control computer interfaces to accomplish human tasks
are emerging. Leveraging LLMs to power such agents has been of special
interest, but unless fine-tuned on human-collected task demonstrations,
performance is still relatively low. In this work we study whether fine-tuning
alone is a viable approach for building real-world computer control agents. In
particularly, we investigate how performance measured on both high and
low-level tasks in domain and out of domain scales as more training data is
collected. To this end we collect and release a new dataset, AndroidControl,
consisting of 15,283 demonstrations of everyday tasks with Android apps.
Compared to existing datasets, each AndroidControl task instance includes both
high and low-level human-generated instructions, allowing us to explore the
level of task complexity an agent can handle. Moreover, AndroidControl is the
most diverse computer control dataset to date, including 14,548 unique tasks
over 833 Android apps, thus allowing us to conduct in-depth analysis of the
model performance in and out of the domain of the training data. Using the
dataset, we find that when tested in domain fine-tuned models outperform zero
and few-shot baselines and scale in such a way that robust performance might
feasibly be obtained simply by collecting more data. Out of domain, performance
scales significantly more slowly and suggests that in particular for high-level
tasks, fine-tuning on more data alone may be insufficient for achieving robust
out-of-domain performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Datasets and Benchmarks)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioProtoPNet: An interpretable deep learning model for bird sound
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10420v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10420v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Heinrich, Lukas Rauch, Bernhard Sick, Christoph Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have significantly advanced acoustic bird monitoring by
being able to recognize numerous bird species based on their vocalizations.
However, traditional deep learning models are black boxes that provide no
insight into their underlying computations, limiting their usefulness to
ornithologists and machine learning engineers. Explainable models could
facilitate debugging, knowledge discovery, trust, and interdisciplinary
collaboration. This study introduces AudioProtoPNet, an adaptation of the
Prototypical Part Network (ProtoPNet) for multi-label bird sound
classification. It is an inherently interpretable model that uses a ConvNeXt
backbone to extract embeddings, with the classification layer replaced by a
prototype learning classifier trained on these embeddings. The classifier
learns prototypical patterns of each bird species' vocalizations from
spectrograms of training instances. During inference, audio recordings are
classified by comparing them to the learned prototypes in the embedding space,
providing explanations for the model's decisions and insights into the most
informative embeddings of each bird species. The model was trained on the
BirdSet training dataset, which consists of 9,734 bird species and over 6,800
hours of recordings. Its performance was evaluated on the seven test datasets
of BirdSet, covering different geographical regions. AudioProtoPNet
outperformed the state-of-the-art model Perch, achieving an average AUROC of
0.90 and a cmAP of 0.42, with relative improvements of 7.1% and 16.7% over
Perch, respectively. These results demonstrate that even for the challenging
task of multi-label bird sound classification, it is possible to develop
powerful yet inherently interpretable deep learning models that provide
valuable insights for ornithologists and machine learning engineers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Bias of Mirror Flow on Separable Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Pesme, Radu-Alexandru Dragomir, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the continuous-time counterpart of mirror descent, namely mirror
flow, on classification problems which are linearly separable. Such problems
are minimised `at infinity' and have many possible solutions; we study which
solution is preferred by the algorithm depending on the mirror potential. For
exponential tailed losses and under mild assumptions on the potential, we show
that the iterates converge in direction towards a $\phi_\infty$-maximum margin
classifier. The function $\phi_\infty$ is the \textit{horizon function} of the
mirror potential and characterises its shape `at infinity'. When the potential
is separable, a simple formula allows to compute this function. We analyse
several examples of potentials and provide numerical experiments highlighting
our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips camera ready. Minor changes from the previous versions.
  Mainly added full iterate trajectories (Figure 4)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Distribution Shifts: Empirical Analysis and Inductive
  Modeling for Tabular Data <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Liu, Tianyu Wang, Peng Cui, Hongseok Namkoong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different distribution shifts require different interventions, and algorithms
must be grounded in the specific shifts they address. However, methodological
development for robust algorithms typically relies on structural assumptions
that lack empirical validation. Advocating for an empirically grounded
data-driven approach to research, we build an empirical testbed comprising
natural shifts across 5 tabular datasets and 60,000 method configurations
encompassing imbalanced learning and distributionally robust optimization (DRO)
methods. We find $Y|X$-shifts are most prevalent on our testbed, in stark
contrast to the heavy focus on $X$ (covariate)-shifts in the ML literature. The
performance of robust algorithms varies significantly over shift types, and is
no better than that of vanilla methods. To understand why, we conduct an
in-depth empirical analysis of DRO methods and find that although often
neglected by researchers, implementation details -- such as the choice of
underlying model class (e.g., XGBoost) and hyperparameter selection -- have a
bigger impact on performance than the ambiguity set or its radius. To further
bridge that gap between methodological research and practice, we design case
studies that illustrate how such a data-driven, inductive understanding of
distribution shifts can enhance both data-centric and algorithmic
interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference version appeared in NeurIPS 2023, previously titled "On
  the Need for a Language Describing Distribution Shifts: Illustrations on
  Tabular Datasets"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrating Bayesian <span class="highlight-title">Generative</span> Machine Learning for Bayesiamplification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bieringer, Sascha Diefenbacher, Gregor Kasieczka, Mathias Trabs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, combinations of generative and Bayesian machine learning have been
introduced in particle physics for both fast detector simulation and inference
tasks. These neural networks aim to quantify the uncertainty on the generated
distribution originating from limited training statistics. The interpretation
of a distribution-wide uncertainty however remains ill-defined. We show a clear
scheme for quantifying the calibration of Bayesian generative machine learning
models. For a Continuous Normalizing Flow applied to a low-dimensional toy
example, we evaluate the calibration of Bayesian uncertainties from either a
mean-field Gaussian weight posterior, or Monte Carlo sampling network weights,
to gauge their behaviour on unsteady distribution edges. Well calibrated
uncertainties can then be used to roughly estimate the number of uncorrelated
truth samples that are equivalent to the generated sample and clearly indicate
data amplification for smooth features of the distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, updated references, fixed typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Persistence Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Zeng, Florian Graf, Martin Uray, Stefan Huber, Roland Kwitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning the dynamics in the topology of
time-evolving point clouds, the prevalent spatiotemporal model for systems
exhibiting collective behavior, such as swarms of insects and birds or
particles in physics. In such systems, patterns emerge from (local)
interactions among self-propelled entities. While several well-understood
governing equations for motion and interaction exist, they are notoriously
difficult to fit to data, as most prior work requires knowledge about
individual motion trajectories, i.e., a requirement that is challenging to
satisfy with an increasing number of entities. To evade such confounding
factors, we investigate collective behavior from a $\textit{topological
perspective}$, but instead of summarizing entire observation sequences (as done
previously), we propose learning a latent dynamical model from topological
features $\textit{per time point}$. The latter is then used to formulate a
downstream regression task to predict the parametrization of some a priori
specified governing equation. We implement this idea based on a latent ODE
learned from vectorized (static) persistence diagrams and show that a
combination of recent stability results for persistent homology justifies this
modeling choice. Various (ablation) experiments not only demonstrate the
relevance of each model component but provide compelling empirical evidence
that our proposed model - $\textit{Neural Persistence Dynamics}$ -
substantially outperforms the state-of-the-art across a diverse set of
parameter regression tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circuit design in biology and machine learning. I. Random networks and
  dimensional reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven A. Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A biological circuit is a neural or biochemical cascade, taking inputs and
producing outputs. How have biological circuits learned to solve environmental
challenges over the history of life? The answer certainly follows Dobzhansky's
famous quote that ``nothing in biology makes sense except in the light of
evolution.'' But that quote leaves out the mechanistic basis by which natural
selection's trial-and-error learning happens, which is exactly what we have to
understand. How does the learning process that designs biological circuits
actually work? How much insight can we gain about the form and function of
biological circuits by studying the processes that have made those circuits?
Because life's circuits must often solve the same problems as those faced by
machine learning, such as environmental tracking, homeostatic control,
dimensional reduction, or classification, we can begin by considering how
machine learning designs computational circuits to solve problems. We can then
ask: How much insight do those computational circuits provide about the design
of biological circuits? How much does biology differ from computers in the
particular circuit designs that it uses to solve problems? This article steps
through two classic machine learning models to set the foundation for analyzing
broad questions about the design of biological circuits. One insight is the
surprising power of randomly connected networks. Another is the central role of
internal models of the environment embedded within biological circuits,
illustrated by a model of dimensional reduction and trend prediction. Overall,
many challenges in biology have machine learning analogs, suggesting hypotheses
about how biology's circuits are designed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added background info in two text boxes and new figure, edited
  throughout</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Free Lunch in <span class="highlight-title">LLM</span> Watermarking: Trade-offs in Watermarking Design
  Choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in generative models have made it possible for AI-generated text,
code, and images to mirror human-generated content in many applications.
Watermarking, a technique that aims to embed information in the output of a
model to verify its source, is useful for mitigating the misuse of such
AI-generated content. However, we show that common design choices in LLM
watermarking schemes make the resulting systems surprisingly susceptible to
attack -- leading to fundamental trade-offs in robustness, utility, and
usability. To navigate these trade-offs, we rigorously study a set of simple
yet effective attacks on common watermarking systems, and propose guidelines
and defenses for LLM watermarking in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Large Electric Vehicle Charging Stations via User Behavior
  Modeling and Stochastic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an Electric Vehicle Charging Station (EVCS) model that
incorporates real-world constraints, such as slot power limitations, contract
threshold overruns penalties, or early disconnections of electric vehicles
(EVs). We propose a formulation of the problem of EVCS control under
uncertainty, and implement two Multi-Stage Stochastic Programming approaches
that leverage user-provided information, namely, Model Predictive Control and
Two-Stage Stochastic Programming. The model addresses uncertainties in charging
session start and end times, as well as in energy demand. A user's behavior
model based on a sojourn-time-dependent stochastic process enhances cost
reduction while maintaining customer satisfaction. The benefits of the two
proposed methods are showcased against two baselines over a 22-day simulation
using a real-world dataset. The two-stage approach demonstrates robustness
against early disconnections by considering a wider range of uncertainty
scenarios for optimization. The algorithm prioritizing user satisfaction over
electricity cost achieves a 20% and 36% improvement in two user satisfaction
metrics compared to an industry-standard baseline. Additionally, the algorithm
striking the best balance between cost and user satisfaction exhibits a mere 3%
relative cost increase compared to the theoretically optimal baseline - for
which the nonanticipativity constraint is relaxed - while attaining 94% and 84%
of the user satisfaction performance in the two used satisfaction metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponential separations between classical and quantum learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casper Gyurik, Vedran Dunjko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant effort, the quantum machine learning community has only
demonstrated quantum learning advantages for artificial cryptography-inspired
datasets when dealing with classical data. In this paper we address the
challenge of finding learning problems where quantum learning algorithms can
achieve a provable exponential speedup over classical learning algorithms. We
reflect on computational learning theory concepts related to this question and
discuss how subtle differences in definitions can result in significantly
different requirements and tasks for the learner to meet and solve. We examine
existing learning problems with provable quantum speedups and find that they
largely rely on the classical hardness of evaluating the function that
generates the data, rather than identifying it. To address this, we present two
new learning separations where the classical difficulty primarily lies in
identifying the function generating the data. Furthermore, we explore
computational hardness assumptions that can be leveraged to prove quantum
speedups in scenarios where data is quantum-generated, which implies likely
quantum advantages in a plethora of more natural settings (e.g., in condensed
matter and high energy physics). We also discuss the limitations of the
classical shadow paradigm in the context of learning separations, and how
physically-motivated settings such as characterizing phases of matter and
Hamiltonian learning fit in the computational learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>this article supersedes arXiv:2208.06339</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Neural Collapse and the Neural Collapse of
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Collapse refers to the curious phenomenon in the end of training of a
neural network, where feature vectors and classification weights converge to a
very simple geometrical arrangement (a simplex). While it has been observed
empirically in various cases and has been theoretically motivated, its
connection with crucial properties of neural networks, like their
generalization and robustness, remains unclear. In this work, we study the
stability properties of these simplices. We find that the simplex structure
disappears under small adversarial attacks, and that perturbed examples "leap"
between simplex vertices. We further analyze the geometry of networks that are
optimized to be robust against adversarial perturbations of the input, and find
that Neural Collapse is a pervasive phenomenon in these cases as well, with
clean and perturbed representations forming aligned simplices, and giving rise
to a robust simple nearest-neighbor classifier. By studying the propagation of
the amount of collapse inside the network, we identify novel properties of both
robust and non-robust machine learning models, and show that earlier, unlike
later layers maintain reliable simplices on perturbed data. Our code is
available at https://github.com/JingtongSu/robust_neural_collapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Machine Learning Research, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Inference in Multi-environment Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of constructing valid confidence intervals and sets
in problems of prediction across multiple environments. We investigate two
types of coverage suitable for these problems, extending the jackknife and
split-conformal methods to show how to obtain distribution-free coverage in
such non-traditional, potentially hierarchical data-generating scenarios. We
demonstrate a novel resizing method to adapt to problem difficulty, which
applies both to existing approaches for predictive inference and the methods we
develop; this reduces prediction set sizes using limited information from the
test environment, a key to the methods' practical performance, which we
evaluate through neurochemical sensing and species classification datasets. Our
contributions also include extensions for settings with non-real-valued
responses, a theory of consistency for predictive inference in these general
problems, and insights on the limits of conditional coverage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effectiveness of Explainability Methods in Parkinson's
  Detection from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Mancini, Francesco Paissan, Paolo Torroni, Mirco Ravanelli, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this research: author
  order is alphabetical</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeSubNet: Gene Interaction Inference for Disease Subtype Network
  Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Yang, Zheng Chen, Xin Liu, Rikuto Kotoge, Peng Chen, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving gene functional networks from knowledge databases presents a
challenge due to the mismatch between disease networks and subtype-specific
variations. Current solutions, including statistical and deep learning methods,
often fail to effectively integrate gene interaction knowledge from databases
or explicitly learn subtype-specific interactions. To address this mismatch, we
propose GeSubNet, which learns a unified representation capable of predicting
gene interactions while distinguishing between different disease subtypes.
Graphs generated by such representations can be considered subtype-specific
networks. GeSubNet is a multi-step representation learning framework with three
modules: First, a deep generative model learns distinct disease subtypes from
patient gene expression profiles. Second, a graph neural network captures
representations of prior gene networks from knowledge databases, ensuring
accurate physical gene interactions. Finally, we integrate these two
representations using an inference loss that leverages graph generation
capabilities, conditioned on the patient separation loss, to refine
subtype-specific information in the learned representation. GeSubNet
consistently outperforms traditional methods, with average improvements of
30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged
over four cancer datasets. Particularly, we conduct a biological simulation
experiment to assess how the behavior of selected genes from over 11,000
candidates affects subtypes or patient distributions. The results show that the
generated network has the potential to identify subtype-specific genes with an
83% likelihood of impacting patient distribution shifts. The GeSubNet resource
is available: https://anonymous.4open.science/r/GeSubNet/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Normalization Provably Benefits Nonconvex SGD under
  Heavy-Tailed Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Sun, Xinwang Liu, Kun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the roles of gradient normalization and clipping in
ensuring the convergence of Stochastic Gradient Descent (SGD) under
heavy-tailed noise. While existing approaches consider gradient clipping
indispensable for SGD convergence, we theoretically demonstrate that gradient
normalization alone without clipping is sufficient to ensure convergence.
Furthermore, we establish that combining gradient normalization with clipping
offers significantly improved convergence rates compared to using either
technique in isolation, particularly as gradient noise diminishes. With these
results, our work provides the first theoretical evidence demonstrating the
benefits of gradient normalization in SGD under heavy-tailed noise. Finally, we
introduce an accelerated SGD variant that incorporates both gradient
normalization and clipping, further enhancing convergence rates under
heavy-tailed noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V-LoL: A Diagnostic <span class="highlight-title">Dataset</span> for Visual Logical Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Helff, Wolfgang Stammer, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the successes of recent developments in visual AI, different
shortcomings still exist; from missing exact logical reasoning, to abstract
generalization abilities, to understanding complex and noisy scenes.
Unfortunately, existing benchmarks, were not designed to capture more than a
few of these aspects. Whereas deep learning datasets focus on visually complex
data but simple visual reasoning tasks, inductive logic datasets involve
complex logical learning tasks, however, lack the visual component. To address
this, we propose the diagnostic visual logical learning dataset, V-LoL, that
seamlessly combines visual and logical challenges. Notably, we introduce the
first instantiation of V-LoL, V-LoL-Train, - a visual rendition of a classic
benchmark in symbolic AI, the Michalski train problem. By incorporating
intricate visual scenes and flexible logical reasoning tasks within a versatile
framework, V-LoL-Train provides a platform for investigating a wide range of
visual logical learning challenges. We evaluate a variety of AI systems
including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our
evaluations demonstrate that even SOTA AI faces difficulties in dealing with
visual logical learning challenges, highlighting unique advantages and
limitations of each methodology. Overall, V-LoL opens up new avenues for
understanding and enhancing current abilities in visual logical learning for AI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are <span class="highlight-title">Large Language Model</span>s Table-based Fact-Checkers? <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table-based Fact Verification (TFV) aims to extract the entailment relation
between statements and structured tables. Existing TFV methods based on
small-scaled models suffer from insufficient labeled data and weak zero-shot
ability. Recently, the appearance of Large Language Models (LLMs) has gained
lots of attraction in research fields. They have shown powerful zero-shot and
in-context learning abilities on several NLP tasks, but their potential on TFV
is still unknown. In this work, we implement a preliminary study about whether
LLMs are table-based fact-checkers. In detail, we design diverse prompts to
explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and
few-shot TFV capability. Besides, we carefully design and construct TFV
instructions to study the performance gain brought by the instruction tuning of
LLMs. Experimental results demonstrate that LLMs can achieve acceptable results
on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning
can stimulate the TFV capability significantly. We also make some valuable
findings about the format of zero-shot prompts and the number of in-context
examples. Finally, we analyze some possible directions to promote the accuracy
of TFV via LLMs, which is beneficial to further research of table reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CSCWD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthesizRR: Generating Diverse <span class="highlight-title">Dataset</span>s with Retrieval Augmentation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Divekar, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is often desirable to distill the capabilities of large language models
(LLMs) into smaller student models due to compute and memory constraints. One
way to do this for classification tasks is via dataset synthesis, which can be
accomplished by generating examples of each label from the LLM. Prior
approaches to synthesis use few-shot prompting, which relies on the LLM's
parametric knowledge to generate usable examples. However, this leads to issues
of repetition, bias towards popular entities, and stylistic differences from
human text. In this work, we propose Synthesize by Retrieval and Refinement
(SynthesizRR), which uses retrieval augmentation to introduce variety into the
dataset synthesis process: as retrieved passages vary, the LLM is seeded with
different content to generate its examples. We empirically study the synthesis
of six datasets, covering topic classification, sentiment analysis, tone
detection, and humor, requiring complex synthesis strategies. We find that
SynthesizRR greatly improves lexical and semantic diversity, similarity to
human-written text, and distillation performance, when compared to 32-shot
prompting and four prior approaches. We release our code to perform all steps
at https://github.com/amazon-science/synthesizrr
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at EMNLP 2024. Code available at
  https://github.com/amazon-science/synthesizrr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonic Path Integral <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Behjoo, Michael Chertkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this manuscript, we present a novel approach for sampling from a
continuous multivariate probability distribution, which may either be
explicitly known (up to a normalization factor) or represented via empirical
samples. Our method constructs a time-dependent bridge from a delta function
centered at the origin of the state space at $t=0$, optimally transforming it
into the target distribution at $t=1$. We formulate this as a Stochastic
Optimal Control problem of the Path Integral Control type, with a cost function
comprising (in its basic form) a quadratic control term, a quadratic state
term, and a terminal constraint. This framework, which we refer to as Harmonic
Path Integral Diffusion (H-PID), leverages an analytical solution through a
mapping to an auxiliary quantum harmonic oscillator in imaginary time.
  The H-PID framework results in a set of efficient sampling algorithms,
without the incorporation of Neural Networks. The algorithms are validated on
two standard use cases: a mixture of Gaussians over a grid and images from
CIFAR-10. The transparency of the method allows us to analyze the algorithms in
detail, particularly revealing that the current weighted state is an order
parameter for the dynamic phase transition, signaling earlier, at $t<1$, that
the sample generation process is almost complete. We contrast these algorithms
with other sampling methods, particularly simulated annealing and path integral
sampling, highlighting their advantages in terms of analytical control,
accuracy, and computational efficiency on benchmark problems.
  Additionally, we extend the methodology to more general cases where the
underlying stochastic differential equation includes an external deterministic,
possibly non-conservative force, and where the cost function incorporates a
gauge potential term.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Fractional Inference via Re-Parametrization & Interpolation
  between Tree-Re-Weighted- and Belief Propagation- Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10369v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10369v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Behjoo, Michael Chertkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing the partition function, $Z$, of an Ising model over a graph of $N$
\enquote{spins} is most likely exponential in $N$. Efficient variational
methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms,
compute $Z$ approximately by minimizing the respective (BP- or TRW-) free
energy. We generalize the variational scheme by building a $\lambda$-fractional
interpolation, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to
TRW- and BP-approximations, respectively. This fractional scheme -- coined
Fractional Belief Propagation (FBP) -- guarantees that in the attractive
(ferromagnetic) case $Z^{(TRW)} \geq Z^{(\lambda)} \geq Z^{(BP)}$, and there
exists a unique (\enquote{exact}) $\lambda_*$ such that $Z=Z^{(\lambda_*)}$.
Generalizing the re-parametrization approach of
\citep{wainwright_tree-based_2002} and the loop series approach of
\citep{chertkov_loop_2006}, we show how to express $Z$ as a product, $\forall
\lambda:\ Z=Z^{(\lambda)}{\tilde Z}^{(\lambda)}$, where the multiplicative
correction, ${\tilde Z}^{(\lambda)}$, is an expectation over a node-independent
probability distribution built from node-wise fractional marginals. Our
theoretical analysis is complemented by extensive experiments with models from
Ising ensembles over planar and random graphs of medium and large sizes. Our
empirical study yields a number of interesting observations, such as the
ability to estimate ${\tilde Z}^{(\lambda)}$ with $O(N^{2::4})$ fractional
samples and suppression of variation in $\lambda_*$ estimates with an increase
in $N$ for instances from a particular random Ising ensemble, where $[2::4]$
indicates a range from $2$ to $4$. We also discuss the applicability of this
approach to the problem of image de-noising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Recipe for the Analysis of Randomized Multi-Armed Bandit
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dorian Baudry, Kazuya Suzuki, Junya Honda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a general methodology to derive regret bounds for
randomized multi-armed bandit algorithms. It consists in checking a set of
sufficient conditions on the sampling probability of each arm and on the family
of distributions to prove a logarithmic regret. As a direct application we
revisit two famous bandit algorithms, Minimum Empirical Divergence (MED) and
Thompson Sampling (TS), under various models for the distributions including
single parameter exponential families, Gaussian distributions, bounded
distributions, or distributions satisfying some conditions on their moments. In
particular, we prove that MED is asymptotically optimal for all these models,
but also provide a simple regret analysis of some TS algorithms for which the
optimality is already known. We then further illustrate the interest of our
approach, by analyzing a new Non-Parametric TS algorithm (h-NPTS), adapted to
some families of unbounded reward distributions with a bounded h-moment. This
model can for instance capture some non-parametric families of distributions
whose variance is upper bounded by a known constant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active learning of digenic functions with boolean matrix logic
  programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lun Ai, Stephen H. Muggleton, Shi-shun Liang, Geoff S. Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply logic-based machine learning techniques to facilitate cellular
engineering and drive biological discovery, based on comprehensive databases of
metabolic processes called genome-scale metabolic network models (GEMs).
Predicted host behaviours are not always correctly described by GEMs. Learning
the intricate genetic interactions within GEMs presents computational and
empirical challenges. To address these, we describe a novel approach called
Boolean Matrix Logic Programming (BMLP) by leveraging boolean matrices to
evaluate large logic programs. We introduce a new system, $BMLP_{active}$,
which efficiently explores the genomic hypothesis space by guiding informative
experimentation through active learning. In contrast to sub-symbolic methods,
$BMLP_{active}$ encodes a state-of-the-art GEM of a widely accepted bacterial
host in an interpretable and logical representation using datalog logic
programs. Notably, $BMLP_{active}$ can successfully learn the interaction
between a gene pair with fewer training examples than random experimentation,
overcoming the increase in experimental design space. $BMLP_{active}$ enables
rapid optimisation of metabolic models and offers a realistic approach to a
self-driving lab for microbial engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2405.06724</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty
  Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Darshana Abeyrathna, Sara El Mekkaoui, Andreas Hafver, Christian Agrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tsetlin Machines (TMs) have emerged as a compelling alternative to
conventional deep learning methods, offering notable advantages such as smaller
memory footprint, faster inference, fault-tolerant properties, and
interpretability. Although various adaptations of TMs have expanded their
applicability across diverse domains, a fundamental gap remains in
understanding how TMs quantify uncertainty in their predictions. In response,
this paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed
at providing a robust, reliable, and interpretable approach for uncertainty
quantification. Unlike the original TM, the PTM learns the probability of
staying on each state of each Tsetlin Automaton (TA) across all clauses. These
probabilities are updated using the feedback tables that are part of the TM
framework: Type I and Type II feedback. During inference, TAs decide their
actions by sampling states based on learned probability distributions, akin to
Bayesian neural networks when generating weight values. In our experimental
analysis, we first illustrate the spread of the probabilities across TA states
for the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models
using both simulated and real-world datasets. The experiments on the simulated
dataset reveal the PTM's effectiveness in uncertainty quantification,
particularly in delineating decision boundaries and identifying regions of high
uncertainty. Moreover, when applied to multiclass classification tasks using
the Iris dataset, the PTM demonstrates competitive performance in terms of
predictive entropy and expected calibration error, showcasing its potential as
a reliable tool for uncertainty estimation. Our findings underscore the
importance of selecting appropriate models for accurate uncertainty
quantification in predictive tasks, with the PTM offering a particularly
interpretable and effective solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 6 tables, accepted and presented at ICAAI 2024,
  London</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact, Tractable Gauss-Newton Optimization in Deep Reversible
  Architectures Reveal Poor Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Buffelli, Jamie McGowan, Wangkun Xu, Alexandru Cioba, Da-shan Shiu, Guillaume Hennequin, Alberto Bernacchia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order optimization has been shown to accelerate the training of deep
neural networks in many applications, often yielding faster progress per
iteration on the training loss compared to first-order optimizers. However, the
generalization properties of second-order methods are still being debated.
Theoretical investigations have proved difficult to carry out outside the
tractable settings of heavily simplified model classes -- thus, the relevance
of existing theories to practical deep learning applications remains unclear.
Similarly, empirical studies in large-scale models and real datasets are
significantly confounded by the necessity to approximate second-order updates
in practice. It is often unclear whether the observed generalization behaviour
arises specifically from the second-order nature of the parameter updates, or
instead reflects the specific structured (e.g.\ Kronecker) approximations used
or any damping-based interpolation towards first-order updates. Here, we show
for the first time that exact Gauss-Newton (GN) updates take on a tractable
form in a class of deep reversible architectures that are sufficiently
expressive to be meaningfully applied to common benchmark datasets. We exploit
this novel setting to study the training and generalization properties of the
GN optimizer. We find that exact GN generalizes poorly. In the mini-batch
training setting, this manifests as rapidly saturating progress even on the
\emph{training} loss, with parameter updates found to overfit each
mini-batchatch without producing the features that would support generalization
to other mini-batches. We show that our experiments run in the ``lazy'' regime,
in which the neural tangent kernel (NTK) changes very little during the course
of training. This behaviour is associated with having no significant changes in
neural representations, explaining the lack of generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Review</span> of Electromagnetic Elimination Methods for low-field portable
  MRI scanner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyu Bian, Panfeng Li, Mengyao Zheng, Chihang Wang, Anying Li, Ying Li, Haowei Ni, Zixuan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes conventional and deep learning methods for eliminating
electromagnetic interference (EMI) in MRI systems. We compare traditional
analytical and adaptive techniques with advanced deep learning approaches. Key
strengths and limitations of each method are highlighted. Recent advancements
in active EMI elimination, such as external EMI receiver coils, are discussed
alongside deep learning methods, which show superior EMI suppression by
leveraging neural networks trained on MRI data. While deep learning improves
EMI elimination and diagnostic capabilities, it introduces security and safety
concerns, particularly in commercial applications. A balanced approach,
integrating conventional reliability with deep learning's advanced
capabilities, is proposed for more effective EMI suppression in MRI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Machine Learning and
  Computer Application</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TreeC: a method to generate interpretable energy management systems
  using a metaheuristic algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Ruddick, Luis Ramirez Camargo, Muhammad Andy Putratama, Maarten Messagie, Thierry Coosemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy management systems (EMS) have traditionally been implemented using
rule-based control (RBC) and model predictive control (MPC) methods. However,
recent research has explored the use of reinforcement learning (RL) as a
promising alternative. This paper introduces TreeC, a machine learning method
that utilizes the covariance matrix adaptation evolution strategy metaheuristic
algorithm to generate an interpretable EMS modeled as a decision tree. Unlike
RBC and MPC approaches, TreeC learns the decision strategy of the EMS based on
historical data, adapting the control model to the controlled energy grid. The
decision strategy is represented as a decision tree, providing interpretability
compared to RL methods that often rely on black-box models like neural
networks. TreeC is evaluated against MPC with perfect forecast and RL EMSs in
two case studies taken from literature: an electric grid case and a household
heating case. In the electric grid case, TreeC achieves an average energy loss
and constraint violation score of 19.2, which is close to MPC and RL EMSs that
achieve scores of 14.4 and 16.2 respectively. All three methods control the
electric grid well especially when compared to the random EMS, which obtains an
average score of 12 875. In the household heating case, TreeC performs
similarly to MPC on the adjusted and averaged electricity cost and total
discomfort (0.033 EUR/m$^2$ and 0.42 Kh for TreeC compared to 0.037 EUR/m$^2$
and 2.91 kH for MPC), while outperforming RL (0.266 EUR/m$^2$ and 24.41 Kh).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version Knowledge based system</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The effect of <span class="highlight-title">dataset</span> size and the process of big data mining for
  investigating solar-thermal desalination by using machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilong Peng, Senshan Sun, Zhenwei Xu, Juxin Du, Yangjun Qin, Swellam W. Sharshir, A. W. Kandel, A. E. Kabeel, Nuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning's application in solar-thermal desalination is limited by
data shortage and inconsistent analysis. This study develops an optimized
dataset collection and analysis process for the representative solar still. By
ultra-hydrophilic treatment on the condensation cover, the dataset collection
process reduces the collection time by 83.3%. Over 1,000 datasets are
collected, which is nearly one order of magnitude larger than up-to-date works.
Then, a new interdisciplinary process flow is proposed. Some meaningful results
are obtained that were not addressed by previous studies. It is found that
Radom Forest might be a better choice for datasets larger than 1,000 due to
both high accuracy and fast speed. Besides, the dataset range affects the
quantified importance (weighted value) of factors significantly, with up to a
115% increment. Moreover, the results show that machine learning has a high
accuracy on the extrapolation prediction of productivity, where the minimum
mean relative prediction error is just around 4%. The results of this work not
only show the necessity of the dataset characteristics' effect but also provide
a standard process for studying solar-thermal desalination by machine learning,
which would pave the way for interdisciplinary study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advantages of Neural Population Coding for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00393v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00393v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalar variables, e.g., the orientation of a shape in an image, are commonly
predicted using a single output neuron in a neural network. In contrast, the
mammalian cortex represents variables with a population of neurons. In this
population code, each neuron is most active at its preferred value and shows
partial activity for other values. Here, we investigate the benefit of using a
population code for the output layer of a neural network. We compare population
codes against single-neuron outputs and one-hot vectors. First, we show
theoretically and in experiments with synthetic data that population codes
improve robustness to input noise in networks of stacked linear layers. Second,
we demonstrate the benefit of using population codes to encode ambiguous
outputs, such as the pose of symmetric objects. Using the T-LESS dataset of
feature-less real-world objects, we show that population codes improve the
accuracy of predicting 3D object orientation from image input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioMarkBench: Benchmarking Robustness of Audio Watermarking <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Liu, Moyang Guo, Zhengyuan Jiang, Lun Wang, Neil Zhenqiang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing realism of synthetic speech, driven by advancements in
text-to-speech models, raises ethical concerns regarding impersonation and
disinformation. Audio watermarking offers a promising solution via embedding
human-imperceptible watermarks into AI-generated audios. However, the
robustness of audio watermarking against common/adversarial perturbations
remains understudied. We present AudioMarkBench, the first systematic benchmark
for evaluating the robustness of audio watermarking against watermark removal
and watermark forgery. AudioMarkBench includes a new dataset created from
Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art
watermarking methods, and 15 types of perturbations. We benchmark the
robustness of these methods against the perturbations in no-box, black-box, and
white-box settings. Our findings highlight the vulnerabilities of current
watermarking techniques and emphasize the need for more robust and fair audio
watermarking solutions. Our dataset and code are publicly available at
https://github.com/moyangkuo/AudioMarkBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in NeurIPS Datasets and Benchmarks, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pLDDT-Predictor: High-speed Protein Screening Using <span class="highlight-title">Transformer</span> and ESM2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongwon Chae, Zhenyu Wang, Ijaz Gul, Jiansong Ji, Zhenglin Chen, Peiwu Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in protein structure prediction, particularly AlphaFold2,
have revolutionized structural biology by achieving near-experimental accuracy
($\text{average RMSD} < 1.5\text{\AA}$). However, the computational demands of
these models (approximately 30 minutes per protein on an RTX 4090)
significantly limit their application in high-throughput protein screening.
While large language models like ESM (Evolutionary Scale Modeling) have shown
promise in extracting structural information directly from protein sequences,
rapid assessment of protein structure quality for large-scale analyses remains
a major challenge.
  We introduce pLDDT-Predictor, a high-speed protein screening tool that
achieves a $250,000\times$ speedup compared to AlphaFold2 by leveraging
pre-trained ESM2 protein embeddings and a Transformer architecture. Our model
predicts AlphaFold2's pLDDT (predicted Local Distance Difference Test) scores
with a Pearson correlation of 0.7891 and processes proteins in just 0.007
seconds on average. Using a comprehensive dataset of 1.5 million diverse
protein sequences (ranging from 50 to 2048 amino acids), we demonstrate that
pLDDT-Predictor accurately classifies high-confidence structures (pLDDT $>$ 70)
with 91.2\% accuracy and achieves an MSE of 84.8142 compared to AlphaFold2's
predictions.
  The source code and pre-trained models are freely available at
\url{https://github.com/jw-chae/pLDDT_Predictor}, enabling the research
community to perform rapid, large-scale protein structure quality assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages main topic, 8 pages including citiation, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM4Steal: <span class="highlight-title">Diffusion</span> Model For Link Stealing Attack On Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Haonan Ma, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph has become increasingly integral to the advancement of recommendation
systems, particularly with the fast development of graph neural network(GNN).
By exploring the virtue of rich node features and link information, GNN is
designed to provide personalized and accurate suggestions. Meanwhile, the
privacy leakage of GNN in such contexts has also captured special attention.
Prior work has revealed that a malicious user can utilize auxiliary knowledge
to extract sensitive link data of the target graph, integral to recommendation
systems, via the decision made by the target GNN model. This poses a
significant risk to the integrity and confidentiality of data used in
recommendation system. Though important, previous works on GNN's privacy
leakage are still challenged in three aspects, i.e., limited stealing attack
scenarios, sub-optimal attack performance, and adaptation against defense. To
address these issues, we propose a diffusion model based link stealing attack,
named DM4Steal. It differs previous work from three critical aspects. (i)
Generality: aiming at six attack scenarios with limited auxiliary knowledge, we
propose a novel training strategy for diffusion models so that DM4Steal is
transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from
the retention of semantic structure in the diffusion model during the training
process, DM4Steal is capable to learn the precise topology of the target graph
through the GNN decision process. (iii) Adaptation: when GNN is defensive
(e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling
the score model multiple times to keep performance degradation to a minimum,
thus DM4Steal implements successful adaptive attack on defensive GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We found that there were critical problems in our paper, and we
  needed to redo the experiment, which was incomplete</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAuReL: Learned Augmented Residual Layer <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Menghani, Ravi Kumar, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Efficient Systems for Foundation Models Workshop
  at the International Conference on Machine Learning (ICML) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data movement limits to frontier model training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Erdil, David Schneider-Joseph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a theoretical model of distributed training, and use it to analyze
how far dense and sparse training runs can be scaled. Under our baseline
assumptions, given a three month training duration, data movement bottlenecks
begin to significantly lower hardware utilization for training runs exceeding
about $10^{28}$ FLOP, two orders of magnitude above the largest training run to
date, suggesting the arrival of fundamental barriers to scaling in three years
given recent rates of growth. A training run exceeding about $10^{31}$ FLOP is
infeasible even at low utilization. However, more aggressive batch size scaling
and/or shorter and fatter model shapes, if achievable, have the potential to
permit much larger training runs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAGER: Exact Gradient Inversion for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning works by aggregating locally computed gradients from
multiple clients, thus enabling collaborative training without sharing private
client data. However, prior work has shown that the data can actually be
recovered by the server using so-called gradient inversion attacks. While these
attacks perform well when applied on images, they are limited in the text
domain and only permit approximate reconstruction of small batches and short
input sequences. In this work, we propose DAGER, the first algorithm to recover
whole batches of input text exactly. DAGER leverages the low-rank structure of
self-attention layer gradients and the discrete nature of token embeddings to
efficiently check if a given token sequence is part of the client data. We use
this check to exactly recover full batches in the honest-but-curious setting
without any prior on the data for both encoder- and decoder-based architectures
using exhaustive heuristic search and a greedy approach, respectively. We
provide an efficient GPU implementation of DAGER and show experimentally that
it recovers full batches of size up to 128 on large language models (LLMs),
beating prior attacks in speed (20x at same batch size), scalability (10x
larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Verification with Branch-and-Bound for General
  Nonlinearities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.21063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.21063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouxing Shi, Qirui Jin, Zico Kolter, Suman Jana, Cho-Jui Hsieh, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Branch-and-bound (BaB) is among the most effective techniques for neural
network (NN) verification. However, existing works on BaB for NN verification
have mostly focused on NNs with piecewise linear activations, especially ReLU
networks. In this paper, we develop a general framework, named GenBaB, to
conduct BaB on general nonlinearities to verify NNs with general architectures,
based on linear bound propagation for NN verification. To decide which neuron
to branch, we design a new branching heuristic which leverages linear bounds as
shortcuts to efficiently estimate the potential improvement after branching. To
decide nontrivial branching points for general nonlinear functions, we propose
to pre-optimize branching points, which can be efficiently leveraged during
verification with a lookup table. We demonstrate the effectiveness of our
GenBaB on verifying a wide range of NNs, including NNs with activation
functions such as Sigmoid, Tanh, Sine and GeLU, as well as NNs involving
multi-dimensional nonlinear operations such as multiplications in LSTMs and
Vision Transformers. Our framework also allows the verification of general
nonlinear computation graphs and enables verification applications beyond
simple NNs, particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of
the latest $\alpha,\!\beta$-CROWN, the winner of the 4th and the 5th
International Verification of Neural Networks Competition (VNN-COMP 2023 and
2024).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADI: Adversarial Dominating Inputs in Vertical Federated Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02775v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02775v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Pang, Yuanyuan Yuan, Shuai Wang, Wenting Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical federated learning (VFL) system has recently become prominent as a
concept to process data distributed across many individual sources without the
need to centralize it. Multiple participants collaboratively train models based
on their local data in a privacy-aware manner. To date, VFL has become a de
facto solution to securely learn a model among organizations, allowing
knowledge to be shared without compromising privacy of any individuals. Despite
the prosperous development of VFL systems, we find that certain inputs of a
participant, named adversarial dominating inputs (ADIs), can dominate the joint
inference towards the direction of the adversary's will and force other
(victim) participants to make negligible contributions, losing rewards that are
usually offered regarding the importance of their contributions in federated
learning scenarios. We conduct a systematic study on ADIs by first proving
their existence in typical VFL systems. We then propose gradient-based methods
to synthesize ADIs of various formats and exploit common VFL systems. We
further launch greybox fuzz testing, guided by the saliency score of ``victim''
participants, to perturb adversary-controlled inputs and systematically explore
the VFL attack surface in a privacy-preserving manner. We conduct an in-depth
study on the influence of critical parameters and settings in synthesizing
ADIs. Our study reveals new VFL attack opportunities, promoting the
identification of unknown threats before breaches and building more secure VFL
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Gradient Overlap in Deep Residual Networks with Gradient
  Normalization for Improved Non-Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyoung Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, Residual Networks (ResNets) have proven effective in
addressing the vanishing gradient problem, allowing for the successful training
of very deep networks. However, skip connections in ResNets can lead to
gradient overlap, where gradients from both the learned transformation and the
skip connection combine, potentially resulting in overestimated gradients. This
overestimation can cause inefficiencies in optimization, as some updates may
overshoot optimal regions, affecting weight updates. To address this, we
examine Z-score Normalization (ZNorm) as a technique to manage gradient
overlap. ZNorm adjusts the gradient scale, standardizing gradients across
layers and reducing the negative impact of overlapping gradients. Our
experiments demonstrate that ZNorm improves training process, especially in
non-convex optimization scenarios common in deep learning, where finding
optimal solutions is challenging. These findings suggest that ZNorm can affect
the gradient flow, enhancing performance in large-scale data processing where
accuracy is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly Mild Generalization for Offline Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiu Mao, Qi Wang, Yun Qu, Yuhang Jiang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) suffers from the extrapolation error and
value overestimation. From a generalization perspective, this issue can be
attributed to the over-generalization of value functions or policies towards
out-of-distribution (OOD) actions. Significant efforts have been devoted to
mitigating such generalization, and recent in-sample learning approaches have
further succeeded in entirely eschewing it. Nevertheless, we show that mild
generalization beyond the dataset can be trusted and leveraged to improve
performance under certain conditions. To appropriately exploit generalization
in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild
action generalization and (ii) mild generalization propagation. The former
refers to selecting actions in a close neighborhood of the dataset to maximize
the Q values. Even so, the potential erroneous generalization can still be
propagated, accumulated, and exacerbated by bootstrapping. In light of this,
the latter concept is introduced to mitigate the generalization propagation
without impeding the propagation of RL learning signals. Theoretically, DMG
guarantees better performance than the in-sample optimal policy in the oracle
generalization scenario. Even under worst-case generalization, DMG can still
control value overestimation at a certain level and lower bound the
performance. Empirically, DMG achieves state-of-the-art performance across
Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting
from its flexibility in both generalization aspects, DMG enjoys a seamless
transition from offline to online learning and attains strong online
fine-tuning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Power of Timestamps for Robust Time Series <span class="highlight-title">Forecast</span>ing: A
  Global-Local Fusion Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Jianxin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting has played a pivotal role across various industries,
including finance, transportation, energy, healthcare, and climate. Due to the
abundant seasonal information they contain, timestamps possess the potential to
offer robust global guidance for forecasting techniques. However, existing
works primarily focus on local observations, with timestamps being treated
merely as an optional supplement that remains underutilized. When data gathered
from the real world is polluted, the absence of global information will damage
the robust prediction capability of these algorithms. To address these
problems, we propose a novel framework named GLAFF. Within this framework, the
timestamps are modeled individually to capture the global dependencies. Working
as a plugin, GLAFF adaptively adjusts the combined weights for global and local
information, enabling seamless collaboration with any time series forecasting
backbone. Extensive experiments conducted on nine real-world datasets
demonstrate that GLAFF significantly enhances the average performance of widely
used mainstream forecasting models by 12.5%, surpassing the previous
state-of-the-art method by 5.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Xinlin Zhang, Yuanbin Chen, Yuanbo Zhou, Longxuan Zhao, Tao Tan, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has received considerable attention for its
potential to leverage abundant unlabeled data to enhance model robustness.
Pseudo labeling is a widely used strategy in semi supervised learning. However,
existing methods often suffer from noise contamination, which can undermine
model performance. To tackle this challenge, we introduce a novel
Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework.
Built upon the mean teacher network, we employ a Mix Augmentation module to
enhance the unlabeled data. By evaluating the synergy before and after
augmentation, we strategically partition the pseudo labels into distinct
regions. Additionally, we introduce a Region Loss Evaluation module to assess
the loss across each delineated area. Extensive experiments conducted on the LA
dataset have demonstrated superior performance over state-of-the-art
techniques, underscoring the efficiency and practicality of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and
  Tabnet with SMOTEENN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yixin Jin, Qianwen Xing, Ye Zhang, Shaobo Guo, Shuchen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bank credit risk is a significant challenge in modern financial transactions,
and the ability to identify qualified credit card holders among a large number
of applicants is crucial for the profitability of a bank'sbank's credit card
business. In the past, screening applicants'applicants' conditions often
required a significant amount of manual labor, which was time-consuming and
labor-intensive. Although the accuracy and reliability of previously used ML
models have been continuously improving, the pursuit of more reliable and
powerful AI intelligent models is undoubtedly the unremitting pursuit by major
banks in the financial industry. In this study, we used a dataset of over
40,000 records provided by a commercial bank as the research object. We
compared various dimensionality reduction techniques such as PCA and T-SNE for
preprocessing high-dimensional datasets and performed in-depth adaptation and
tuning of distributed models such as LightGBM and XGBoost, as well as deep
models like Tabnet. After a series of research and processing, we obtained
excellent research results by combining SMOTEENN with these techniques. The
experiments demonstrated that LightGBM combined with PCA and SMOTEENN
techniques can assist banks in accurately predicting potential high-quality
customers, showing relatively outstanding performance compared to other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pagess on IEEE ICPICS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Geospatial Inference with a Population Dynamics Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohit Agarwal, Mimi Sun, Chaitanya Kamath, Arbaaz Muslim, Prithul Sarker, Joydeep Paul, Hector Yee, Marcin Sieniek, Kim Jablonski, Yael Mayer, David Fork, Sheila de Guia, Jamie McPike, Adam Boulanger, Tomer Shekel, David Schottlander, Yao Xiao, Manjit Chakravarthy Manukonda, Yun Liu, Neslihan Bulut, Sami Abu-el-haija, Arno Eigenwillig, Parth Kothari, Bryan Perozzi, Monica Bharel, Von Nguyen, Luke Barrington, Niv Efron, Yossi Matias, Greg Corrado, Krish Eswaran, Shruthi Prabhakara, Shravya Shetty, Gautam Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supporting the health and well-being of dynamic populations around the world
requires governmental agencies, organizations and researchers to understand and
reason over complex relationships between human behavior and local contexts in
order to identify high-risk groups and strategically allocate limited
resources. Traditional approaches to these classes of problems often entail
developing manually curated, task-specific features and models to represent
human behavior and the natural and built environment, which can be challenging
to adapt to new, or even, related tasks. To address this, we introduce a
Population Dynamics Foundation Model (PDFM) that aims to capture the
relationships between diverse data modalities and is applicable to a broad
range of geospatial tasks. We first construct a geo-indexed dataset for postal
codes and counties across the United States, capturing rich aggregated
information on human behavior from maps, busyness, and aggregated search
trends, and environmental factors such as weather and air quality. We then
model this data and the complex relationships between locations using a graph
neural network, producing embeddings that can be adapted to a wide range of
downstream tasks using relatively simple models. We evaluate the effectiveness
of our approach by benchmarking it on 27 downstream tasks spanning three
distinct domains: health indicators, socioeconomic factors, and environmental
measurements. The approach achieves state-of-the-art performance on all 27
geospatial interpolation tasks, and on 25 out of the 27 extrapolation and
super-resolution tasks. We combined the PDFM with a state-of-the-art
forecasting foundation model, TimesFM, to predict unemployment and poverty,
achieving performance that surpasses fully supervised forecasting. The full set
of embeddings and sample code are publicly available for researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 16 figures, preprint; v2: updated github url</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Analyzing Meta-algorithms in Online Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Pedramfar, Vaneet Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze the problem of online convex optimization in
different settings, including different feedback types
(full-information/semi-bandit/bandit/etc) in either stochastic or
non-stochastic setting and different notions of regret (static adversarial
regret/dynamic regret/adaptive regret). This is done through a framework which
allows us to systematically propose and analyze meta-algorithms for the various
settings described above. We show that any algorithm for online linear
optimization with fully adaptive adversaries is an algorithm for online convex
optimization. We also show that any such algorithm that requires
full-information feedback may be transformed to an algorithm with semi-bandit
feedback with comparable regret bound. We further show that algorithms that are
designed for fully adaptive adversaries using deterministic semi-bandit
feedback can obtain similar bounds using only stochastic semi-bandit feedback
when facing oblivious adversaries. We use this to describe general
meta-algorithms to convert first order algorithms to zeroth order algorithms
with comparable regret bounds. Our framework allows us to analyze online
optimization in various settings, recovers several results in the literature
with a simplified proof technique, and provides new results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Verifiable Neural Network Inference Service <span class="chip">ACSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Riasi, Jorge Guajardo, Thang Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has revolutionized data analysis and pattern recognition,
but its resource-intensive training has limited accessibility. Machine Learning
as a Service (MLaaS) simplifies this by enabling users to delegate their data
samples to an MLaaS provider and obtain the inference result using a
pre-trained model. Despite its convenience, leveraging MLaaS poses significant
privacy and reliability concerns to the client. Specifically, sensitive
information from the client inquiry data can be leaked to an adversarial MLaaS
provider. Meanwhile, the lack of a verifiability guarantee can potentially
result in biased inference results or even unfair payment issues. While
existing trustworthy machine learning techniques, such as those relying on
verifiable computation or secure computation, offer solutions to privacy and
reliability concerns, they fall short of simultaneously protecting the privacy
of client data and providing provable inference verifiability.
  In this paper, we propose vPIN, a privacy-preserving and verifiable CNN
inference scheme that preserves privacy for client data samples while ensuring
verifiability for the inference. vPIN makes use of partial homomorphic
encryption and commit-and-prove succinct non-interactive argument of knowledge
techniques to achieve desirable security properties. In vPIN, we develop
various optimization techniques to minimize the proving circuit for homomorphic
inference evaluation thereby, improving the efficiency and performance of our
technique. We fully implemented and evaluated our vPIN scheme on standard
datasets (e.g., MNIST, CIFAR-10). Our experimental results show that vPIN
achieves high efficiency in terms of proving time, verification time, and proof
size, while providing client data privacy guarantees and provable
verifiability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Annual Computer Security Applications Conference
  (ACSAC) 2024. Source code: github.com/vt-asaplab/vPIN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Memory Mechanisms for Decision Making through Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yue, Bo Liu, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Partially Observable Markov Decision Processes, integrating an agent's
history into memory poses a significant challenge for decision-making.
Traditional imitation learning, relying on observation-action pairs for expert
demonstrations, fails to capture the expert's memory mechanisms used in
decision-making. To capture memory processes as demonstrations, we introduce
the concept of memory dependency pairs $(p, q)$ indicating that events at time
$p$ are recalled for decision-making at time $q$. We introduce AttentionTuner
to leverage memory dependency pairs in Transformers and find significant
improvements across several tasks compared to standard Transformers when
evaluated on Memory Gym and the Long-term Memory Benchmark. Code is available
at https://github.com/WilliamYue37/AttentionTuner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation
  Models, Convolutional Neural Networks, and Uncertainty Quantification for
  High-Speed Video Phase Detection Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chika Maduabuchi, Ericmoore Jossou, Matteo Bucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: High-speed video (HSV) phase detection (PD) segmentation is vital in
nuclear reactors, chemical processing, and electronics cooling for detecting
vapor, liquid, and microlayer phases. Traditional segmentation models face
pixel-level accuracy and generalization issues in multimodal data. MSEG-VCUQ
introduces VideoSAM, a hybrid framework leveraging convolutional neural
networks (CNNs) and transformer-based vision models to enhance segmentation
accuracy and generalizability across complex multimodal PD tasks. Methods:
VideoSAM combines U-Net CNN and the Segment Anything Model (SAM) for advanced
feature extraction and segmentation across diverse HSV PD modalities, spanning
fluids like water, FC-72, nitrogen, and argon under varied heat flux
conditions. The framework also incorporates uncertainty quantification (UQ) to
assess pixel-based discretization errors, delivering reliable metrics such as
contact line density and dry area fraction under experimental conditions.
Results: VideoSAM outperforms SAM and modality-specific CNN models in
segmentation accuracy, excelling in environments with complex phase boundaries,
overlapping bubbles, and dynamic liquid-vapor interactions. Its hybrid
architecture supports cross-dataset generalization, adapting effectively to
varying modalities. The UQ module provides accurate error estimates, enhancing
the reliability of segmentation outputs for advanced HSV PD research.
Conclusion: MSEG-VCUQ, via VideoSAM, offers a robust solution for HSV PD
segmentation, addressing previous limitations with advanced deep learning and
UQ techniques. The open-source datasets and tools introduced enable scalable,
precise, and adaptable segmentation for multimodal PD datasets, supporting
advancements in HSV analysis and autonomous experimentation. The codes and data
used for this paper are publicly available at:
\url{https://github.com/chikap421/mseg_vcuq}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in EAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation
  in EEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanglin Li, Motoaki Kawanabe, Reinmar J. Kobler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The non-stationary nature of electroencephalography (EEG) introduces
distribution shifts across domains (e.g., days and subjects), posing a
significant challenge to EEG-based neurotechnology generalization. Without
labeled calibration data for target domains, the problem is a source-free
unsupervised domain adaptation (SFUDA) problem. For scenarios with constant
label distribution, Riemannian geometry-aware statistical alignment frameworks
on the symmetric positive definite (SPD) manifold are considered
state-of-the-art. However, many practical scenarios, including EEG-based sleep
staging, exhibit label shifts. Here, we propose a geometric deep learning
framework for SFUDA problems under specific distribution shifts, including
label shifts. We introduce a novel, realistic generative model and show that
prior Riemannian statistical alignment methods on the SPD manifold can
compensate for specific marginal and conditional distribution shifts but hurt
generalization under label shifts. As a remedy, we propose a
parameter-efficient manifold optimization strategy termed SPDIM. SPDIM uses the
information maximization principle to learn a single SPD-manifold-constrained
parameter per target domain. In simulations, we demonstrate that SPDIM can
compensate for the shifts under our generative model. Moreover, using public
EEG-based brain-computer interface and sleep staging datasets, we show that
SPDIM outperforms prior approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Complexity of Opinion Formation on Networks with Linear
  Regression Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Liu, Rajmohan Rajaraman, Ravi Sundaram, Anil Vullikanti, Omer Wasim, Haifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider public health officials aiming to spread awareness about a new
vaccine in a community interconnected by a social network. How can they
distribute information with minimal resources, so as to avoid polarization and
ensure community-wide convergence of opinion? To tackle such challenges, we
initiate the study of sample complexity of opinion convergence in networks. Our
framework is built on the recognized opinion formation game, where we regard
the opinion of each agent as a data-derived model, unlike previous works that
treat opinions as data-independent scalars. The opinion model for every agent
is initially learned from its local samples and evolves game-theoretically as
all agents communicate with neighbors and revise their models towards an
equilibrium. Our focus is on the sample complexity needed to ensure that the
opinions converge to an equilibrium such that the final model of every agent
has low generalization error.
  Our paper has two main technical results. First, we present a novel
polynomial time optimization framework to quantify the total sample complexity
for arbitrary networks, when the underlying learning problem is (generalized)
linear regression. Second, we leverage this optimization to study the network
gain which measures the improvement of sample complexity when learning over a
network compared to that in isolation. Towards this end, we derive network gain
bounds for various network classes including cliques, star graphs, and random
regular graphs. Additionally, our framework provides a method to study sample
distribution within the network, suggesting that it is sufficient to allocate
samples inversely to the degree. Empirical results on both synthetic and
real-world networks strongly support our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIOSCAN-5M: A Multimodal <span class="highlight-title">Dataset</span> for Insect Biodiversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12723v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12723v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Gharaee, Scott C. Lowe, ZeMing Gong, Pablo Millan Arias, Nicholas Pellegrino, Austin T. Wang, Joakim Bruslund Haurum, Iuliia Zarubiieva, Lila Kari, Dirk Steinke, Graham W. Taylor, Paul Fieguth, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As part of an ongoing worldwide effort to comprehend and monitor insect
biodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine
learning community and establish several benchmark tasks. BIOSCAN-5M is a
comprehensive dataset containing multi-modal information for over 5 million
insect specimens, and it significantly expands existing image-based biological
datasets by including taxonomic labels, raw nucleotide barcode sequences,
assigned barcode index numbers, geographical, and size information. We propose
three benchmark experiments to demonstrate the impact of the multi-modal data
types on the classification and clustering accuracy. First, we pretrain a
masked language model on the DNA barcode sequences of the BIOSCAN-5M dataset,
and demonstrate the impact of using this large reference library on species-
and genus-level classification performance. Second, we propose a zero-shot
transfer learning task applied to images and DNA barcodes to cluster feature
embeddings obtained from self-supervised learning, to investigate whether
meaningful clusters can be derived from these representation embeddings. Third,
we benchmark multi-modality by performing contrastive learning on DNA barcodes,
image data, and taxonomic information. This yields a general shared embedding
space enabling taxonomic classification using multiple types of information and
modalities. The code repository of the BIOSCAN-5M Insect dataset is available
at https://github.com/bioscan-ml/BIOSCAN-5M.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretability Needs a New Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Madsen, Himabindu Lakkaraju, Siva Reddy, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is the study of explaining models in understandable terms to
humans. At present, interpretability is divided into two paradigms: the
intrinsic paradigm, which believes that only models designed to be explained
can be explained, and the post-hoc paradigm, which believes that black-box
models can be explained. At the core of this debate is how each paradigm
ensures its explanations are faithful, i.e., true to the model's behavior. This
is important, as false but convincing explanations lead to unsupported
confidence in artificial intelligence (AI), which can be dangerous. This
paper's position is that we should think about new paradigms while staying
vigilant regarding faithfulness. First, by examining the history of paradigms
in science, we see that paradigms are constantly evolving. Then, by examining
the current paradigms, we can understand their underlying beliefs, the value
they bring, and their limitations. Finally, this paper presents 3 emerging
paradigms for interpretability. The first paradigm designs models such that
faithfulness can be easily measured. Another optimizes models such that
explanations become faithful. The last paradigm proposes to develop models that
produce both a prediction and an explanation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Emulation of a Global Climate Model with Spherical
  DYffusion <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salva Rühling Cachay, Brian Henn, Oliver Watt-Meyer, Christopher S. Bretherton, Rose Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven deep learning models are transforming global weather forecasting.
It is an open question if this success can extend to climate modeling, where
the complexity of the data and long inference rollouts pose significant
challenges. Here, we present the first conditional generative model that
produces accurate and physically consistent global climate ensemble simulations
by emulating a coarse version of the United States' primary operational global
forecast model, FV3GFS. Our model integrates the dynamics-informed diffusion
framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO)
architecture, enabling stable 100-year simulations at 6-hourly timesteps while
maintaining low computational overhead compared to single-step deterministic
baselines. The model achieves near gold-standard performance for climate model
emulation, outperforming existing approaches and demonstrating promising
ensemble skill. This work represents a significant advance towards efficient,
data-driven climate simulations that can enhance our understanding of the
climate system and inform adaptation strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; Code is available at
  https://github.com/Rose-STL-Lab/spherical-dyffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taming Latent <span class="highlight-title">Diffusion</span> Model for Neural Radiance Field Inpainting <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) is a representation for 3D reconstruction from
multi-view images. Despite some recent work showing preliminary success in
editing a reconstructed NeRF with diffusion prior, they remain struggling to
synthesize reasonable geometry in completely uncovered regions. One major
reason is the high diversity of synthetic contents from the diffusion model,
which hinders the radiance field from converging to a crisp and deterministic
geometry. Moreover, applying latent diffusion models on real data often yields
a textural shift incoherent to the image condition due to auto-encoding errors.
These two problems are further reinforced with the use of pixel-distance
losses. To address these issues, we propose tempering the diffusion model's
stochasticity with per-scene customization and mitigating the textural shift
with masked adversarial training. During the analyses, we also found the
commonly used pixel and perceptual losses are harmful in the NeRF inpainting
task. Through rigorous experiments, our framework yields state-of-the-art NeRF
inpainting results on various real-world scenes. Project page:
https://hubert0527.github.io/MALD-NeRF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Project page:
  https://hubert0527.github.io/MALD-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super Consistency of Neural Network Landscapes and Learning Rate
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing evidence that if the width and depth of a
neural network are scaled toward the so-called rich feature learning limit
(\mup and its depth extension), then some hyperparameters -- such as the
learning rate -- exhibit transfer from small to very large models. From an
optimization perspective, this phenomenon is puzzling, as it implies that the
loss landscape is consistently similar across very different model sizes. In
this work, we study the landscape through the lens of the loss Hessian, with a
focus on its largest eigenvalue (i.e. the sharpness), and find that certain
spectral properties under $\mu$P are largely independent of the size of the
network, and remain consistent as training progresses. We name this property
Super Consistency of the landscape. On the other hand, we show that in the
Neural Tangent Kernel (NTK) and other scaling regimes, the sharpness exhibits
very different dynamics at different scales. But what causes these differences
in the sharpness dynamics? Through a connection between the Hessian's and the
NTK's spectrum, we argue that the cause lies in the presence (for $\mu$P) or
progressive absence (for the NTK scaling) of feature learning. We corroborate
our claims with a substantial suite of experiments, covering a wide range of
datasets and architectures: from ResNets and Vision Transformers trained on
benchmark vision datasets to Transformers-based language models trained on
WikiText.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at Neurips 2024. This is a revised
  version of the paper previously titled "Why do Learning Rates Transfer?
  Reconciling Optimization and Scaling Limits for Deep Learning"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "No Matter What You Do": Purifying GNN Models via Backdoor Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Zhang, Chengcheng Zhu, Bosen Rao, Hao Sui, Xiaobing Sun, Bing Chen, Chunyi Zhou, Shouling Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have exposed that GNNs are vulnerable to several adversarial
attacks, among which backdoor attack is one of the toughest. Similar to Deep
Neural Networks (DNNs), backdoor attacks in GNNs lie in the fact that the
attacker modifies a portion of graph data by embedding triggers and enforces
the model to learn the trigger feature during the model training process.
Despite the massive prior backdoor defense works on DNNs, defending against
backdoor attacks in GNNs is largely unexplored, severely hindering the
widespread application of GNNs in real-world tasks. To bridge this gap, we
present GCleaner, the first backdoor mitigation method on GNNs. GCleaner can
mitigate the presence of the backdoor logic within backdoored GNNs by reversing
the backdoor learning procedure, aiming to restore the model performance to a
level similar to that is directly trained on the original clean dataset. To
achieve this objective, we ask: How to recover universal and hard backdoor
triggers in GNNs? How to unlearn the backdoor trigger feature while maintaining
the model performance? We conduct the graph trigger recovery via the
explanation method to identify optimal trigger locations, facilitating the
search of universal and hard backdoor triggers in the feature space of the
backdoored model through maximal similarity. Subsequently, we introduce the
backdoor unlearning mechanism, which combines knowledge distillation and
gradient-based explainable knowledge for fine-grained backdoor erasure.
Extensive experimental evaluations on four benchmark datasets demonstrate that
GCleaner can reduce the backdoor attack success rate to 10% with only 1% of
clean data, and has almost negligible degradation in model performance, which
far outperforms the state-of-the-art (SOTA) defense methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Prep-Kit: getting your data ready for <span class="highlight-title">LLM</span> application development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wood, Boris Lublinsky, Alexy Roytman, Shivdeep Singh, Constantin Adam, Abdulhamid Adebayo, Sungeun An, Yuan Chi Chang, Xuan-Hong Dang, Nirmit Desai, Michele Dolfi, Hajar Emami-Gohari, Revital Eres, Takuya Goto, Dhiraj Joshi, Yan Koyfman, Mohammad Nassar, Hima Patel, Paramesvaran Selvam, Yousaf Shah, Saptha Surendran, Daiki Tsuzuku, Petros Zerfos, Shahrokh Daijavad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural-Rendezvous: Provably Robust Guidance and Control to Encounter
  Interstellar Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04883v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04883v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroyasu Tsukamoto, Soon-Jo Chung, Yashwanth Kumar Nakka, Benjamin Donitz, Declan Mages, Michel Ingham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interstellar objects (ISOs) are likely representatives of primitive materials
invaluable in understanding exoplanetary star systems. Due to their poorly
constrained orbits with generally high inclinations and relative velocities,
however, exploring ISOs with conventional human-in-the-loop approaches is
significantly challenging. This paper presents Neural-Rendezvous -- a deep
learning-based guidance and control framework for encountering fast-moving
objects, including ISOs, robustly, accurately, and autonomously in real time.
It uses pointwise minimum norm tracking control on top of a guidance policy
modeled by a spectrally-normalized deep neural network, where its
hyperparameters are tuned with a loss function directly penalizing the MPC
state trajectory tracking error. We show that Neural-Rendezvous provides a high
probability exponential bound on the expected spacecraft delivery error, the
proof of which leverages stochastic incremental stability analysis. In
particular, it is used to construct a non-negative function with a
supermartingale property, explicitly accounting for the ISO state uncertainty
and the local nature of nonlinear state estimation guarantees. In numerical
simulations, Neural-Rendezvous is demonstrated to satisfy the expected error
bound for 100 ISO candidates. This performance is also empirically validated
using our spacecraft simulator and in high-conflict and distributed UAV swarm
reconfiguration with up to 20 UAVs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version, Accepted: October, 2024 (One-minute YouTube
  summary: https://youtu.be/q3e0LYS2IYQ, DOI:
  https://doi.org/10.2514/1.G007671)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Selection Based on Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07217v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07217v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel feature selection method leveraging the
Wasserstein distance to improve feature selection in machine learning. Unlike
traditional methods based on correlation or Kullback-Leibler (KL) divergence,
our approach uses the Wasserstein distance to assess feature similarity,
inherently capturing class relationships and making it robust to noisy labels.
We introduce a Markov blanket-based feature selection algorithm and demonstrate
its effectiveness. Our analysis shows that the Wasserstein distance-based
feature selection method effectively reduces the impact of noisy labels without
relying on specific noise models. We provide a lower bound on its
effectiveness, which remains meaningful even in the presence of noise.
Experimental results across multiple datasets demonstrate that our approach
consistently outperforms traditional methods, particularly in noisy settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Advantages of Perturbing Cosine Router in Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Nguyen, Pedram Akbarian, Trang Pham, Trang Nguyen, Shujian Zhang, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cosine router in Mixture of Experts (MoE) has recently emerged as an
attractive alternative to the conventional linear router. Indeed, the cosine
router demonstrates favorable performance in image and language tasks and
exhibits better ability to mitigate the representation collapse issue, which
often leads to parameter redundancy and limited representation potentials.
Despite its empirical success, a comprehensive analysis of the cosine router in
MoE has been lacking. Considering the least square estimation of the cosine
routing MoE, we demonstrate that due to the intrinsic interaction of the model
parameters in the cosine router via some partial differential equations,
regardless of the structures of the experts, the estimation rates of experts
and model parameters can be as slow as $\mathcal{O}(1/\log^{\tau}(n))$ where
$\tau > 0$ is some constant and $n$ is the sample size. Surprisingly, these
pessimistic non-polynomial convergence rates can be circumvented by the widely
used technique in practice to stabilize the cosine router -- simply adding
noises to the $L^2$ norms in the cosine router, which we refer to as
\textit{perturbed cosine router}. Under the strongly identifiable settings of
the expert functions, we prove that the estimation rates for both the experts
and model parameters under the perturbed cosine routing MoE are significantly
improved to polynomial rates. Finally, we conduct extensive simulation studies
in both synthetic and real data settings to empirically validate our
theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occam Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20194v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20194v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. N. Kausik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning neural network models must be large enough to adapt to their
problem domain, while small enough to avoid overfitting training data during
gradient descent. To balance these competing demands, overprovisioned deep
learning models such as transformers are trained for a single epoch on large
data sets, and hence inefficient with both computing resources and training
data. In response to these inefficiencies, we exploit learning theory to derive
Occam Gradient Descent, an algorithm that interleaves adaptive reduction of
model size to minimize generalization error, with gradient descent on model
weights to minimize fitting error. In contrast, traditional gradient descent
greedily minimizes fitting error without regard to generalization error. Our
algorithm simultaneously descends the space of weights and topological size of
any neural network without modification. With respect to loss, compute and
model size, our experiments show (a) on image classification benchmarks, linear
and convolutional neural networks trained with Occam Gradient Descent
outperform traditional gradient descent with or without post-train pruning; (b)
on a range of tabular data classification tasks, neural networks trained with
Occam Gradient Descent outperform traditional gradient descent, as well as
Random Forests; (c) on natural language transformers, Occam Gradient Descent
outperforms traditional gradient descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Effects Deep Learning for the interpretable analysis of single
  cell RNA sequencing data by quantifying and visualizing batch effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aixa X. Andrade, Son Nguyen, Albert Montillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-cell RNA sequencing (scRNA-seq) data are often confounded by technical
or biological batch effects. Existing deep learning models mitigate these
effects but often discard batch-specific information, potentially losing
valuable biological insights. We propose a Mixed Effects Deep Learning (MEDL)
autoencoder framework that separately models batch-invariant (fixed effects)
and batch-specific (random effects) components. By decoupling batch-invariant
biological states from batch variations, our framework integrates both into
predictive models. Our approach also generates 2D visualizations of how the
same cell appears across batches, enhancing interpretability. Retaining both
fixed and random effect latent spaces improves classification accuracy.
  We applied our framework to three datasets spanning the cardiovascular system
(Healthy Heart), Autism Spectrum Disorder (ASD), and Acute Myeloid Leukemia
(AML). With 147 batches in the Healthy Heart dataset, far exceeding typical
numbers, we tested our framework's ability to handle many batches. In the ASD
dataset, our approach captured donor heterogeneity between autistic and healthy
individuals. In the AML dataset, it distinguished donor heterogeneity despite
missing cell types and diseased donors exhibiting both healthy and malignant
cells. These results highlight our framework's ability to characterize fixed
and random effects, enhance batch effect visualization, and improve prediction
accuracy across diverse datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main manuscript: 29 pages, including 10 figures and 8 tables.
  Supplemental material: 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete
  Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etrit Haxholli, Yeti Z. Gürbüz, Oğul Can, Eli Waxman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outperforming autoregressive models on categorical data distributions, such
as textual data, remains challenging for continuous diffusion and flow models.
Discrete flow matching, a recent framework for modeling categorical data, has
shown competitive performance with autoregressive models. Despite its
similarities with continuous flow matching, the rectification strategy applied
in the continuous version does not directly extend to the discrete one due to
the inherent stochasticity of discrete paths. This limitation necessitates
exploring alternative methods to minimize state transitions during generation.
To address this, we propose a dynamic-optimal-transport-like minimization
objective for discrete flows with convex interpolants and derive its equivalent
Kantorovich formulation. The latter defines transport cost solely in terms of
inter-state similarity and is optimized using a minibatch strategy. Another
limitation we address in the discrete flow framework is model evaluation.
Unlike continuous flows, wherein the instantaneous change of variables enables
density estimation, discrete models lack a similar mechanism due to the
inherent non-determinism and discontinuity of their paths. To alleviate this
issue, we propose an upper bound on the perplexity of discrete flow models,
enabling performance evaluation and comparison with other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A metric embedding kernel for live cell microscopy signaling patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here a metric
kernel function for spatiotemporal patterns of cell signaling dynamics in 5-D
live cell microscopy movies unique in requiring no a priori knowledge of
expected pattern dynamics, and no training data. The approach uses Kolmogorov
complexity theory to compute a metric distance between movies and to measure
the meaningful information among subsets of movies. Cell signaling kymographs
store at each spatiotemporal cell centroid the cell signaling state, or a
functional output such as velocity. Patterns of similarity are identified via
the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input cell signaling kymographs
as points in a low dimensional embedding that optimally captures the pattern
similarity identified by the NCD throughout the space. The only parameter is
the expected cell radii ($\mu m$). A new formulation of the cluster structure
function optimally estimates the meaningful information captured by the
embedding. Also presented is the cell signaling structure function (SSF), a
Kolmogorov structure function that optimally measures cell signaling state as
nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement
compared to the current state-of-the-art cytonuclear ratio. Results are
presented quantifying the impact of ERK and AKT signaling between different
oncogenic mutations, and by the relation between ERK signaling and cellular
velocity patterns for movies of 2-D monolayers of human breast epithelial
(MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation of ERK, and
human induced pluripotent stem cells.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Newton Method for Optimization in Complex Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Yudin, Roland Hildebrand, Sergey Bakhurin, Alexander Degtyarev, Anna Lisachenko, Ilya Kuruzov, Andrei Semenov, Mohammad Alkousa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we modify and apply the recently introduced Mixed Newton
Method, which is originally designed for minimizing real-valued functions of
complex variables, to the minimization of real-valued functions of real
variables by extending the functions to complex space. We show that arbitrary
regularizations preserve the favorable local convergence properties of the
method, and construct a special type of regularization used to prevent
convergence to complex minima. We compare several variants of the method
applied to training neural networks with real and complex parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Global Simulation of Nonlocal Gravity Wave Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Gupta, Aditi Sheshadri, Sujit Roy, Vishal Gaur, Manil Maskey, Rahul Ramachandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global climate models typically operate at a grid resolution of hundreds of
kilometers and fail to resolve atmospheric mesoscale processes, e.g., clouds,
precipitation, and gravity waves (GWs). Model representation of these processes
and their sources is essential to the global circulation and planetary energy
budget, but subgrid scale contributions from these processes are often only
approximately represented in models using parameterizations. These
parameterizations are subject to approximations and idealizations, which limit
their capability and accuracy. The most drastic of these approximations is the
"single-column approximation" which completely neglects the horizontal
evolution of these processes, resulting in key biases in current climate
models. With a focus on atmospheric GWs, we present the first-ever global
simulation of atmospheric GW fluxes using machine learning (ML) models trained
on the WINDSET dataset to emulate global GW emulation in the atmosphere, as an
alternative to traditional single-column parameterizations. Using an Attention
U-Net-based architecture trained on globally resolved GW momentum fluxes, we
illustrate the importance and effectiveness of global nonlocality, when
simulating GWs using data-driven schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending <span class="highlight-title">Large Language Model</span>s Against Attacks With Residual Stream
  Activation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03230v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03230v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amelia Kawasaki, Andrew Davis, Houssam Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of Large Language Models (LLMs), exemplified by
OpenAI's ChatGPT, brings to the forefront the imperative to defend against
adversarial threats on these models. These attacks, which manipulate an LLM's
output by introducing malicious inputs, undermine the model's integrity and the
trust users place in its outputs. In response to this challenge, our paper
presents an innovative defensive strategy, given white box access to an LLM,
that harnesses residual activation analysis between transformer layers of the
LLM. We apply a novel methodology for analyzing distinctive activation patterns
in the residual streams for attack prompt classification. We curate multiple
datasets to demonstrate how this method of classification has high accuracy
across multiple types of attack scenarios, including our newly-created attack
dataset. Furthermore, we enhance the model's resilience by integrating safety
fine-tuning techniques for LLMs in order to measure its effect on our
capability to detect attacks. The results underscore the effectiveness of our
approach in enhancing the detection and mitigation of adversarial inputs,
advancing the security framework within which LLMs operate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MDCure: A Scalable Pipeline for Multi-Document Instruction-Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document (MD) processing is crucial for LLMs to handle real-world tasks
such as summarization and question-answering across large sets of documents.
While LLMs have improved at processing long inputs, MD contexts still present
challenges, such as managing inter-document dependencies, redundancy, and
incoherent structures. We introduce MDCure, a scalable and effective
fine-tuning pipeline to enhance the MD capabilities of LLMs without the
computational cost of pre-training or reliance on human annotated data. MDCure
is based on generation of high-quality synthetic MD instruction data from sets
of related articles via targeted prompts. We further introduce MDCureRM, a
multi-objective reward model which filters generated data based on their
training utility for MD settings. With MDCure, we fine-tune a variety of LLMs,
from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in
size. Extensive evaluations on a wide range of MD and long-context benchmarks
spanning various tasks show MDCure consistently improves performance over
pre-trained baselines and over corresponding base models by up to 75.5%. Our
code, datasets, and models are available at https://github.com/yale-nlp/MDCure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable AI through a Democratic Lens: DhondtXAI for Proportional
  Feature Importance Using the D'Hondt Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Turker Berk Donmez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In democratic societies, electoral systems play a crucial role in translating
public preferences into political representation. Among these, the D'Hondt
method is widely used to ensure proportional representation, balancing fair
representation with governmental stability. Recently, there has been a growing
interest in applying similar principles of proportional representation to
enhance interpretability in machine learning, specifically in Explainable AI
(XAI). This study investigates the integration of D'Hondt-based voting
principles in the DhondtXAI method, which leverages resource allocation
concepts to interpret feature importance within AI models. Through a comparison
of SHAP (Shapley Additive Explanations) and DhondtXAI, we evaluate their
effectiveness in feature attribution within CatBoost and XGBoost models for
breast cancer and diabetes prediction, respectively. The DhondtXAI approach
allows for alliance formation and thresholding to enhance interpretability,
representing feature importance as seats in a parliamentary view. Statistical
correlation analyses between SHAP values and DhondtXAI allocations support the
consistency of interpretations, demonstrating DhondtXAI's potential as a
complementary tool for understanding feature importance in AI models. The
results highlight that integrating electoral principles, such as proportional
representation and alliances, into AI explainability can improve user
understanding, especially in high-stakes fields like healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Trigger Detection: Accelerating Real-time
  Tracking-by-detection Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1902.00615v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1902.00615v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Ding, Zhixin Lai, Siyang Li, Panfeng Li, Qikai Yang, Edward Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time object tracking necessitates a delicate balance between speed and
accuracy, a challenge exacerbated by the computational demands of deep learning
methods. In this paper, we propose Confidence-Triggered Detection (CTD), an
innovative approach that strategically bypasses object detection for frames
closely resembling intermediate states, leveraging tracker confidence scores.
CTD not only enhances tracking speed but also preserves accuracy, surpassing
existing tracking algorithms. Through extensive evaluation across various
tracker confidence thresholds, we identify an optimal trade-off between
tracking speed and accuracy, providing crucial insights for parameter
fine-tuning and enhancing CTD's practicality in real-world scenarios. Our
experiments across diverse detection models underscore the robustness and
versatility of the CTD framework, demonstrating its potential to enable
real-time tracking in resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-12T00:00:00Z">2024-11-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">86</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language Model</span>s as Causal Effect Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius E. J. Bynum, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpressivityArena: Can <span class="highlight-title">LLM</span>s Express Information Implicitly? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tint, Som Sagar, Aditya Taparia, Kelly Raines, Bimsara Pathiraja, Caleb Liu, Ransalu Senanayake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have demonstrated remarkable performance
in certain dimensions, their ability to express implicit language cues that
human use for effective communication remains unclear. This paper presents
ExpressivityArena, a Python library for measuring the implicit communication
abilities of LLMs. We provide a comprehensive framework to evaluate
expressivity of arbitrary LLMs and explore its practical implications. To this
end, we refine the definition and measurements of ``expressivity,'' and use our
framework in a set of small experiments. These experiments test LLMs in
creative and logical tasks such as poetry, coding, and emotion-based responses.
They are then evaluated by an automated grader, through ExpressivityArena,
which we verify to be the most pragmatic for testing expressivity. Building on
these experiments, we deepen our understanding of the expressivity of LLMs by
assessing their ability to remain expressive in conversations. Our findings
indicate that LLMs are capable of generating and understanding expressive
content, however, with some limitations. These insights will inform the future
development and deployment of expressive LLMs. We provide the code for
ExpressivityArena alongside our paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can adversarial attacks by <span class="highlight-title">large language model</span>s be attributed? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Cebrian, Jan Arne Telle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attributing outputs from Large Language Models (LLMs) in adversarial
settings-such as cyberattacks and disinformation-presents significant
challenges that are likely to grow in importance. We investigate this
attribution problem using formal language theory, specifically language
identification in the limit as introduced by Gold and extended by Angluin. By
modeling LLM outputs as formal languages, we analyze whether finite text
samples can uniquely pinpoint the originating model. Our results show that due
to the non-identifiability of certain language classes, under some mild
assumptions about overlapping outputs from fine-tuned models it is
theoretically impossible to attribute outputs to specific LLMs with certainty.
This holds also when accounting for expressivity limitations of Transformer
architectures. Even with direct model access or comprehensive monitoring,
significant computational hurdles impede attribution efforts. These findings
highlight an urgent need for proactive measures to mitigate risks posed by
adversarial LLM use as their influence continues to expand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivational Morphology Reveals Analogical Generalization in Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Schütze, Janet Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified
  Multimodal Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From General to Specific: Utilizing General Hallucation to Automatically
  Measure the Role Relationship Fidelity for Specific Role-Play <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advanced role-playing capabilities of Large Language Models (LLMs) have
paved the way for developing Role-Playing Agents (RPAs). However, existing
benchmarks, such as HPD, which incorporates manually scored character
relationships into the context for LLMs to sort coherence, and SocialBench,
which uses specific profiles generated by LLMs in the context of
multiple-choice tasks to assess character preferences, face limitations like
poor generalizability, implicit and inaccurate judgments, and excessive context
length. To address the above issues, we propose an automatic, scalable, and
generalizable paradigm. Specifically, we construct a benchmark by extracting
relations from a general knowledge graph and leverage RPA's inherent
hallucination properties to prompt it to interact across roles, employing
ChatGPT for stance detection and defining relationship hallucination along with
three related metrics. Extensive experiments validate the effectiveness and
stability of our metrics. Our findings further explore factors influencing
these metrics and discuss the trade-off between relationship hallucination and
factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crypto<span class="highlight-title">LLM</span>: Unleashing the Power of <span class="highlight-title">Prompt</span>ed <span class="highlight-title">LLM</span>s for SmartQnA and
  Classification of Crypto Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of social media has resulted in an large volume of
user-generated content, particularly in niche domains such as cryptocurrency.
This task focuses on developing robust classification models to accurately
categorize cryptocurrency-related social media posts into predefined classes,
including but not limited to objective, positive, negative, etc. Additionally,
the task requires participants to identify the most relevant answers from a set
of posts in response to specific questions. By leveraging advanced LLMs, this
research aims to enhance the understanding and filtering of cryptocurrency
discourse, thereby facilitating more informed decision-making in this volatile
sector. We have used a prompt-based technique to solve the classification task
for reddit posts and twitter posts. Also, we have used 64-shot technique along
with prompts on GPT-4-Turbo model to determine whether a answer is relevant to
a question or not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Opinion Extraction and Question
  Answering from CryptoCurrency-Related Tweets and Reddit posts (CryptOQA))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping the Podcast Ecosystem with the Structured Podcast Research
  Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Litterer, David Jurgens, Dallas Card
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Podcasts provide highly diverse content to a massive listener base through a
unique on-demand modality. However, limited data has prevented large-scale
computational analysis of the podcast ecosystem. To fill this gap, we introduce
a massive dataset of over 1.1M podcast transcripts that is largely
comprehensive of all English language podcasts available through public RSS
feeds from May and June of 2020. This data is not limited to text, but rather
includes audio features and speaker turns for a subset of 370K episodes, and
speaker role inferences and other metadata for all 1.1M episodes. Using this
data, we also conduct a foundational investigation into the content, structure,
and responsiveness of this ecosystem. Together, our data and analyses open the
door to continued computational research of this popular and impactful medium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustful <span class="highlight-title">LLM</span>s: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Jaya Krishna Mandivarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When unsure about an answer, humans often respond with more words than
necessary, hoping that part of the response will be correct. We observe a
similar behavior in large language models (LLMs), which we term "Verbosity
Compensation" (VC). VC is harmful because it confuses the user understanding,
leading to low efficiency, and influences the LLM services by increasing the
latency and cost of generating useless tokens. In this paper, we present the
first work that defines and analyzes Verbosity Compensation, explores its
causes, and proposes a simple mitigating approach. We define Verbosity
Compensation as the behavior of generating responses that can be compressed
without information loss when prompted to write concisely. Our experiments,
conducted on five datasets of knowledge and reasoning-based QA tasks with 14
newly developed LLMs, reveal three conclusions. 1) We reveal a pervasive
presence of verbosity compensation across all models and all datasets. Notably,
GPT-4 exhibits a VC frequency of 50.40%. 2) We reveal the large performance gap
between verbose and concise responses, with a notable difference of 27.61% on
the Qasper dataset. We also demonstrate that this difference does not naturally
diminish as LLM capability increases. Both 1) and 2) highlight the urgent need
to mitigate the frequency of VC behavior and disentangle verbosity with
veracity. We propose a simple yet effective cascade algorithm that replaces the
verbose responses with the other model-generated responses. The results show
that our approach effectively alleviates the VC of the Mistral model from
63.81% to 16.16% on the Qasper dataset. 3) We also find that verbose responses
exhibit higher uncertainty across all five datasets, suggesting a strong
connection between verbosity and model uncertainty. Our dataset and code are
available at https://github.com/psunlpgroup/VerbosityLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tucano: Advancing Neural Text Generation for Portuguese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kluge Corrêa, Aniket Sen, Sophia Falk, Shiza Fatimah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyin Yi, Jiacheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples, which are inputs deliberately perturbed with
imperceptible changes to induce model errors, have raised serious concerns for
the reliability and security of deep neural networks (DNNs). While adversarial
attacks have been extensively studied in continuous data domains such as
images, the discrete nature of text presents unique challenges. In this paper,
we propose Irony-based Adversarial Examples (IAE), a method that transforms
straightforward sentences into ironic ones to create adversarial text. This
approach exploits the rhetorical device of irony, where the intended meaning is
opposite to the literal interpretation, requiring a deeper understanding of
context to detect. The IAE method is particularly challenging due to the need
to accurately locate evaluation words, substitute them with appropriate
collocations, and expand the text with suitable ironic elements while
maintaining semantic coherence. Our research makes the following key
contributions: (1) We introduce IAE, a strategy for generating textual
adversarial examples using irony. This method does not rely on pre-existing
irony corpora, making it a versatile tool for creating adversarial text in
various NLP tasks. (2) We demonstrate that the performance of several
state-of-the-art deep learning models on sentiment analysis tasks significantly
deteriorates when subjected to IAE attacks. This finding underscores the
susceptibility of current NLP systems to adversarial manipulation through
irony. (3) We compare the impact of IAE on human judgment versus NLP systems,
revealing that humans are less susceptible to the effects of irony in text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics
  Statements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonia Karamolegkou, Sandrine Schiller Hansen, Ariadni Christopoulou, Filippos Stamatiou, Anne Lauscher, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What ethical concerns, if any, do LLM researchers have? We introduce EthiCon,
a corpus of 1,580 ethical concern statements extracted from scientific papers
published in the ACL Anthology. We extract ethical concern keywords from the
statements and show promising results in automating the concern identification
process. Through a survey, we compare the ethical concerns of the corpus to the
concerns listed by the general public and professionals in the field. Finally,
we compare our retrieved ethical concerns with existing taxonomies pointing to
gaps and future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain Association-based Attacking and Shielding Natural Language
  Processing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Huang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association as a gift enables people do not have to mention something in
completely straightforward words and allows others to understand what they
intend to refer to. In this paper, we propose a chain association-based
adversarial attack against natural language processing systems, utilizing the
comprehension gap between humans and machines. We first generate a chain
association graph for Chinese characters based on the association paradigm for
building search space of potential adversarial examples. Then, we introduce an
discrete particle swarm optimization algorithm to search for the optimal
adversarial examples. We conduct comprehensive experiments and show that
advanced natural language processing models and applications, including large
language models, are vulnerable to our attack, while humans appear good at
understanding the perturbed text. We also explore two methods, including
adversarial training and associative graph-based recovery, to shield systems
from chain association-based attack. Since a few examples that use some
derogatory terms, this paper contains materials that may be offensive or
upsetting to some people.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the \textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a
novel approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likelihood as a Performance Gauge for Retrieval-Augmented Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at NAACL 2025. Code is available at
  https://github.com/lyutyuh/poptimizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text, 3 figures
  in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Spider 2.0: Evaluating <span class="highlight-title">Language Model</span>s on Real-World Enterprise
  Text-to-SQL Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, <span class="highlight-author">Tao Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world enterprise text-to-SQL workflows often involve complex cloud or
local data across various database systems, multiple SQL queries in various
dialects, and diverse operations from data transformation to analytics. We
introduce Spider 2.0, an evaluation framework comprising 632 real-world
text-to-SQL workflow problems derived from enterprise-level database use cases.
The databases in Spider 2.0 are sourced from real data applications, often
containing over 1,000 columns and stored in local or cloud database systems
such as BigQuery and Snowflake. We show that solving problems in Spider 2.0
frequently requires understanding and searching through database metadata,
dialect documentation, and even project-level codebases. This challenge calls
for models to interact with complex SQL workflow environments, process
extremely long contexts, perform intricate reasoning, and generate multiple SQL
queries with diverse operations, often exceeding 100 lines, which goes far
beyond traditional text-to-SQL challenges. Our evaluations indicate that based
on o1-preview, our code agent framework successfully solves only 17.0% of the
tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on
Spider 2.0 show that while language models have demonstrated remarkable
performance in code generation -- especially in prior text-to-SQL benchmarks --
they require significant improvement in order to achieve adequate performance
for real-world enterprise usage. Progress on Spider 2.0 represents crucial
steps towards developing intelligent, autonomous, code agents for real-world
enterprise settings. Our code, baseline models, and data are available at
https://spider2-sql.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Bias in Queer Representation within <span class="highlight-title">Large Language Model</span>s: A
  Collaborative <span class="highlight-title">Agent</span> Approach <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Arya Somasundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often perpetuate biases in pronoun usage,
leading to misrepresentation or exclusion of queer individuals. This paper
addresses the specific problem of biased pronoun usage in LLM outputs,
particularly the inappropriate use of traditionally gendered pronouns ("he,"
"she") when inclusive language is needed to accurately represent all
identities. We introduce a collaborative agent pipeline designed to mitigate
these biases by analyzing and optimizing pronoun usage for inclusivity. Our
multi-agent framework includes specialized agents for both bias detection and
correction. Experimental evaluations using the Tango dataset-a benchmark
focused on gender pronoun usage-demonstrate that our approach significantly
improves inclusive pronoun classification, achieving a 32.6 percentage point
increase over GPT-4o in correctly disagreeing with inappropriate traditionally
gendered pronouns $(\chi^2 = 38.57, p < 0.0001)$. These results accentuate the
potential of agent-driven frameworks in enhancing fairness and inclusivity in
AI-generated content, demonstrating their efficacy in reducing biases and
promoting socially responsible AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotating Constructions with UD: the experience of the Italian
  Constructicon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Pannitto, Beatrice Bernasconi, Lucia Busso, Flavio Pisciotta, Giulia Rambelli, Francesca Masini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper descirbes a first attempt of linking the Italian constructicon to
UD resources
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Preference Optimization Using Sparse Feature-Level Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Clinical Reasoning through Knowledge-augmented Rationale
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Niu, Jing Ma, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical rationales play a pivotal role in accurate disease diagnosis;
however, many models predominantly use discriminative methods and overlook the
importance of generating supportive rationales. Rationale distillation is a
process that transfers knowledge from large language models (LLMs) to smaller
language models (SLMs), thereby enhancing the latter's ability to break down
complex tasks. Despite its benefits, rationale distillation alone is inadequate
for addressing domain knowledge limitations in tasks requiring specialized
expertise, such as disease diagnosis. Effectively embedding domain knowledge in
SLMs poses a significant challenge. While current LLMs are primarily geared
toward processing textual data, multimodal LLMs that incorporate time series
data, especially electronic health records (EHRs), are still evolving. To
tackle these limitations, we introduce ClinRaGen, an SLM optimized for
multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a
unique knowledge-augmented attention mechanism to merge domain knowledge with
time series EHR data, utilizing a stepwise rationale distillation strategy to
produce both textual and time series-based clinical rationales. Our evaluations
show that ClinRaGen markedly improves the SLM's capability to interpret
multimodal EHR data and generate accurate clinical rationales, supporting more
reliable disease diagnosis, advancing LLM applications in healthcare, and
narrowing the performance divide between LLMs and SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring
  Conversations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rose E. Wang, Pawan Wirawarn, Kenny Lam, Omar Khattab, Dorottya Demszky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many open-ended conversations (e.g., tutoring lessons or business meetings)
revolve around pre-defined reference materials, like worksheets or meeting
bullets. To provide a framework for studying such conversation structure, we
introduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly
breaking down conversations into segments and linking each segment to the
relevant reference item. As a case study, we apply POSR to education where
effectively structuring lessons around problems is critical yet difficult. We
present LessonLink, the first dataset of real-world tutoring lessons, featuring
3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT
math problems. We define and evaluate several joint and independent approaches
for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT),
and large language models (LLMs) methods. Our results highlight that modeling
POSR as one joint task is essential: POSR methods outperform independent
segmentation and retrieval pipelines by up to +76% on joint metrics and surpass
traditional segmentation methods by up to +78% on segmentation metrics. We
demonstrate POSR's practical impact on downstream education applications,
deriving new insights on the language and time use in real-world lesson
structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. Our code and dataset are open-sourced at
  https://github.com/rosewang2008/posr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Controllable Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Contrastive</span> Language <span class="highlight-title">Prompt</span>ing to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Neurolinguistic Subjects: Identifying Internal
  Representations for Form and Meaning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyang He, Ercong Nie, Helmut Schmid, Hinrich Schütze, Nima Mesgarani, Jonathan Brennan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the linguistic understanding of Large Language Models
(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two
LLM evaluation paradigms: psycholinguistic and neurolinguistic. Traditional
psycholinguistic evaluations often reflect statistical biases that may
misrepresent LLMs' true linguistic capabilities. We introduce a neurolinguistic
approach, utilizing a novel method that combines minimal pair and diagnostic
probing to analyze activation patterns across model layers. This method allows
for a detailed examination of how LLMs represent form and meaning, and whether
these representations are consistent across languages. Our contributions are
three-fold: (1) We compare neurolinguistic and psycholinguistic methods,
revealing distinct patterns in LLM assessment; (2) We demonstrate that LLMs
exhibit higher competence in form compared to meaning, with the latter largely
correlated to the former; (3) We present new conceptual minimal pair datasets
for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecEncoder: Logs are All You Need in Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-enhanced Network for Hateful Meme Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxi Liu, Yanyan Feng, Jiehai Chen, Yun Xue, Fenghuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic expansion of social media has led to an inundation of hateful
memes on media platforms, accentuating the growing need for efficient
identification and removal. Acknowledging the constraints of conventional
multimodal hateful meme classification, which heavily depends on external
knowledge and poses the risk of including irrelevant or redundant content, we
developed Pen -- a prompt-enhanced network framework based on the prompt
learning approach. Specifically, after constructing the sequence through the
prompt method and encoding it with a language model, we performed region
information global extraction on the encoded sequence for multi-view
perception. By capturing global information about inference instances and
demonstrations, Pen facilitates category selection by fully leveraging sequence
information. This approach significantly improves model classification
accuracy. Additionally, to bolster the model's reasoning capabilities in the
feature space, we introduced prompt-aware contrastive learning into the
framework to improve the quality of sample feature distributions. Through
extensive ablation experiments on two public datasets, we evaluate the
effectiveness of the Pen framework, concurrently comparing it with
state-of-the-art model baselines. Our research findings highlight that Pen
surpasses manual prompt methods, showcasing superior generalization and
classification accuracy in hateful meme classification tasks. Our code is
available at https://github.com/juszzi/Pen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the Thirty-Third International Joint
  Conference on Artificial Intelligence Main Track. Pages 6397-6405</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair <span class="highlight-title">Summarization</span>: Bridging Quality and Diversity in Extractive
  Summaries <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in multi-document summarization of user-generated content remains a
critical challenge in natural language processing (NLP). Existing summarization
methods often fail to ensure equitable representation across different social
groups, leading to biased outputs. In this paper, we introduce two novel
methods for fair extractive summarization: FairExtract, a clustering-based
approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.
We evaluate these methods using Divsumm summarization dataset of White-aligned,
Hispanic, and African-American dialect tweets and compare them against relevant
baselines. The results obtained using a comprehensive set of summarization
quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well
as a fairness metric F, demonstrate that FairExtract and FairGPT achieve
superior fairness while maintaining competitive summarization quality.
Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that
integrate quality and fairness into a single evaluation framework, offering a
more nuanced understanding of the trade-offs between these objectives. This
work highlights the importance of fairness in summarization and sets a
benchmark for future research in fairness-aware NLP models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Algorithmic Fairness through the Lens of Metrics and
  Evaluation Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparrowVQE: Visual Question Explanation for Course Content Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Li, Manish Kumar Thota, Ruslan Gokhman, Radek Holik, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) research seeks to create AI systems to answer
natural language questions in images, yet VQA methods often yield overly
simplistic and short answers. This paper aims to advance the field by
introducing Visual Question Explanation (VQE), which enhances the ability of
VQA to provide detailed explanations rather than brief responses and address
the need for more complex interaction with visual content. We first created an
MLVQE dataset from a 14-week streamed video machine learning course, including
885 slide images, 110,407 words of transcripts, and 9,416 designed
question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3
billion parameters multimodal model. We trained our model with a three-stage
training mechanism consisting of multimodal pre-training (slide images and
transcripts feature alignment), instruction tuning (tuning the pre-trained
model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide
image and QA pairs). Eventually, our SparrowVQE can understand and connect
visual information using the SigLIP model with transcripts using the Phi-2
language model with an MLP adapter. Experimental results demonstrate that our
SparrowVQE achieves better performance in our developed MLVQE dataset and
outperforms state-of-the-art methods in the other five benchmark VQA datasets.
The source code is available at
\url{https://github.com/YoushanZhang/SparrowVQE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Response: Mitigating <span class="highlight-title">LLM</span> Jailbreaks with a Few Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) grow more powerful, ensuring their safety
against misuse becomes crucial. While researchers have focused on developing
robust defenses, no method has yet achieved complete invulnerability to
attacks. We propose an alternative approach: instead of seeking perfect
adversarial robustness, we develop rapid response techniques to look to block
whole classes of jailbreaks after observing only a handful of attacks. To study
this setting, we develop RapidResponseBench, a benchmark that measures a
defense's robustness against various jailbreak strategies after adapting to a
few observed examples. We evaluate five rapid response methods, all of which
use jailbreak proliferation, where we automatically generate additional
jailbreaks similar to the examples observed. Our strongest method, which
fine-tunes an input classifier to block proliferated jailbreaks, reduces attack
success rate by a factor greater than 240 on an in-distribution set of
jailbreaks and a factor greater than 15 on an out-of-distribution set, having
observed just one example of each jailbreaking strategy. Moreover, further
studies suggest that the quality of proliferation model and number of
proliferated examples play an key role in the effectiveness of this defense.
Overall, our results highlight the potential of responding rapidly to novel
jailbreaks to limit LLM misuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Evaluation of Syntactic Knowledge in Multilingual Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Kryvosheieva, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are capable of acquiring elements of human-like
syntactic knowledge. Targeted syntactic evaluation tests have been employed to
measure how well they form generalizations about syntactic phenomena in
high-resource languages such as English. However, we still lack a thorough
understanding of LMs' capacity for syntactic generalizations in low-resource
languages, which are responsible for much of the diversity of syntactic
patterns worldwide. In this study, we develop targeted syntactic evaluation
tests for three low-resource languages (Basque, Hindi, and Swahili) and use
them to evaluate five families of open-access multilingual Transformer LMs. We
find that some syntactic tasks prove relatively easy for LMs while others
(agreement in sentences containing indirect objects in Basque, agreement across
a prepositional phrase in Swahili) are challenging. We additionally uncover
issues with publicly available Transformers, including a bias toward the
habitual aspect in Hindi in multilingual BERT and underperformance compared to
similar-sized models in XGLM-4.5B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kawshik Manikantan, Makarand Tapaswi, Vineet Gandhi, Shubham Toshniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent evaluations of LLMs on coreference resolution have revealed that
traditional output formats and evaluation metrics do not fully capture the
models' referential understanding. To address this, we introduce IdentifyMe, a
new benchmark for mention resolution presented in a multiple-choice question
(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long
narratives and employs heuristics to exclude easily identifiable mentions,
creating a more challenging task. The benchmark also consists of a curated
mixture of different mention types and corresponding entities, allowing for a
fine-grained analysis of model performance. We evaluate both closed- and open
source LLMs on IdentifyMe and observe a significant performance gap (20-30%)
between the state-of-the-art sub-10B open models vs. closed ones. We observe
that pronominal mentions, which have limited surface information, are typically
much harder for models to resolve than nominal mentions. Additionally, we find
that LLMs often confuse entities when their mentions overlap in nested
structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,
highlighting the strong referential capabilities of state-of-the-art LLMs while
also indicating room for further improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BudgetML<span class="highlight-title">Agent</span>: A Cost-Effective <span class="highlight-title">LLM</span> Multi-<span class="highlight-title">Agent</span> system for Automating
  Machine Learning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Gandhi, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in diverse applications including
generation of code snippets, but often struggle with generating code for
complex Machine Learning (ML) tasks. Although existing LLM single-agent based
systems give varying performance depending on the task complexity, they purely
rely on larger and expensive models such as GPT-4. Our investigation reveals
that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama
perform far worse than GPT-4 in a single-agent setting. With the motivation of
developing a cost-efficient LLM based solution for solving ML tasks, we propose
an LLM Multi-Agent based system which leverages combination of experts using
profiling, efficient retrieval of past observations, LLM cascades, and
ask-the-expert calls. Through empirical analysis on ML engineering tasks in the
MLAgentBench benchmark, we demonstrate the effectiveness of our system, using
no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and
expert to serve occasional ask-the-expert calls for planning. With 94.2\%
reduction in the cost (from \$0.931 per run cost averaged over all tasks for
GPT-4 single agent system to \$0.054), our system is able to yield better
average success rate of 32.95\% as compared to GPT-4 single-agent system
yielding 22.72\% success rate averaged over all the tasks of MLAgentBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AIMLSystems '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deco<span class="highlight-title">Prompt</span> : Decoding <span class="highlight-title">Prompt</span>s Reduces Hallucinations when Large Language
  Models Meet False Premises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated increasing power, they
have also called upon studies on their hallucinated outputs that deviate from
factually correct statements. In this paper, we focus on one important scenario
of false premises, where LLMs are distracted by misaligned claims although the
model possesses the required factual knowledge to answer original questions
accurately. Inspired by the observation that entropy of the false-premise
prompt is closely related to its likelihood to elicit hallucination generation,
we propose a new prompting algorithm, named DecoPrompt, to mitigate
hallucination. DecoPrompt leverages LLMs to "decode" the false-premise prompts
without really eliciting hallucination output from LLMs. We perform experiments
on two datasets, demonstrating that DecoPrompt can reduce hallucinations
effectively on outputs from different LLMs. Moreover, DecoPrompt exhibits
cross-model transferability, which facilitates its applications to scenarios
such as LLMs of large sizes or unavailable model logits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Accurate <span class="highlight-title">Prompt</span> Optimization: the Benefit of Memory in
  Exemplar-Guided Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, Yangyang Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic prompt engineering aims to enhance the generation quality of large
language models (LLMs). Recent works utilize feedbacks generated from erroneous
cases to guide the prompt optimization. During inference, they may further
retrieve several semantically-related exemplars and concatenate them to the
optimized prompts to improve the performance. However, those works only utilize
the feedback at the current step, ignoring historical and unseleccted feedbacks
which are potentially beneficial. Moreover, the selection of exemplars only
considers the general semantic relationship and may not be optimal in terms of
task performance and matching with the optimized prompt. In this work, we
propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize
more efficient and accurate prompt optimization. Specifically, we design an
exemplar-guided reflection mechanism where the feedback generation is
additionally guided by the generated exemplars. We further build two kinds of
memory to fully utilize the historical feedback information and support more
effective exemplar retrieval. Empirical evaluations show our method surpasses
previous state-of-the-arts with less optimization steps, i.e., improving F1
score by 10.1 on LIAR dataset, and reducing half of the optimization steps on
ProTeGi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyao Li, Mingze Ni, Yongshun Gong, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning underpins most of the currently advanced natural language
processing (NLP) tasks such as textual classification, neural machine
translation (NMT), abstractive summarization and question-answering (QA).
However, the robustness of the models, particularly QA models, against
adversarial attacks is a critical concern that remains insufficiently explored.
This paper introduces QA-Attack (Question Answering Attack), a novel word-level
adversarial strategy that fools QA models. Our attention-based attack exploits
the customized attention mechanism and deletion ranking strategy to identify
and target specific words within contextual passages. It creates deceptive
inputs by carefully choosing and substituting synonyms, preserving grammatical
integrity while misleading the model to produce incorrect responses. Our
approach demonstrates versatility across various question types, particularly
when dealing with extensive long textual inputs. Extensive experiments on
multiple benchmark datasets demonstrate that QA-Attack successfully deceives
baseline QA models and surpasses existing adversarial techniques regarding
success rate, semantics changes, BLEU score, fluency and grammar error rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Safety Bundle: Auditing the Helpful and Harmless <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaoula Chehbouni, Jonathan Colaço-Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an effort to mitigate the harms of large language models (LLMs), learning
from human feedback (LHF) has been used to steer LLMs towards outputs that are
intended to be both less harmful and more helpful. Despite the widespread
adoption of LHF in practice, the quality of this feedback and its effectiveness
as a safety mitigation technique remain unclear. This study addresses these
issues by auditing the widely-used Helpful and Harmless (HH) dataset by
Anthropic. Our work includes: (1) a thorough investigation of the dataset's
content through both manual and automated evaluation; (2) experiments
demonstrating the dataset's impact on models' safety; and (3) an analysis of
the 100 most influential papers citing this dataset. Through our audit, we
showcase how conceptualization failures and quality issues identified in the HH
dataset can create additional harms by leading to disparate safety behaviors
across demographic groups. Our findings highlight the need for more nuanced,
context-sensitive approaches to safety mitigation in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Prepared for conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for
  Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzhi Li, Cehao Yang, Chengjin Xu, Xuhui Jiang, Yiyan Qi, Jian Guo, Ho-fung Leung, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Knowledge Graph Completion~(KGC) task aims to infer the missing entity
from an incomplete triple. Existing embedding-based methods rely solely on
triples in the KG, which is vulnerable to specious relation patterns and
long-tail entities. On the other hand, text-based methods struggle with the
semantic gap between KG triples and natural language. Apart from triples,
entity contexts (e.g., labels, descriptions, aliases) also play a significant
role in augmenting KGs. To address these limitations, we propose KGR3, a
context-enriched framework for KGC. KGR3 is composed of three modules. Firstly,
the Retrieval module gathers supporting triples from the KG, collects plausible
candidate answers from a base embedding model, and retrieves context for each
related entity. Then, the Reasoning module employs a large language model to
generate potential answers for each query triple. Finally, the Re-ranking
module combines candidate answers from the two modules mentioned above, and
fine-tunes an LLM to provide the best answer. Extensive experiments on widely
used datasets demonstrate that KGR3 consistently improves various KGC methods.
Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of
12.3% and 5.6% on the FB15k237 and WN18RR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s Can Self-Improve in Long-context Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved substantial progress in processing
long contexts but still struggle with long-context reasoning. Existing
approaches typically involve fine-tuning LLMs with synthetic data, which
depends on annotations from human experts or advanced models like GPT-4, thus
restricting further advancements. To address this issue, we investigate the
potential for LLMs to self-improve in long-context reasoning and propose \ours,
an approach specifically designed for this purpose. This approach is
straightforward: we sample multiple outputs for each question, score them with
Minimum Bayes Risk, and then apply supervised fine-tuning or preference
optimization based on these outputs. Extensive experiments on several leading
LLMs demonstrate the effectiveness of \ours, with an absolute improvement of
$4.2$ points for Llama-3.1-8B-Instruct. Furthermore, \ours achieves superior
performance compared to prior approaches that depend on data produced by human
experts or advanced models. We anticipate that this work will open new avenues
for self-improvement techniques in long-context scenarios, which are essential
for the continual advancement of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/SihengLi99/SEALONG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Speech Data in Reducing Toxicity Detection Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo Sánchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text toxicity detection systems exhibit significant biases, producing
disproportionate rates of false positives on samples mentioning demographic
groups. But what about toxicity detection in speech? To investigate the extent
to which text-based biases are mitigated by speech-based systems, we produce a
set of high-quality group annotations for the multilingual MuTox dataset, and
then leverage these annotations to systematically compare speech- and
text-based toxicity classifiers. Our findings indicate that access to speech
data during inference supports reduced bias against group mentions,
particularly for ambiguous and disagreement-inducing samples. Our results also
suggest that improving classifiers, rather than transcription pipelines, is
more helpful for reducing group bias. We publicly release our annotations and
provide recommendations for future toxicity dataset construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08479v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08479v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Ranjan Satapathy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of complex and opaque black box models requires the
adoption of interpretable measures, one such option is extractive rationalizing
models, which serve as a more interpretable alternative. These models, also
known as Explain-Then-Predict models, employ an explainer model to extract
rationales and subsequently condition the predictor with the extracted
information. Their primary objective is to provide precise and faithful
explanations, represented by the extracted rationales. In this paper, we take a
semi-supervised approach to optimize for the plausibility of extracted
rationales. We adopt a pre-trained natural language inference (NLI) model and
further fine-tune it on a small set of supervised rationales ($10\%$). The NLI
predictor is leveraged as a source of supervisory signals to the explainer via
entailment alignment. We show that, by enforcing the alignment agreement
between the explanation and answer in a question-answering task, the
performance can be improved without access to ground truth labels. We evaluate
our approach on the ERASER dataset and show that our approach achieves
comparable results with supervised extractive models and outperforms
unsupervised approaches by $> 100\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-training <span class="highlight-title">Large Language Model</span>s through Knowledge Detection <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often necessitate extensive labeled datasets and
training compute to achieve impressive performance across downstream tasks.
This paper explores a self-training paradigm, where the LLM autonomously
curates its own labels and selectively trains on unknown data samples
identified through a reference-free consistency method. Empirical evaluations
demonstrate significant improvements in reducing hallucination in generation
across multiple subjects. Furthermore, the selective training framework
mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing
a critical limitation in training LLMs. Our findings suggest that such an
approach can substantially reduce the dependency on large labeled datasets,
paving the way for more scalable and cost-effective language model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Language Model</span> Factuality via Activation-Based Confidence
  Calibration and Guided Decoding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Liu, Farima Fatahi Bayat, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating language models (LMs) aligns their generation confidence with the
actual likelihood of answer correctness, which can inform users about LMs'
reliability and mitigate hallucinated content. However, prior calibration
methods, such as self-consistency-based and logit-based approaches, are either
limited in inference-time efficiency or fall short of providing informative
signals. Moreover, simply filtering out low-confidence responses reduces the
LM's helpfulness when the answers are correct. Therefore, effectively using
calibration techniques to enhance an LM's factuality remains an unsolved
challenge. In this paper, we first propose an activation-based calibration
method, ActCab, which trains a linear layer on top of the LM's last-layer
activations that can better capture the representations of knowledge. Built on
top of ActCab, we further propose CoDec, a confidence-guided decoding strategy
to elicit truthful answers with high confidence from LMs. By evaluating on five
popular QA benchmarks, ActCab achieves superior calibration performance than
all competitive baselines, e.g., by reducing the average expected calibration
error (ECE) score by up to 39%. Further experiments on CoDec show consistent
improvements in several LMs' factuality on challenging QA datasets, such as
TruthfulQA, highlighting the value of confidence signals in enhancing
factuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting User Comments for Early Detection of Fake News Prior to
  Users' Commenting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Nan, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Guang Yang, Jintao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both accuracy and timeliness are key factors in detecting fake news on social
media. However, most existing methods encounter an accuracy-timeliness dilemma:
Content-only methods guarantee timeliness but perform moderately because of
limited available information, while social con-text-based ones generally
perform better but inevitably lead to latency because of social context
accumulation needs. To break such a dilemma, a feasible but not well-studied
solution is to leverage social contexts (e.g., comments) from historical news
for training a detection model and apply it to newly emerging news without
social contexts. This requires the model to (1) sufficiently learn helpful
knowledge from social contexts, and (2) be well compatible with situations that
social contexts are available or not. To achieve this goal, we propose to
absorb and parameterize useful knowledge from comments in historical news and
then inject it into a content-only detection model. Specifically, we design the
Comments ASsisted FakE News Detection method (CAS-FEND), which transfers useful
knowledge from a comment-aware teacher model to a content-only student model
and detects newly emerging news with the student model. Experiments show that
the CAS-FEND student model outperforms all content-only methods and even
comment-aware ones with 1/4 comments as inputs, demonstrating its superiority
for early detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 7 tables. The article has been accepted by
  Frontiers of Computer Science (FCS), with the DOI:
  {10.1007/s11704-024-40674-6}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do <span class="highlight-title">Large Language Model</span>s Acquire Factual Knowledge During
  <span class="highlight-title">Pretrain</span>ing? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent observation that large language models (LLMs) can store
substantial factual knowledge, there is a limited understanding of the
mechanisms of how they acquire factual knowledge through pretraining. This work
addresses this gap by studying how LLMs acquire factual knowledge during
pretraining. The findings reveal several important insights into the dynamics
of factual knowledge acquisition during pretraining. First, counterintuitively,
we observe that pretraining on more data shows no significant improvement in
the model's capability to acquire and maintain factual knowledge. Next, there
is a power-law relationship between training steps and forgetting of
memorization and generalization of factual knowledge, and LLMs trained with
duplicated training data exhibit faster forgetting. Third, training LLMs with
larger batch sizes can enhance the models' robustness to forgetting. Overall,
our observations suggest that factual knowledge acquisition in LLM pretraining
occurs by progressively increasing the probability of factual knowledge
presented in the pretraining data at each step. However, this increase is
diluted by subsequent forgetting. Based on this interpretation, we demonstrate
that we can provide plausible explanations for recently observed behaviors of
LLMs, such as the poor performance of LLMs on long-tail knowledge and the
benefits of deduplicating the pretraining corpus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Multi-Task Learning Architecture for Hate Detection Leveraging
  User-Based Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Kapil, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech, offensive language, aggression, racism, sexism, and other
abusive language are common phenomena in social media. There is a need for
Artificial Intelligence(AI)based intervention which can filter hate content at
scale. Most existing hate speech detection solutions have utilized the features
by treating each post as an isolated input instance for the classification.
This paper addresses this issue by introducing a unique model that improves
hate speech identification for the English language by utilising intra-user and
inter-user-based information. The experiment is conducted over single-task
learning (STL) and multi-task learning (MTL) paradigms that use deep neural
networks, such as convolutional neural networks (CNN), gated recurrent unit
(GRU), bidirectional encoder representations from the transformer (BERT), and A
Lite BERT (ALBERT). We use three benchmark datasets and conclude that combining
certain user features with textual features gives significant improvements in
macro-F1 and weighted-F1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, and two tables. Accepted at the 20th International
  Conference on Natural Language Processing (ICON) 2023.
  https://aclanthology.org/2023.icon-1.53</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Can Evolve <span class="highlight-title">Continual</span>ly on Modality for X-Modal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazuo Yu, Haomiao Xiong, Lu Zhang, Haiwen Diao, Yunzhi Zhuge, Lanqing Hong, Dong Wang, Huchuan Lu, You He, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have gained significant attention
due to their impressive capabilities in multimodal understanding. However,
existing methods rely heavily on extensive modal-specific pretraining and
joint-modal tuning, leading to significant computational burdens when expanding
to new modalities. In this paper, we propose PathWeave, a flexible and scalable
framework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs
to continually EVolve on modalities for $\mathbb{X}$-modal reasoning. We
leverage the concept of Continual Learning and develop an incremental training
strategy atop pre-trained MLLMs, enabling their expansion to new modalities
using uni-modal data, without executing joint-modal pretraining. In detail, a
novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and
cross-modal adapters are seamlessly integrated to facilitate efficient modality
alignment and collaboration. Additionally, an MoE-based gating module is
applied between two types of adapters to further enhance the multimodal
interaction. To investigate the proposed method, we establish a challenging
benchmark called Continual Learning of Modality (MCL), which consists of
high-quality QA data from five distinct modalities: image, video, audio, depth
and point cloud. Extensive experiments demonstrate the effectiveness of the
proposed AnA framework on learning plasticity and memory stability during
continual learning. Furthermore, PathWeave performs comparably to
state-of-the-art MLLMs while concurrently reducing parameter training burdens
by 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Patterns of Personalized Persuasion in <span class="highlight-title">Large Language Model</span>s:
  Exposing Persuasive Linguistic Features for Big Five Personality Traits in
  <span class="highlight-title">LLM</span>s Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiktoria Mieleszczenko-Kowszewicz, Dawid Płudowski, Filip Kołodziejczyk, Jakub Świstak, Julian Sienkiewicz, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores how the Large Language Models (LLMs) adjust linguistic
features to create personalized persuasive outputs. While research showed that
LLMs personalize outputs, a gap remains in understanding the linguistic
features of their persuasive capabilities. We identified 13 linguistic features
crucial for influencing personalities across different levels of the Big Five
model of personality. We analyzed how prompts with personality trait
information influenced the output of 19 LLMs across five model families. The
findings show that models use more anxiety-related words for neuroticism,
increase achievement-related words for conscientiousness, and employ fewer
cognitive processes words for openness to experience. Some model families excel
at adapting language for openness to experience, others for conscientiousness,
while only one model adapts language for neuroticism. Our findings show how
LLMs tailor responses based on personality cues in prompts, indicating their
potential to create persuasive content affecting the mind and well-being of the
recipients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">LLM</span> Comparative Assessment: a Product of Experts Framework for
  Pairwise Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-judge approaches are a practical and effective way of assessing a
range of text tasks. However, when using pairwise comparisons to rank a set of
candidates, the computational cost scales quadratically with the number of
candidates, which has practical limitations. This paper introduces a Product of
Expert (PoE) framework for efficient LLM Comparative Assessment. Here
individual comparisons are considered experts that provide information on a
pair's score difference. The PoE framework combines the information from these
experts to yield an expression that can be maximized with respect to the
underlying set of candidates, and is highly flexible where any form of expert
can be assumed. When Gaussian experts are used one can derive simple
closed-form solutions for the optimal candidate ranking, and expressions for
selecting which comparisons should be made to maximize the probability of this
ranking. Our approach enables efficient comparative assessment, where by using
only a small subset of the possible comparisons, one can generate score
predictions that correlate well with human judgements. We evaluate the approach
on multiple NLG tasks and demonstrate that our framework can yield considerable
computational savings when performing pairwise comparative assessment. With
many candidate texts, using as few as 2% of comparisons the PoE solution can
achieve similar performance to when all comparisons are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2.5-Coder Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we introduce the Qwen2.5-Coder series, a significant upgrade
from its predecessor, CodeQwen1.5. This series includes six models:
Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,
Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained
on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,
scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder
demonstrates impressive code generation capabilities while retaining general
and math skills. These models have been evaluated on a wide range of
code-related tasks, achieving state-of-the-art (SOTA) performance across more
than 10 benchmarks, including code generation, completion, reasoning, and
repair, consistently outperforming larger models of the same model size. We
believe that the release of the Qwen2.5-Coder series will advance research in
code intelligence and, with its permissive licensing, support wider adoption by
developers in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kwai-STaR: Transform <span class="highlight-title">LLM</span>s into State-Transition Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lu, Yuhang Hu, Changyi Liu, Tianke Zhang, Zhenyu Yang, Zhixiang Ding, Shengsheng Qian, Meng Du, Ruiwen Kang, Kaiyu Tang, Fan Yang, Tingting Gao, Di Zhang, Hai-Tao Zheng, Bin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s for Generating and Evaluating Counterfactuals: A Comprehensive
  Study <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Bach Nguyen, Paul Youssef, Christin Seifert, Jörg Schlötterer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As NLP models become more complex, understanding their decisions becomes more
crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's
prediction, offer a way to explain these models. While Large Language Models
(LLMs) have shown remarkable performance in NLP tasks, their efficacy in
generating high-quality CFs remains uncertain. This work fills this gap by
investigating how well LLMs generate CFs for two NLU tasks. We conduct a
comprehensive comparison of several common LLMs, and evaluate their CFs,
assessing both intrinsic metrics, and the impact of these CFs on data
augmentation. Moreover, we analyze differences between human and LLM-generated
CFs, providing insights for future research directions. Our results show that
LLMs generate fluent CFs, but struggle to keep the induced changes minimal.
Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where
LLMs show weaknesses in generating CFs that flip the original label. This also
reflects on the data augmentation performance, where we observe a large gap
between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs'
ability to assess CFs in a mislabelled data setting, and show that they have a
strong bias towards agreeing with the provided labels. GPT4 is more robust
against this bias and its scores correlate well with automatic metrics. Our
findings reveal several limitations and point to potential future work
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLHF Workflow: From Reward Modeling to Online RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (09/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeKUBE: A Legal Knowledge Update BEnchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyue Wang, Weihang Su, Hu Yiran, Qingyao Ai, Yueyue Wu, Cheng Luo, Yiqun Liu, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have significantly shaped the
applications of AI in multiple fields, including the studies of legal
intelligence. Trained on extensive legal texts, including statutes and legal
documents, the legal LLMs can capture important legal knowledge/concepts
effectively and provide important support for downstream legal applications
such as legal consultancy. Yet, the dynamic nature of legal statutes and
interpretations also poses new challenges to the use of LLMs in legal
applications. Particularly, how to update the legal knowledge of LLMs
effectively and efficiently has become an important research problem in
practice. Existing benchmarks for evaluating knowledge update methods are
mostly designed for the open domain and cannot address the specific challenges
of the legal domain, such as the nuanced application of new legal knowledge,
the complexity and lengthiness of legal regulations, and the intricate nature
of legal reasoning. To address this gap, we introduce the Legal Knowledge
Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for
legal LLMs across five dimensions. Specifically, we categorize the needs of
knowledge updates in the legal domain with the help of legal professionals, and
then hire annotators from law schools to create synthetic updates to the
Chinese Criminal and Civil Code as well as sets of questions of which the
answers would change after the updates. Through a comprehensive evaluation of
state-of-the-art knowledge update methods, we reveal a notable gap between
existing knowledge update methods and the unique needs of the legal domain,
emphasizing the need for further research and development of knowledge update
mechanisms tailored for legal LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Advanced <span class="highlight-title">Large Language Model</span>s with <span class="highlight-title">LLM</span>suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Roffo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison,
  LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset
  Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing
  Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite,
  Comprehensive LLM Evaluation Toolkit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Om<span class="highlight-title">Agent</span>: A Multi-modal <span class="highlight-title">Agent</span> Framework for Complex Video Understanding
  with Task Divide-and-Conquer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have expanded their
capabilities to multimodal contexts, including comprehensive video
understanding. However, processing extensive videos such as 24-hour CCTV
footage or full-length films presents significant challenges due to the vast
data and processing demands. Traditional methods, like extracting key frames or
converting frames to text, often result in substantial information loss. To
address these shortcomings, we develop OmAgent, efficiently stores and
retrieves relevant video frames for specific queries, preserving the detailed
content of videos. Additionally, it features an Divide-and-Conquer Loop capable
of autonomous reasoning, dynamically invoking APIs and tools to enhance query
processing and accuracy. This approach ensures robust video understanding,
significantly reducing information loss. Experimental results affirm OmAgent's
efficacy in handling various types of videos and complex tasks. Moreover, we
have endowed it with greater autonomy and a robust tool-calling system,
enabling it to accomplish even more intricate tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciDFM: A <span class="highlight-title">Large Language Model</span> with Mixture-of-Experts for Science <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant upsurge of interest in leveraging
large language models (LLMs) to assist scientific discovery. However, most LLMs
only focus on general science, while they lack domain-specific knowledge, such
as chemical molecules and amino acid sequences. To bridge these gaps, we
introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and
is able to conduct college-level scientific reasoning and understand molecules
and amino acid sequences. We collect a large-scale training corpus containing
numerous scientific papers and books from different disciplines as well as data
from domain-specific databases. We further fine-tune the pre-trained model on
lots of instruction data to improve performances on downstream benchmarks. From
experiment results, we show that SciDFM achieves strong performance on general
scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA
performance on domain-specific benchmarks among models of similar size. We
further analyze the expert layers and show that the results of expert selection
vary with data from different disciplines. To benefit the broader research
community, we open-source SciDFM at
https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS
  2024 Workshop FM4Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does the Textual Information Affect the Retrieval of Multimodal
  In-Context Learning? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increase in parameter size of multimodal large language models (MLLMs)
introduces significant capabilities, particularly in-context learning, where
MLLMs enhance task performance without updating pre-trained parameters. This
effectiveness, however, hinges on the appropriate selection of in-context
examples, a process that is currently biased towards visual data, overlooking
textual information. Furthermore, the area of supervised retrievers for MLLMs,
crucial for optimal in-context example selection, continues to be
uninvestigated. Our study offers an in-depth evaluation of the impact of
textual information on the unsupervised selection of in-context examples in
multimodal contexts, uncovering a notable sensitivity of retriever performance
to the employed modalities. Responding to this, we introduce a novel supervised
MLLM-retriever MSIER that employs a neural network to select examples that
enhance multimodal in-context learning efficiency. This approach is validated
through extensive testing across three distinct tasks, demonstrating the
method's effectiveness. Additionally, we investigate the influence of
modalities on our supervised retrieval method's training and pinpoint factors
contributing to our model's success. This exploration paves the way for future
advancements, highlighting the potential for refined in-context learning in
MLLMs through the strategic use of multimodal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKVQ: Sliding-window Key and Value Cache Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can now handle longer sequences of tokens,
enabling complex tasks like book understanding and generating lengthy novels.
However, the key-value (KV) cache required for LLMs consumes substantial memory
as context length increasing, becoming the bottleneck for deployment. In this
paper, we present a strategy called SKVQ, which stands for sliding-window KV
cache quantization, to address the issue of extremely low bitwidth KV cache
quantization. To achieve this, SKVQ rearranges the channels of the KV cache in
order to improve the similarity of channels in quantization groups, and applies
clipped dynamic quantization at the group level. Additionally, SKVQ ensures
that the most recent window tokens in the KV cache are preserved with high
precision. This helps maintain the accuracy of a small but important portion of
the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.
Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization
approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit
values with minimal loss of accuracy. With SKVQ, it is possible to process
context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7
times faster decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced <span class="highlight-title">LLM</span>
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing Zhao, Andi Chen, Panfeng Li, Shiyao Zhang, Qifu Yin, Yuanqing Wang, Yuxi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MASIVE: Open-Ended Affective State Identification in English and Spanish <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Deas, Elsbeth Turcan, Iván Pérez Mejía, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of emotion analysis, much NLP research focuses on identifying a
limited number of discrete emotion categories, often applied across languages.
These basic sets, however, are rarely designed with textual data in mind, and
culture, language, and dialect can influence how particular emotions are
interpreted. In this work, we broaden our scope to a practically unbounded set
of \textit{affective states}, which includes any terms that humans use to
describe their experiences of feeling. We collect and publish MASIVE, a dataset
of Reddit posts in English and Spanish containing over 1,000 unique affective
states each. We then define the new problem of \textit{affective state
identification} for language generation models framed as a masked span
prediction task. On this task, we find that smaller finetuned multilingual
models outperform much larger LLMs, even on region-specific Spanish affective
states. Additionally, we show that pretraining on MASIVE improves model
performance on existing emotion benchmarks. Finally, through machine
translation experiments, we find that native speaker-written data is vital to
good performance on this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Diverse Methods in Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAMP: A <span class="highlight-title">Language Model</span> on the Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale Balsebre, Weiming Huang, Gao Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are poised to play an increasingly important
role in our lives, providing assistance across a wide array of tasks. In the
geospatial domain, LLMs have demonstrated the ability to answer generic
questions, such as identifying a country's capital; nonetheless, their utility
is hindered when it comes to answering fine-grained questions about specific
places, such as grocery stores or restaurants, which constitute essential
aspects of people's everyday lives. This is mainly because the places in our
cities haven't been systematically fed into LLMs, so as to understand and
memorize them. This study introduces a novel framework for fine-tuning a
pre-trained model on city-specific data, to enable it to provide accurate
recommendations, while minimizing hallucinations. We share our model, LAMP, and
the data used to train it. We conduct experiments to analyze its ability to
correctly retrieving spatial objects, and compare it to well-known open- and
closed- source language models, such as GPT-4. Finally, we explore its emerging
capabilities through a case study on day planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game-theoretic <span class="highlight-title">LLM</span>: <span class="highlight-title">Agent</span> Workflow for Negotiation Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Adaptive Optimization for Effective Sentiment Analysis
  Fine-Tuning on <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Ding, Xuanze Zhao, Shamsul Nahar Abdullah, Deshinta Arrova Dewi, Zixiao Jiang, Xiangyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis plays a crucial role in various domains, such as business
intelligence and financial forecasting. Large language models (LLMs) have
become a popular paradigm for sentiment analysis, leveraging multi-task
learning to address specific tasks concurrently. However, LLMs with fine-tuning
for sentiment analysis often underperforms due to the inherent challenges in
managing diverse task complexities. Moreover, constant-weight approaches in
multi-task learning struggle to adapt to variations in data characteristics,
further complicating model effectiveness. To address these issues, we propose a
novel multi-task learning framework with a dynamic adaptive optimization (DAO)
module. This module is designed as a plug-and-play component that can be
seamlessly integrated into existing models, providing an effective and flexible
solution for multi-task learning. The key component of the DAO module is
dynamic adaptive loss, which dynamically adjusts the weights assigned to
different tasks based on their relative importance and data characteristics
during training. Sentiment analyses on a standard and customized financial text
dataset demonstrate that the proposed framework achieves superior performance.
Specifically, this work improves the Mean Squared Error (MSE) and Accuracy
(ACC) by 15.58% and 1.24% respectively, compared with previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reminding Multimodal <span class="highlight-title">Large Language Model</span>s of Object-aware Knowledge
  with Retrieved Tags <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in the general visual instruction-following ability
of Multimodal Large Language Models (MLLMs), they still struggle with critical
problems when required to provide a precise and detailed response to a visual
instruction: (1) failure to identify novel objects or entities, (2) mention of
non-existent objects, and (3) neglect of object's attributed details. Intuitive
solutions include improving the size and quality of data or using larger
foundation models. They show effectiveness in mitigating these issues, but at
an expensive cost of collecting a vast amount of new data and introducing a
significantly larger model. Standing at the intersection of these approaches,
we examine the three object-oriented problems from the perspective of the
image-to-text mapping process by the multimodal connector. In this paper, we
first identify the limitations of multimodal connectors stemming from
insufficient training data. Driven by this, we propose to enhance the mapping
with retrieval-augmented tag tokens, which contain rich object-aware
information such as object names and attributes. With our Tag-grounded visual
instruction tuning with retrieval Augmentation (TUNA), we outperform baselines
that share the same language model and training data on 12 benchmarks.
Furthermore, we show the zero-shot capability of TUNA when provided with
specific datastores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Conference at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLANG: New Concept Comprehension of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12585v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12585v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic nature of language, particularly evident in the realm of slang
and memes on the Internet, poses serious challenges to the adaptability of
large language models (LLMs). Traditionally anchored to static datasets, these
models often struggle to keep up with the rapid linguistic evolution
characteristic of online communities. This research aims to bridge this gap by
enhancing LLMs' comprehension of the evolving new concepts on the Internet,
without the high cost of continual retraining. In pursuit of this goal, we
introduce $\textbf{SLANG}$, a benchmark designed to autonomously integrate
novel data and assess LLMs' ability to comprehend emerging concepts, alongside
$\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to
understand new phrases and their colloquial context. Our benchmark and approach
involves understanding real-world instances of linguistic shifts, serving as
contextual beacons, to form more precise and contextually relevant connections
between newly emerging expressions and their meanings. The empirical analysis
shows that our causal inference-based approach outperforms the baseline methods
in terms of precision and relevance in the comprehension of Internet slang and
memes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MMLongBench-Doc: Benchmarking Long-context Document Understanding with
  Visualizations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, <span class="highlight-author">Aixin Sun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding documents with rich layouts and multi-modal components is a
long-standing and practical task. Recent Large Vision-Language Models (LVLMs)
have made remarkable strides in various tasks, particularly in single-page
document understanding (DU). However, their abilities on long-context DU remain
an open problem. This work presents MMLongBench-Doc, a long-context,
multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct
from previous datasets, it is constructed upon 130 lengthy PDF-formatted
documents with an average of 49.4 pages and 20,971 textual tokens. Towards
comprehensive evaluation, answers to these questions rely on pieces of evidence
from (1) different sources (text, image, chart, table, and layout structure)
and (2) various locations (i.e. page number). Moreover, 33.2% of the questions
are cross-page questions requiring evidence across multiple pages. 22.8% of the
questions are designed to be unanswerable for detecting potential
hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU
greatly challenges current models. Notably, the best-performing model, GPT-4o,
achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores
31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse
performance than their LLM counterparts which are fed with lossy-parsed OCR
documents. These results validate the necessity of future research toward more
capable long-context LVLMs. Project Page:
https://mayubo2333.github.io/MMLongBench-Doc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Data <span class="highlight-title">Distill</span>ation for Recovering Quality in Pruned Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithursan Thangarasa, Ganesh Venkatesh, Mike Lasby, Nish Sinnadurai, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 6 Tables (Main Paper) + 5 pages (Supplementary
  Material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Active Privacy Auditing in Supervised Fine-tuning for White-Box
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Sun, Hanpeng Wu, Xi Sheryl Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretraining and fine-tuning approach has become the leading technique for
various NLP applications. However, recent studies reveal that fine-tuning data,
due to their sensitive nature, domain-specific characteristics, and
identifiability, pose significant privacy concerns. To help develop more
privacy-resilient fine-tuning models, we introduce a novel active privacy
auditing framework, dubbed Parsing, designed to identify and quantify privacy
leakage risks during the supervised fine-tuning (SFT) of language models (LMs).
The framework leverages improved white-box membership inference attacks (MIAs)
as the core technology, utilizing novel learning objectives and a two-stage
pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the
exposure of privacy risks. Additionally, we have improved the effectiveness of
MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our
research aims to provide the SFT community of LMs with a reliable, ready-to-use
privacy auditing tool, and to offer valuable insights into safeguarding privacy
during the fine-tuning process. Experimental results confirm the framework's
efficiency across various models and tasks, emphasizing notable privacy
concerns in the fine-tuning process. Project code available for
https://anonymous.4open.science/r/PARSING-4817/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Models are NOT Stronger Teachers for Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model
  with Frozen <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly developing large language models (LLMs) have brought tremendous
intelligent applications. GPT-4o's excellent duplex speech interaction ability
has recently brought impressive experience to users. Researchers have recently
proposed several multi-modal LLMs in this direction that can achieve
speech-to-speech dialogue. This paper proposes a novel speech-text multimodal
LLM architecture called Freeze-Omni. Our main contribution is that the speech
input and output modalities can be easily connected to a textual LLM while
keeping the LLM's parameters frozen throughout the training process. We
designed 3-stage training strategies both for the modeling of speech input and
output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using
text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round
text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the
intelligence of the Freeze-Omni in the speech modality is at the same level
compared with that in the text modality of its backbone LLM, while the
end-to-end latency of the spoken response achieves a low level. In addition, we
also designed a method to achieve duplex dialogue ability through multi-task
training, making Freeze-Omni have a more natural style of dialogue ability
between the users. Freeze-Omni mainly provides a possibility for researchers to
conduct multimodal LLM under the condition of a frozen LLM, avoiding various
impacts caused by the catastrophic forgetting of LLM caused by fewer data and
training resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://freeze-omni.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation
  Extraction in Long Sentences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Xinyi Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction as an important natural Language processing (NLP) task is
to identify relations between named entities in text. Recently, graph
convolutional networks over dependency trees have been widely used to capture
syntactic features and achieved attractive performance. However, most existing
dependency-based approaches ignore the positive influence of the words outside
the dependency trees, sometimes conveying rich and useful information on
relation extraction. In this paper, we propose a novel model, Entity-aware
Self-attention Contextualized GCN (ESC-GCN), which efficiently incorporates
syntactic structure of input sentences and semantic context of sequences. To be
specific, relative position self-attention obtains the overall semantic
pairwise correlation related to word position, and contextualized graph
convolutional networks capture rich intra-sentence dependencies between words
by adequately pruning operations. Furthermore, entity-aware attention layer
dynamically selects which token is more decisive to make final relation
prediction. In this way, our proposed model not only reduces the noisy impact
from dependency trees, but also obtains easily-ignored entity-related semantic
representation. Extensive experiments on various tasks demonstrate that our
model achieves encouraging performance as compared to existing dependency-based
and sequence-based models. Specially, our model excels in extracting relations
between entities of long sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity-Aware Biaffine Attention Model for Improved Constituent Parsing
  with Reduced Entity Violations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constituency parsing involves analyzing a sentence by breaking it into
sub-phrases, or constituents. While many deep neural models have achieved
state-of-the-art performance in this task, they often overlook the
entity-violating issue, where an entity fails to form a complete sub-tree in
the resultant parsing tree. To address this, we propose an entity-aware
biaffine attention model for constituent parsing. This model incorporates
entity information into the biaffine attention mechanism by using additional
entity role vectors for potential phrases, which enhances the parsing accuracy.
We introduce a new metric, the Entity Violating Rate (EVR), to quantify the
extent of entity violations in parsing results. Experiments on three popular
datasets-ONTONOTES, PTB, and CTB-demonstrate that our model achieves the lowest
EVR while maintaining high precision, recall, and F1-scores comparable to
existing models. Further evaluation in downstream tasks, such as sentence
sentiment analysis, highlights the effectiveness of our model and the validity
of the proposed EVR metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining <span class="highlight-title">Large Language Model</span>s Decisions Using Shapley Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnam Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) has opened up exciting
possibilities for simulating human behavior and cognitive processes, with
potential applications in various domains, including marketing research and
consumer behavior analysis. However, the validity of utilizing LLMs as
stand-ins for human subjects remains uncertain due to glaring divergences that
suggest fundamentally different underlying processes at play and the
sensitivity of LLM responses to prompt variations. This paper presents a novel
approach based on Shapley values from cooperative game theory to interpret LLM
behavior and quantify the relative contribution of each prompt component to the
model's output. Through two applications - a discrete choice experiment and an
investigation of cognitive biases - we demonstrate how the Shapley value method
can uncover what we term "token noise" effects, a phenomenon where LLM
decisions are disproportionately influenced by tokens providing minimal
informative content. This phenomenon raises concerns about the robustness and
generalizability of insights obtained from LLMs in the context of human
behavior simulation. Our model-agnostic approach extends its utility to
proprietary LLMs, providing a valuable tool for practitioners and researchers
to strategically optimize prompts and mitigate apparent cognitive biases. Our
findings underscore the need for a more nuanced understanding of the factors
driving LLM responses before relying on them as substitutes for human subjects
in survey settings. We emphasize the importance of researchers reporting
results conditioned on specific prompt templates and exercising caution when
drawing parallels between human behavior and LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Identification of Hate Speech towards Islam using Graph
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04916v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04916v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Islamophobic language on online platforms fosters intolerance, making
detection and elimination crucial for promoting harmony. Traditional hate
speech detection models rely on NLP techniques like tokenization,
part-of-speech tagging, and encoder-decoder models. However, Graph Neural
Networks (GNNs), with their ability to utilize relationships between data
points, offer more effective detection and greater explainability. In this
work, we represent speeches as nodes and connect them with edges based on their
context and similarity to develop the graph. This study introduces a novel
paradigm using GNNs to identify and explain hate speech towards Islam. Our
model leverages GNNs to understand the context and patterns of hate speech by
connecting texts via pretrained NLP-generated word embeddings, achieving
state-of-the-art performance and enhancing detection accuracy while providing
valuable explanations. This highlights the potential of GNNs in combating
online hate speech and fostering a safer, more inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (Non-archival)
  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP
  2024 : NLP for Positive Impact Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4pi-1.23/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogErg<span class="highlight-title">LLM</span>: Exploring <span class="highlight-title">Large Language Model</span> Systems Design Perspective
  Using Cognitive Ergonomics <span class="chip">ICML'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02885v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02885v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Mst Rafia Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating cognitive ergonomics with LLMs is crucial for improving safety,
reliability, and user satisfaction in human-AI interactions. Current LLM
designs often lack this integration, resulting in systems that may not fully
align with human cognitive capabilities and limitations. This oversight
exacerbates biases in LLM outputs and leads to suboptimal user experiences due
to inconsistent application of user-centered design principles. Researchers are
increasingly leveraging NLP, particularly LLMs, to model and understand human
behavior across social sciences, psychology, psychiatry, health, and
neuroscience. Our position paper explores the need to integrate cognitive
ergonomics into LLM design, providing a comprehensive framework and practical
guidelines for ethical development. By addressing these challenges, we aim to
advance safer, more reliable, and ethically sound human-AI interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Page, 3 Figures. Accepted in: (i) ICML'24: LLMs & Cognition
  Workshop (Non-archival; OpenReview:
  https://openreview.net/forum?id=63C9YSc77p) (ii) EMNLP'24 : NLP for Science
  Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4science-1.22/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlossLM: A Massively Multilingual Corpus and <span class="highlight-title">Pretrain</span>ed Model for
  Interlinear Glossed Text <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06399v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06399v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ginn, Lindia Tjuatja, Taiqi He, Enora Rice, Graham Neubig, Alexis Palmer, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language documentation projects often involve the creation of annotated text
in a format such as interlinear glossed text (IGT), which captures fine-grained
morphosyntactic analyses in a morpheme-by-morpheme format. However, there are
few existing resources providing large amounts of standardized, easily
accessible IGT data, limiting their applicability to linguistic research, and
making it difficult to use such data in NLP modeling.
  We compile the largest existing corpus of IGT data from a variety of sources,
covering over 450k examples across 1.8k languages, to enable research on
crosslingual transfer and IGT generation. We normalize much of our data to
follow a standard set of labels across languages.
  Furthermore, we explore the task of automatically generating IGT in order to
aid documentation projects. As many languages lack sufficient monolingual data,
we pretrain a large multilingual model on our corpus. We demonstrate the
utility of this model by finetuning it on monolingual corpora, outperforming
SOTA models by up to 6.6\%. Our pretrained model and dataset are available on
Hugging Face.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. First two authors are equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One fish, two fish, but not the whole sea: Alignment reduces language
  models' conceptual diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia K. Murthy, Tomer Ullman, Jennifer Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers in social science and psychology have recently proposed using
large language models (LLMs) as replacements for humans in behavioral research.
In addition to arguments about whether LLMs accurately capture population-level
patterns, this has raised questions about whether LLMs capture human-like
conceptual diversity. Separately, it is debated whether post-training alignment
(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,
we use a new way of measuring the conceptual diversity of
synthetically-generated LLM "populations" by relating the internal variability
of simulated individuals to the population-level variability. We use this
approach to evaluate non-aligned and aligned LLMs on two domains with rich
human behavioral data. While no model reaches human-like diversity, aligned
models generally display less diversity than their instruction fine-tuned
counterparts. Our findings highlight potential trade-offs between increasing
models' value alignment and decreasing the diversity of their conceptual
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures; corrected figure version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeTree: <span class="highlight-title">Agent</span>-guided Tree Search for Code Generation with Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained on massive amounts of code and text data, large language models
(LLMs) have demonstrated remarkable achievements in performing code generation
tasks. With additional execution-based feedback, these models can act as agents
with capabilities to self-refine and improve generated code autonomously.
However, on challenging coding tasks with extremely large search space, current
agentic approaches still struggle with multi-stage planning, generating, and
debugging. To address this problem, we propose CodeTree, a framework for LLM
agents to efficiently explore the search space in different stages of the code
generation process. Specifically, we adopted a unified tree structure to
explicitly explore different coding strategies, generate corresponding coding
solutions, and subsequently refine the solutions. In each stage, critical
decision-making (ranking, termination, expanding) of the exploration process is
guided by both the environmental execution-based feedback and
LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code
generation benchmarks and demonstrated the significant performance gains of
CodeTree against strong baselines. Using GPT-4o as the base model, we
consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0
on CodeContests. On the challenging SWEBench benchmark, our approach led to
significant performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-aware Inductive Knowledge Graph Completion with Latent Type
  Constraints and Subgraph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzhi Li, Cehao Yang, Chengjin Xu, Zixing Song, Xuhui Jiang, Jian Guo, Ho-fung Leung, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive knowledge graph completion (KGC) aims to predict missing triples
with unseen entities. Recent works focus on modeling reasoning paths between
the head and tail entity as direct supporting evidence. However, these methods
depend heavily on the existence and quality of reasoning paths, which limits
their general applicability in different scenarios. In addition, we observe
that latent type constraints and neighboring facts inherent in KGs are also
vital in inferring missing triples. To effectively utilize all useful
information in KGs, we introduce CATS, a novel context-aware inductive KGC
solution. With sufficient guidance from proper prompts and supervised
fine-tuning, CATS activates the strong semantic understanding and reasoning
capabilities of large language models to assess the existence of query triples,
which consist of two modules. First, the type-aware reasoning module evaluates
whether the candidate entity matches the latent entity type as required by the
query relation. Then, the subgraph reasoning module selects relevant reasoning
paths and neighboring facts, and evaluates their correlation to the query
triple. Experiment results on three widely used datasets demonstrate that CATS
significantly outperforms state-of-the-art methods in 16 out of 18
transductive, inductive, and few-shot settings with an average absolute MRR
improvement of 7.2%.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the \textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a
novel approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretical Analysis of Recommendation Loss Functions under Negative
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Di Teodoro, Federico Siciliano, Nicola Tonellotto, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) are pivotal in diverse domains such as e-commerce,
music streaming, and social media. This paper conducts a comparative analysis
of prevalent loss functions in RSs: Binary Cross-Entropy (BCE), Categorical
Cross-Entropy (CCE), and Bayesian Personalized Ranking (BPR). Exploring the
behaviour of these loss functions across varying negative sampling settings, we
reveal that BPR and CCE are equivalent when one negative sample is used.
Additionally, we demonstrate that all losses share a common global minimum.
Evaluation of RSs mainly relies on ranking metrics known as Normalized
Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR). We produce
bounds of the different losses for negative sampling settings to establish a
probabilistic lower bound for NDCG. We show that the BPR bound on NDCG is
weaker than that of BCE, contradicting the common assumption that BPR is
superior to BCE in RSs training. Experiments on five datasets and four models
empirically support these theoretical findings. Our code is available at
\url{https://anonymous.4open.science/r/recsys_losses} .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper 8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Legal Knowledge with Multi-Layered Embedding-Based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Alberto de Oliveira Lima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the challenge of capturing the complexities of legal
knowledge by proposing a multi-layered embedding-based retrieval method for
legal and legislative texts. Creating embeddings not only for individual
articles but also for their components (paragraphs, clauses) and structural
groupings (books, titles, chapters, etc), we seek to capture the subtleties of
legal information through the use of dense vectors of embeddings, representing
it at varying levels of granularity. Our method meets various information needs
by allowing the Retrieval Augmented Generation system to provide accurate
responses, whether for specific segments or entire sections, tailored to the
user's query. We explore the concepts of aboutness, semantic chunking, and
inherent hierarchy within legal texts, arguing that this method enhances the
legal information retrieval. Despite the focus being on Brazil's legislative
methods and the Brazilian Constitution, which follow a civil law tradition, our
findings should in principle be applicable across different legal systems,
including those adhering to common law traditions. Furthermore, the principles
of the proposed method extend beyond the legal domain, offering valuable
insights for organizing and retrieving information in any field characterized
by information encoded in hierarchical text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Sustainability via Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Lei Zhang, Honglei Zhang, Yixin Zhang, Xiaoxiong Zhang, Jie Zhang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human behavioral patterns and consumption paradigms have emerged as pivotal
determinants in environmental degradation and climate change, with quotidian
decisions pertaining to transportation, energy utilization, and resource
consumption collectively precipitating substantial ecological impacts.
Recommender systems, which generate personalized suggestions based on user
preferences and historical interaction data, exert considerable influence on
individual behavioral trajectories. However, conventional recommender systems
predominantly optimize for user engagement and economic metrics, inadvertently
neglecting the environmental and societal ramifications of their
recommendations, potentially catalyzing over-consumption and reinforcing
unsustainable behavioral patterns. Given their instrumental role in shaping
user decisions, there exists an imperative need for sustainable recommender
systems that incorporate sustainability principles to foster eco-conscious and
socially responsible choices. This comprehensive survey addresses this critical
research gap by presenting a systematic analysis of sustainable recommender
systems. As these systems can simultaneously advance multiple sustainability
objectives--including resource conservation, sustainable consumer behavior, and
social impact enhancement--examining their implementations across distinct
application domains provides a more rigorous analytical framework. Through a
methodological analysis of domain-specific implementations encompassing
transportation, food, buildings, and auxiliary sectors, we can better elucidate
how these systems holistically advance sustainability objectives while
addressing sector-specific constraints and opportunities. Moreover, we
delineate future research directions for evolving recommender systems beyond
sustainability advocacy toward fostering environmental resilience and social
consciousness in society.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages, 10 figures. Working paper: https://github.com/enoche/SusRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overhead-free User-side Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, recommendation algorithms have been designed for service
developers. But recently, a new paradigm called user-side recommender systems
has been proposed. User-side recommender systems are built and used by end
users, in sharp contrast to traditional provider-side recommender systems. Even
if the official recommender system offered by the provider is not fair, end
users can create and enjoy their own user-side recommender systems by
themselves. Although the concept of user-side recommender systems is
attractive, the problem is they require tremendous communication costs between
the user and the official system. Even the most efficient user-side recommender
systems require about 5 times more costs than provider-side recommender
systems. Such high costs hinder the adoption of user-side recommender systems.
In this paper, we propose overhead-free user-side recommender systems,
RecCycle, which realizes user-side recommender systems without any
communication overhead. The main idea of RecCycle is to recycle past
recommendation results offered by the provider's recommender systems. The
ingredients of RecCycle can be retrieved ``for free,'' and it greatly reduces
the cost of user-side recommendations. In the experiments, we confirm that
RecCycle performs as well as state-of-the-art user-side recommendation
algorithms while RecCycle reduces costs significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2208.09864,
  arXiv:2403.15757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automated Model Design on Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Yudong Liu, Feng Cheng, Yufan Cao, Feng Yan, Hai Li, Yiran Chen, Wei Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing popularity of deep learning models has created new
opportunities for developing AI-based recommender systems. Designing
recommender systems using deep neural networks requires careful architecture
design, and further optimization demands extensive co-design efforts on jointly
optimizing model architecture and hardware. Design automation, such as
Automated Machine Learning (AutoML), is necessary to fully exploit the
potential of recommender model design, including model choices and
model-hardware co-design strategies. We introduce a novel paradigm that
utilizes weight sharing to explore abundant solution spaces. Our paradigm
creates a large supernet to search for optimal architectures and co-design
strategies to address the challenges of data multi-modality and heterogeneity
in the recommendation domain. From a model perspective, the supernet includes a
variety of operators, dense connectivity, and dimension search options. From a
co-design perspective, it encompasses versatile Processing-In-Memory (PIM)
configurations to produce hardware-efficient models. Our solution space's
scale, heterogeneity, and complexity pose several challenges, which we address
by proposing various techniques for training and evaluating the supernet. Our
crafted models show promising results on three Click-Through Rates (CTR)
prediction benchmarks, outperforming both manually designed and AutoML-crafted
models with state-of-the-art performance when focusing solely on architecture
search. From a co-design perspective, we achieve 2x FLOPs efficiency, 1.8x
energy efficiency, and 1.5x performance improvements in recommender models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM Transactions on Recommender Systems. arXiv admin
  note: substantial text overlap with arXiv:2207.07187</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Interaction Fusion Self-<span class="highlight-title">Distill</span>ation Network For CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sang, Qiuze Ru, Honghao Li, Yiwen Zhang, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction plays a vital role in recommender
systems, online advertising, and search engines. Most of the current approaches
model feature interactions through stacked or parallel structures, with some
employing knowledge distillation for model compression. However, we observe
some limitations with these approaches: (1) In parallel structure models, the
explicit and implicit components are executed independently and simultaneously,
which leads to insufficient information sharing within the feature set. (2) The
introduction of knowledge distillation technology brings about the problems of
complex teacher-student framework design and low knowledge transfer efficiency.
(3) The dataset and the process of constructing high-order feature interactions
contain significant noise, which limits the model's effectiveness. To address
these limitations, we propose FSDNet, a CTR prediction framework incorporating
a plug-and-play fusion self-distillation module. Specifically, FSDNet forms
connections between explicit and implicit feature interactions at each layer,
enhancing the sharing of information between different features. The deepest
fused layer is then used as the teacher model, utilizing self-distillation to
guide the training of shallow layers. Empirical evaluation across four
benchmark datasets validates the framework's efficacy and generalization
capabilities. The code is available on https://github.com/coder-qiu/FSDNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search
  in Deep Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wei, Yuekui Yang, Yang Zhang, Haiyang Wu, Meixi Liu, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to
represent various categorical features. Traditional DLRMs adopt unified
embedding size for all features, leading to suboptimal performance and
redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works
focus on obtaining mixed embedding sizes with strong model performance.
However, previous AES works can hardly address several challenges together: (1)
The search results of embedding sizes are unstable; (2) Recommendation effect
with AES results is unsatisfactory; (3) Memory cost of embeddings is
uncontrollable. To address these challenges, we propose a novel one-shot AES
framework called AdaS&S, in which a supernet encompassing various candidate
embeddings is built and AES is performed as searching network architectures
within it. Our framework contains two main stages: In the first stage, we
decouple training parameters from searching embedding sizes, and propose the
Adaptive Sampling method to yield a well-trained supernet, which further helps
to produce stable AES results. In the second stage, to obtain embedding sizes
that benefits the model effect, we design a reinforcement learning search
process which utilizes the supernet trained previously. Meanwhile, to adapt
searching to specific resource constraint, we introduce the resource
competition penalty to balance the model effectiveness and memory cost of
embeddings. We conduct extensive experiments on public datasets to show the
superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving
about 20% of model parameters. Empirical analysis also shows that the stability
of searching results in AdaS&S significantly exceeds other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Link Prediction with Fuzzy Graph Attention Networks and
  Dynamic Negative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit and Implicit Semantic Ranking Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Thomas Lin, Vishal Anand, Matthew Calderwood, Eric Clausen-Brown, Gord Lueck, Wen-wai Yim, Cheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge in numerous real-world applications is to match an inquiry
to the best document from a mutable and finite set of candidates. Existing
industry solutions, especially latency-constrained services, often rely on
similarity algorithms that sacrifice quality for speed. In this paper we
introduce a generic semantic learning-to-rank framework, Self-training Semantic
Cross-attention Ranking (sRank). This transformer-based framework uses linear
pairwise loss with mutable training batch sizes and achieves quality gains and
high efficiency, and has been applied effectively to show gains on two industry
tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and
Ambient Clinical Intelligence (ACI). In Smart Reply, sRank assists live
customers with technical support by selecting the best reply from predefined
solutions based on consumer and support agent messages. It achieves 11.7% gain
in offline top-one accuracy on the SR task over the previous system, and has
enabled 38.7% time reduction in composing messages in telemetry recorded since
its general release in January 2021. In the ACI task, sRank selects relevant
historical physician templates that serve as guidance for a text summarization
model to generate higher quality medical notes. It achieves 35.5% top-one
accuracy gain, along with 46% relative ROUGE-L gain in generated medical notes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Content-Based Collaborative Generation for Recommender Systems <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, Zhumin Chen, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have emerged as a promising utility to enhance recommender
systems. It is essential to model both item content and user-item collaborative
interactions in a unified generative framework for better recommendation.
Although some existing large language model (LLM)-based methods contribute to
fusing content information and collaborative signals, they fundamentally rely
on textual language generation, which is not fully aligned with the
recommendation task. How to integrate content knowledge and collaborative
interaction signals in a generative framework tailored for item recommendation
is still an open research challenge.
  In this paper, we propose content-based collaborative generation for
recommender systems, namely ColaRec. ColaRec is a sequence-to-sequence
framework which is tailored for directly generating the recommended item
identifier. Precisely, the input sequence comprises data pertaining to the
user's interacted items, and the output sequence represents the generative
identifier (GID) for the suggested item. To model collaborative signals, the
GIDs are constructed from a pretrained collaborative filtering model, and the
user is represented as the content aggregation of interacted items. To this
end, ColaRec captures both collaborative signals and content information in a
unified framework. Then an item indexing task is proposed to conduct the
alignment between the content-based semantic space and the interaction-based
collaborative space. Besides, a contrastive loss is further introduced to
ensure that items with similar collaborative GIDs have similar content
representations. To verify the effectiveness of ColaRec, we conduct experiments
on four benchmark datasets. Empirical results demonstrate the superior
performance of ColaRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2024; GitHub:
  https://github.com/Junewang0614/ColaRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Enhancing Prediction in Social Network
  Advertisement through Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qikai Yang, Panfeng Li, Xinhe Xu, Zhicheng Ding, Wenjing Zhou, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of social network advertising, the volume and
accuracy of data play a critical role in the performance of predictive models.
However, the development of robust predictive algorithms is often hampered by
the limited size and potential bias present in real-world datasets. This study
presents and explores a generative augmentation framework of social network
advertising data. Our framework explores three generative models for data
augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders
(VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and
diversity in the context of social network advertising analytics effectiveness.
By performing synthetic extensions of the feature space, we find that through
data augmentation, the performance of various classifiers has been
quantitatively improved. Furthermore, we compare the relative performance gains
brought by each data augmentation technique, providing insights for
practitioners to select appropriate techniques to enhance model performance.
This paper contributes to the literature by showing that synthetic data
augmentation alleviates the limitations imposed by small or imbalanced datasets
in the field of social network advertising. At the same time, this article also
provides a comparative perspective on the practicality of different data
augmentation methods, thereby guiding practitioners to choose appropriate
techniques to enhance model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 4th International Conference on Machine Learning and
  Intelligent Systems Engineering (MLISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Identification of Hate Speech towards Islam using Graph
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04916v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04916v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Islamophobic language on online platforms fosters intolerance, making
detection and elimination crucial for promoting harmony. Traditional hate
speech detection models rely on NLP techniques like tokenization,
part-of-speech tagging, and encoder-decoder models. However, Graph Neural
Networks (GNNs), with their ability to utilize relationships between data
points, offer more effective detection and greater explainability. In this
work, we represent speeches as nodes and connect them with edges based on their
context and similarity to develop the graph. This study introduces a novel
paradigm using GNNs to identify and explain hate speech towards Islam. Our
model leverages GNNs to understand the context and patterns of hate speech by
connecting texts via pretrained NLP-generated word embeddings, achieving
state-of-the-art performance and enhancing detection accuracy while providing
valuable explanations. This highlights the potential of GNNs in combating
online hate speech and fostering a safer, more inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (Non-archival)
  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP
  2024 : NLP for Positive Impact Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4pi-1.23/)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">157</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>Phy: Complex Physical Reasoning Using <span class="highlight-title">Large Language Model</span>s and World
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leonardo vindicated: Pythagorean trees for minimal reconstruction of the
  natural branching structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dymitr Ruta, Corrado Mio, Ernesto Damiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trees continue to fascinate with their natural beauty and as engineering
masterpieces optimal with respect to several independent criteria. Pythagorean
tree is a well-known fractal design that realistically mimics the natural tree
branching structures. We study various types of Pythagorean-like fractal trees
with different shapes of the base, branching angles and relaxed scales in an
attempt to identify and explain which variants are the closest match to the
branching structures commonly observed in the natural world. Pursuing
simultaneously the realism and minimalism of the fractal tree model, we have
developed a flexibly parameterised and fast algorithm to grow and visually
examine deep Pythagorean-inspired fractal trees with the capability to orderly
over- or underestimate the Leonardo da Vinci's tree branching rule as well as
control various imbalances and branching angles. We tested the realism of the
generated fractal tree images by means of the classification accuracy of
detecting natural tree with the transfer-trained deep Convolutional Neural
Networks (CNNs). Having empirically established the parameters of the fractal
trees that maximize the CNN's natural tree class classification accuracy we
have translated them back to the scales and angles of branches and came to the
interesting conclusions that support the da Vinci branching rule and golden
ratio based scaling for both the shape of the branch and imbalance between the
child branches, and claim the flexibly parameterized fractal trees can be used
to generate artificial examples to train robust detectors of different species
of trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, lots of hi res figures I had to reduce quality of,
  submitting as a requirement to the Theory of Computing Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language Model</span>s as Causal Effect Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius E. J. Bynum, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelet Latent <span class="highlight-title">Diffusion</span> (Wala): Billion-Parameter 3D <span class="highlight-title">Generative</span> Model
  with Compact Wavelet Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sanghi, Aliasghar Khani, Pradyumna Reddy, Arianna Rampini, Derek Cheung, Kamal Rahimi Malekshan, Kanika Madan, Hooman Shayani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Effectiveness of Explainability Methods in Parkinson's
  Detection from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Mancini, Francesco Paissan, Paolo Torroni, Cem Subakan, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this research: author
  order is alphabetical</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivational Morphology Reveals Analogical Generalization in Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Schütze, Janet Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact, Tractable Gauss-Newton Optimization in Deep Reversible
  Architectures Reveal Poor Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Buffelli, Jamie McGowan, Wangkun Xu, Alexandru Cioba, Da-shan Shiu, Guillaume Hennequin, Alberto Bernacchia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order optimization has been shown to accelerate the training of deep
neural networks in many applications, often yielding faster progress per
iteration on the training loss compared to first-order optimizers.However, the
generalization properties of second-order methods are still being debated.
Theoretical investigations have proved difficult to carry out outside the
tractable settings of heavily simplified model classes -- thus, the relevance
of existing theories to practical deep learning applications remains unclear.
Similarly, empirical studies in large-scale models and real datasets are
significantly confounded by the necessity to approximate second-order updates
in practice. It is often unclear whether the observed generalization behaviour
arises specifically from the second-order nature of the parameter updates, or
instead reflects the specific structured (e.g.\ Kronecker) approximations used
or any damping-based interpolation towards first-order updates. Here, we show
for the first time that exact Gauss-Newton (GN) updates take on a tractable
form in a class of deep reversible architectures that are sufficiently
expressive to be meaningfully applied to common benchmark datasets. We exploit
this novel setting to study the training and generalization properties of the
GN optimizer. We find that exact GN generalizes poorly. In the mini-batch
training setting, this manifests as rapidly saturating progress even on the
\emph{training} loss, with parameter updates found to overfit each
mini-batchatch without producing the features that would support generalization
to other mini-batches. We show that our experiments run in the ``lazy'' regime,
in which the neural tangent kernel (NTK) changes very little during the course
of training. This behaviour is associated with having no significant changes in
neural representations, explaining the lack of generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Robust Regression Discontinuity Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a doubly robust (DR) estimator for regression
discontinuity (RD) designs. In RD designs, treatment effects are estimated in a
quasi-experimental setting where treatment assignment depends on whether a
running variable surpasses a predefined cutoff. A common approach in RD
estimation is to apply nonparametric regression methods, such as local linear
regression. In such an approach, the validity relies heavily on the consistency
of nonparametric estimators and is limited by the nonparametric convergence
rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we
propose the DR-RD estimator, which combines two distinct estimators for the
conditional expected outcomes. If either of these estimators is consistent, the
treatment effect estimator remains consistent. Furthermore, due to the
debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if
both regression estimators satisfy certain mild conditions, which also
simplifies statistical inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Control of Mechanical Ventilators with Learned Respiratory
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Ronald Ward, Dylan M. Asmar, Mansur Arief, Jana Krystofova Mike, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deciding on appropriate mechanical ventilator management strategies
significantly impacts the health outcomes for patients with respiratory
diseases. Acute Respiratory Distress Syndrome (ARDS) is one such disease that
requires careful ventilator operation to be effectively treated. In this work,
we frame the management of ventilators for patients with ARDS as a sequential
decision making problem using the Markov decision process framework. We
implement and compare controllers based on clinical guidelines contained in the
ARDSnet protocol, optimal control theory, and learned latent dynamics
represented as neural networks. The Pulse Physiology Engine's respiratory
dynamics simulator is used to establish a repeatable benchmark, gather
simulated data, and quantitatively compare these controllers. We score
performance in terms of measured improvement in established ARDS health markers
(pertaining to improved respiratory rate, oxygenation, and vital signs). Our
results demonstrate that techniques leveraging neural networks and optimal
control can automatically discover effective ventilation management strategies
without access to explicit ventilator management procedures or guidelines (such
as those defined in the ARDSnet protocol).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE 37th International Symposium on Computer-Based Medical
  Systems (CBMS), 7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sleep Staging from Airflow Signals Using Fourier Approximations of
  Persistence Curves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Manjunath, Hau-Tieng Wu, Aarti Sathyanarayana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sleep staging is a challenging task, typically manually performed by sleep
technologists based on electroencephalogram and other biosignals of patients
taken during overnight sleep studies. Recent work aims to leverage automated
algorithms to perform sleep staging not based on electroencephalogram signals,
but rather based on the airflow signals of subjects. Prior work uses ideas from
topological data analysis (TDA), specifically Hermite function expansions of
persistence curves (HEPC) to featurize airflow signals. However, finite order
HEPC captures only partial information. In this work, we propose Fourier
approximations of persistence curves (FAPC), and use this technique to perform
sleep staging based on airflow signals. We analyze performance using an XGBoost
model on 1155 pediatric sleep studies taken from the Nationwide Children's
Hospital Sleep DataBank (NCHSDB), and find that FAPC methods provide
complimentary information to HEPC methods alone, leading to a 4.9% increase in
performance over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of <span class="highlight-title">Continual</span> Federated Learning Using <span class="highlight-title">Incremental</span>ly
  Aggregated Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satish Kumar Keshri, Nazreen Shah, Ranjitha Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The holy grail of machine learning is to enable Continual Federated Learning
(CFL) to enhance the efficiency, privacy, and scalability of AI systems while
learning from streaming data. The primary challenge of a CFL system is to
overcome global catastrophic forgetting, wherein the accuracy of the global
model trained on new tasks declines on the old tasks. In this work, we propose
Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel
replay-memory based federated strategy consisting of edge-based gradient
updates on memory and aggregated gradients on the current data. We provide
convergence analysis of the C-FLAG approach which addresses forgetting and bias
while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We
formulate an optimization sub-problem that minimizes catastrophic forgetting,
translating CFL into an iterative algorithm with adaptive learning rates that
ensure seamless learning across tasks. We empirically show that C-FLAG
outperforms several state-of-the-art baselines on both task and
class-incremental settings with respect to metrics such as accuracy and
forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tukey g-and-h neural network regression for non-Gaussian data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur P. Guillaumin, Natalia Efremova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses non-Gaussian regression with neural networks via the use
of the Tukey g-and-h distribution.The Tukey g-and-h transform is a flexible
parametric transform with two parameters $g$ and $h$ which, when applied to a
standard normal random variable, introduces both skewness and kurtosis,
resulting in a distribution commonly called the Tukey g-and-h distribution.
Specific values of $g$ and $h$ produce good approximations to other families of
distributions, such as the Cauchy and student-t distributions. The flexibility
of the Tukey g-and-h distribution has driven its popularity in the statistical
community, in applied sciences and finance. In this work we consider the
training of a neural network to predict the parameters of a Tukey g-and-h
distribution in a regression framework via the minimization of the
corresponding negative log-likelihood, despite the latter having no closed-form
expression. We demonstrate the efficiency of our procedure in simulated
examples and apply our method to a real-world dataset of global crop yield for
several types of crops. Finally, we show how we can carry out a goodness-of-fit
analysis between the predicted distributions and the test data. A Pytorch
implementation is made available on Github and as a Pypi package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Memory Mechanisms for Decision Making through Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yue, Bo Liu, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Partially Observable Markov Decision Processes, integrating an agent's
history into memory poses a significant challenge for decision-making.
Traditional imitation learning, relying on observation-action pairs for expert
demonstrations, fails to capture the expert's memory mechanisms used in
decision-making. To capture memory processes as demonstrations, we introduce
the concept of \textbf{memory dependency pairs} $(p, q)$ indicating that events
at time $p$ are recalled for decision-making at time $q$. We introduce
\textbf{AttentionTuner} to leverage memory dependency pairs in Transformers and
find significant improvements across several tasks compared to standard
Transformers when evaluated on Memory Gym and the Long-term Memory Benchmark.
Code is available at https://github.com/WilliamYue37/AttentionTuner .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Low-bit Communication for Tensor Parallel <span class="highlight-title">LLM</span> Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Dong, Tyler Johnson, Minsik Cho, Emad Soroush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor parallelism provides an effective way to increase server large
language model (LLM) inference efficiency despite adding an additional
communication cost. However, as server LLMs continue to scale in size, they
will need to be distributed across more devices, magnifying the communication
cost. One way to approach this problem is with quantization, but current
methods for LLMs tend to avoid quantizing the features that tensor parallelism
needs to communicate. Taking advantage of consistent outliers in communicated
features, we introduce a quantization method that reduces communicated values
on average from 16 bits to 4.2 bits while preserving nearly all of the original
performance. For instance, our method maintains around 98.0% and 99.5% of Gemma
2 27B's and Llama 2 13B's original performance, respectively, averaged across
all tasks we evaluated on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Mild Generalization for Offline Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiu Mao, Qi Wang, Yun Qu, Yuhang Jiang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) suffers from the extrapolation error and
value overestimation. From a generalization perspective, this issue can be
attributed to the over-generalization of value functions or policies towards
out-of-distribution (OOD) actions. Significant efforts have been devoted to
mitigating such generalization, and recent in-sample learning approaches have
further succeeded in entirely eschewing it. Nevertheless, we show that mild
generalization beyond the dataset can be trusted and leveraged to improve
performance under certain conditions. To appropriately exploit generalization
in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild
action generalization and (ii) mild generalization propagation. The former
refers to selecting actions in a close neighborhood of the dataset to maximize
the Q values. Even so, the potential erroneous generalization can still be
propagated, accumulated, and exacerbated by bootstrapping. In light of this,
the latter concept is introduced to mitigate the generalization propagation
without impeding the propagation of RL learning signals. Theoretically, DMG
guarantees better performance than the in-sample optimal policy in the oracle
generalization scenario. Even under worst-case generalization, DMG can still
control value overestimation at a certain level and lower bound the
performance. Empirically, DMG achieves state-of-the-art performance across
Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting
from its flexibility in both generalization aspects, DMG enjoys a seamless
transition from offline to online learning and attains strong online
fine-tuning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. arXiv admin note: substantial text overlap
  with arXiv:2410.19400</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of Acoustic Communication Performance for AUVs using Gaussian
  Process Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Gao, Harun Yetkin, McMahon James, Daniel J. Stilwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic
communication to coordinate their actions effectively. However, the reliability
of underwater acoustic communication decreases as the communication range
between vehicles increases. Consequently, teams of cooperating AUVs typically
make conservative assumptions about the maximum range at which they can
communicate reliably. To address this limitation, we propose a novel approach
that involves learning a map representing the probability of successful
communication based on the locations of the transmitting and receiving
vehicles. This probabilistic communication map accounts for factors such as the
range between vehicles, environmental noise, and multi-path effects at a given
location. In pursuit of this goal, we investigate the application of Gaussian
process binary classification to generate the desired communication map. We
specialize existing results to this specific binary classification problem and
explore methods to incorporate uncertainty in vehicle location into the mapping
process. Furthermore, we compare the prediction performance of the probability
communication map generated using binary classification with that of a
signal-to-noise ratio (SNR) communication map generated using Gaussian process
regression. Our approach is experimentally validated using communication and
navigation data collected during trials with a pair of Virginia Tech 690 AUVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stochastic Optimization Framework for Private and Fair Learning From
  Decentralized Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh Gupta, A. S. Poornash, Andrew Lowy, Meisam Razaviyayn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are often trained on sensitive data (e.g., medical
records and race/gender) that is distributed across different "silos" (e.g.,
hospitals). These federated learning models may then be used to make
consequential decisions, such as allocating healthcare resources. Two key
challenges emerge in this setting: (i) maintaining the privacy of each person's
data, even if other silos or an adversary with access to the central server
tries to infer this data; (ii) ensuring that decisions are fair to different
demographic groups (e.g., race/gender). In this paper, we develop a novel
algorithm for private and fair federated learning (FL). Our algorithm satisfies
inter-silo record-level differential privacy (ISRL-DP), a strong notion of
private FL requiring that silo i's sent messages satisfy record-level
differential privacy for all i. Our framework can be used to promote different
fairness notions, including demographic parity and equalized odds. We prove
that our algorithm converges under mild smoothness assumptions on the loss
function, whereas prior work required strong convexity for convergence. As a
byproduct of our analysis, we obtain the first convergence guarantee for
ISRL-DP nonconvex-strongly concave min-max FL. Experiments demonstrate the
state-of-the-art fairness-accuracy tradeoffs of our algorithm across different
privacy levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INTRABENCH: Interactive Radiological Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Ulrich, Tassilo Wald, Emily Tempus, Maximilian Rokuss, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current interactive segmentation approaches, inspired by the success of
META's Segment Anything model, have achieved notable advancements, however,
they come with substantial limitations that hinder their practical application
in real clinical scenarios. These include unrealistic human interaction
requirements, such as slice-by-slice operations for 2D models on 3D data, a
lack of iterative refinement, and insufficient evaluation experiments. These
shortcomings prevent accurate assessment of model performance and lead to
inconsistent outcomes across studies. IntRaBench overcomes these challenges by
offering a comprehensive and reproducible framework for evaluating interactive
segmentation methods in realistic, clinically relevant scenarios. It includes
diverse datasets, target structures, and segmentation models, and provides a
flexible codebase that allows seamless integration of new models and prompting
strategies. Additionally, we introduce advanced techniques to minimize
clinician interaction, ensuring fair comparisons between 2D and 3D models. By
open-sourcing IntRaBench, we invite the research community to integrate their
models and prompting techniques, ensuring continuous and transparent evaluation
of interactive segmentation models in 3D medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Undergoing Peer-Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse capability and scaling of <span class="highlight-title">diffusion</span> and auto-regressive models
  when learning abstract rules <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binxu Wang, Jiaqi Shang, Haim Sompolinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures. Accepted to NeurIPS2024 Workshop on System 2
  Reasoning At Scale as long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDXFormer: Boosting Remote Sensing Change Detection with Extended Long
  Short-Term Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenkai Wu, Xiaowen Ma, Rongrong Lian, Zhentao Lin, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex scenes and varied conditions, effectively integrating
spatial-temporal context is crucial for accurately identifying changes.
However, current RS-CD methods lack a balanced consideration of performance and
efficiency. CNNs lack global context, Transformers have quadratic computational
complexity, and Mambas are restricted by CUDA acceleration. In this paper, we
propose CDXFormer, with a core component that is a powerful XLSTM-based feature
enhancement layer, integrating the advantages of linear computational
complexity, global context perception, and strong interpret-ability.
Specifically, we introduce a scale-specific Feature Enhancer layer,
incorporating a Cross-Temporal Global Perceptron customized for
semantic-accurate deep features, and a Cross-Temporal Spatial Refiner
customized for detail-rich shallow features. Additionally, we propose a
Cross-Scale Interactive Fusion module to progressively interact global change
representations with spatial responses. Extensive experimental results
demonstrate that CDXFormer achieves state-of-the-art performance across three
benchmark datasets, offering a compelling balance between efficiency and
accuracy. Code is available at https://github.com/xwmaxwma/rschange.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tucano: Advancing Neural Text Generation for Portuguese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kluge Corrêa, Aniket Sen, Sophia Falk, Shiza Fatimah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential time-to-<span class="highlight-title">event</span> prediction model with well-calibrated
  uncertainty estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Huang, Yucheng Xing, Swapnil Mishra, Thierry Denoeux, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-to-event analysis, or Survival analysis, provides valuable insights into
clinical prognosis and treatment recommendations. However, this task is
typically more challenging than other regression tasks due to the censored
observations. Moreover, concerns regarding the reliability of predictions
persist among clinicians, mainly attributed to the absence of confidence
assessment, robustness, and calibration of prediction. To address those
challenges, we introduce an evidential regression model designed especially for
time-to-event prediction tasks, with which the most plausible event time, is
directly quantified by aggregated Gaussian random fuzzy numbers (GRFNs). The
GRFNs are a newly introduced family of random fuzzy subsets of the real line
that generalizes both Gaussian random variables and Gaussian possibility
distributions. Different from conventional methods that construct models based
on strict data distribution, e.g., proportional hazard function, our model only
assumes the event time is encoded in a real line GFRN without any strict
distribution assumption, therefore offering more flexibility in complex data
scenarios. Furthermore, the epistemic and aleatory uncertainty regarding the
event time is quantified within the aggregated GRFN as well. Our model can,
therefore, provide more detailed clinical decision-making guidance with two
more degrees of information. The model is fit by minimizing a generalized
negative log-likelihood function that accounts for data censoring based on
uncertainty evidence reasoning. Experimental results on simulated datasets with
varying data distributions and censoring scenarios, as well as on real-world
datasets across diverse clinical settings and tasks, demonstrate that our model
achieves both accurate and reliable performance, outperforming state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for
  Scalable Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Zmushko, Aleksandr Beznosikov, Martin Takáč, Samuel Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in the number of parameters in large language models, the
process of pre-training and fine-tuning increasingly demands larger volumes of
GPU memory. A significant portion of this memory is typically consumed by the
optimizer state. To overcome this challenge, recent approaches such as low-rank
adaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao
et al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been
proposed. However, in all these algorithms, the $\textit{effective rank of the
weight updates remains low-rank}$, which can lead to a substantial loss of
information from the gradient. This loss can be critically important,
especially during the pre-training stage. In this paper, we introduce
$\texttt{FRUGAL}$ ($\textbf{F}$ull-$\textbf{R}$ank $\textbf{U}$pdates with
$\textbf{G}$r$\textbf{A}$dient sp$\textbf{L}$itting), a new memory-efficient
optimization framework. $\texttt{FRUGAL}$ leverages gradient splitting to
perform low-dimensional updates using advanced algorithms (such as Adam), while
updates along the remaining directions are executed via state-free methods like
SGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with
various low-rank update selection techniques, including GaLore and BAdam. We
provide theoretical convergence guarantees for our framework when using SGDM
for low-dimensional updates and SGD for state-free updates. Additionally, our
method consistently outperforms concurrent approaches across various fixed
memory budgets, achieving state-of-the-art results in pre-training and
fine-tuning tasks while balancing memory efficiency and performance metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of
  Factored-POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Han, Debabrota Basu, Michael Mangan, Eleni Vasilaki, Aditya Gilra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations of underlying environmental dynamics from partial
observations is a critical challenge in machine learning. In the context of
Partially Observable Markov Decision Processes (POMDPs), state representations
are often inferred from the history of past observations and actions. We
demonstrate that incorporating future information is essential to accurately
capture causal dynamics and enhance state representations. To address this, we
introduce a Dynamical Variational Auto-Encoder (DVAE) designed to learn causal
Markovian dynamics from offline trajectories in a POMDP. Our method employs an
extended hindsight framework that integrates past, current, and multi-step
future information within a factored-POMDP setting. Empirical results reveal
that this approach uncovers the causal graph governing hidden state transitions
more effectively than history-based and typical hindsight-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suite-IN: Aggregating Motion Features from Apple Suite for Robust
  Inertial Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Sun, Songpengcheng Xia, Junyuan Deng, Jiarui Yang, Zengyuan Lai, Qi Wu, Ling Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of wearable technology, devices like smartphones,
smartwatches, and headphones equipped with IMUs have become essential for
applications such as pedestrian positioning. However, traditional pedestrian
dead reckoning (PDR) methods struggle with diverse motion patterns, while
recent data-driven approaches, though improving accuracy, often lack robustness
due to reliance on a single device.In our work, we attempt to enhance the
positioning performance using the low-cost commodity IMUs embedded in the
wearable devices. We propose a multi-device deep learning framework named
Suite-IN, aggregating motion data from Apple Suite for inertial navigation.
Motion data captured by sensors on different body parts contains both local and
global motion information, making it essential to reduce the negative effects
of localized movements and extract global motion representations from multiple
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Federated Finetuning of Tiny <span class="highlight-title">Transformer</span>s with
  Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Pfeiffer, Mohamed Aboelenien Ahmed, Ramin Khalili, Jörg Henkel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) through Transformer structures
have dominated many machine learning tasks, especially text processing.
However, these models require massive amounts of data for training and induce
high resource requirements, particularly in terms of the large number of
Floating Point Operations (FLOPs) and the high amounts of memory needed. To
fine-tune such a model in a parameter-efficient way, techniques like Adapter or
LoRA have been developed. However, we observe that the application of LoRA,
when used in federated learning (FL), while still being parameter-efficient, is
memory and FLOP inefficient. Based on that observation, we develop a novel
layer finetuning scheme that allows devices in cross-device FL to make use of
pretrained neural networks (NNs) while adhering to given resource constraints.
We show that our presented scheme outperforms the current state of the art when
dealing with homogeneous or heterogeneous computation and memory constraints
and is on par with LoRA regarding limited communication, thereby achieving
significantly higher accuracies in FL training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Criterion Model Aggregation in Federated Learning: Balancing Data
  Quantity and Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhou Zhang, Xianjia Yu, Tomi Westerlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has become one of the key methods for
privacy-preserving collaborative learning, as it enables the transfer of models
without requiring local data exchange. Within the FL framework, an aggregation
algorithm is recognized as one of the most crucial components for ensuring the
efficacy and security of the system. Existing average aggregation algorithms
typically assume that all client-trained data holds equal value or that weights
are based solely on the quantity of data contributed by each client. In
contrast, alternative approaches involve training the model locally after
aggregation to enhance adaptability. However, these approaches fundamentally
ignore the inherent heterogeneity between different clients' data and the
complexity of variations in data at the aggregation stage, which may lead to a
suboptimal global model.
  To address these issues, this study proposes a novel dual-criterion weighted
aggregation algorithm involving the quantity and quality of data from the
client node. Specifically, we quantify the data used for training and perform
multiple rounds of local model inference accuracy evaluation on a specialized
dataset to assess the data quality of each client. These two factors are
utilized as weights within the aggregation process, applied through a
dynamically weighted summation of these two factors. This approach allows the
algorithm to adaptively adjust the weights, ensuring that every client can
contribute to the global model, regardless of their data's size or initial
quality. Our experiments show that the proposed algorithm outperforms several
existing state-of-the-art aggregation approaches on both a general-purpose
open-source dataset, CIFAR-10, and a dataset specific to visual obstacle
avoidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Low-Rank Adaptation with Differential Privacy over Wireless
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqu Kang, Zixin Wang, Hengtao He, Jun Zhang, Shenghui Song, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained foundation models (FMs) on distributed edge
devices presents considerable computational and privacy challenges. Federated
fine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative
model training without the need to share raw data. To lessen the computational
burden on resource-limited devices, combining low-rank adaptation (LoRA) with
federated learning enables parameter-efficient fine-tuning. Additionally, the
split FedFT architecture partitions an FM between edge devices and a central
server, reducing the necessity for complete model deployment on individual
devices. However, the risk of privacy eavesdropping attacks in FedFT remains a
concern, particularly in sensitive areas such as healthcare and finance. In
this paper, we propose a split FedFT framework with differential privacy (DP)
over wireless networks, where the inherent wireless channel noise in the uplink
transmission is utilized to achieve DP guarantees without adding an extra
artificial noise. We shall investigate the impact of the wireless noise on
convergence performance of the proposed framework. We will also show that by
updating only one of the low-rank matrices in the split FedFT with DP, the
proposed method can mitigate the noise amplification effect. Simulation results
will demonstrate that the proposed framework achieves higher accuracy under
strict privacy budgets compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, submitted to IEEE ICC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-based retrieval models for hyperspectral image data optimized
  with Kernel Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zina-Sabrina Duma, Tuomas Sihvonen, Jouni Susiluoto, Otto Lamminpää, Heikki Haario, Satu-Pia Reinikainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel-based statistical methods are efficient, but their performance depends
heavily on the selection of kernel parameters. In literature, the optimization
studies on kernel-based chemometric methods is limited and often reduced to
grid searching. Previously, the authors introduced Kernel Flows (KF) to learn
kernel parameters for Kernel Partial Least-Squares (K-PLS) regression. KF is
easy to implement and helps minimize overfitting. In cases of high collinearity
between spectra and biogeophysical quantities in spectroscopy, simpler methods
like Principal Component Regression (PCR) may be more suitable. In this study,
we propose a new KF-type approach to optimize Kernel Principal Component
Regression (K-PCR) and test it alongside KF-PLS. Both methods are benchmarked
against non-linear regression techniques using two hyperspectral remote sensing
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PatchCTG: Patch Cardiotocography <span class="highlight-title">Transformer</span> for Antepartum Fetal Health
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jaleed Khan, Manu Vatish, Gabriel Davis Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but
traditional methods like the Dawes-Redman system are often limited by high
inter-observer variability, leading to inconsistent interpretations and
potential misdiagnoses. This paper introduces PatchCTG, a transformer-based
model specifically designed for CTG analysis, employing patch-based
tokenisation, instance normalisation and channel-independent processing to
capture essential local and global temporal dependencies within CTG signals.
PatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over
20,000 CTG traces across diverse clinical outcomes after applying the inclusion
and exclusion criteria. With extensive hyperparameter optimisation, PatchCTG
achieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at
Youden's index threshold, demonstrating adaptability to various clinical needs.
Testing across varying temporal thresholds showed robust predictive
performance, particularly with finetuning on data closer to delivery, achieving
a sensitivity of 52% and specificity of 88% for near-delivery cases. These
findings suggest the potential of PatchCTG to enhance clinical decision-making
in antepartum care by providing a reliable, objective tool for fetal health
assessment. The source code is available at
https://github.com/jaleedkhan/PatchCTG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interaction Asymmetry: A General Principle for Learning Composable
  Abstractions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Brady, Julius von Kügelgen, Sébastien Lachapelle, Simon Buchholz, Thomas Kipf, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning disentangled representations of concepts and re-composing them in
unseen ways is crucial for generalizing to out-of-domain situations. However,
the underlying properties of concepts that enable such disentanglement and
compositional generalization remain poorly understood. In this work, we propose
the principle of interaction asymmetry which states: "Parts of the same concept
have more complex interactions than parts of different concepts". We formalize
this via block diagonality conditions on the $(n+1)$th order derivatives of the
generator mapping concepts to observed data, where different orders of
"complexity" correspond to different $n$. Using this formalism, we prove that
interaction asymmetry enables both disentanglement and compositional
generalization. Our results unify recent theoretical results for learning
concepts of objects, which we show are recovered as special cases with
$n\!=\!0$ or $1$. We provide results for up to $n\!=\!2$, thus extending these
prior works to more flexible generator functions, and conjecture that the same
proof strategies generalize to larger $n$. Practically, our theory suggests
that, to disentangle concepts, an autoencoder should penalize its latent
capacity and the interactions between concepts during decoding. We propose an
implementation of these criteria using a flexible Transformer-based VAE, with a
novel regularizer on the attention weights of the decoder. On synthetic image
datasets consisting of objects, we provide evidence that this model can achieve
comparable object disentanglement to existing models that use more explicit
object-centric priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likelihood as a Performance Gauge for Retrieval-Augmented Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at NAACL 2025. Code is available at
  https://github.com/lyutyuh/poptimizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text, 3 figures
  in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASER: Activation Smoothing and Error Reconstruction for Large Language
  Model Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibo Zhao, Yubin Shi, Xinyu Lyu, Wanchen Sui, Shen Li, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective low-bit quantization. The limited numerical mapping makes the
quantized model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training quantization. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
quantization error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to low-bit ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art quantization
algorithms, showing potential to activation quantization, with minor overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit
  Q-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexi Canesse, Mathieu Petitbois, Ludovic Denoyer, Sylvain Lamprier, Rémy Portelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In this work, we present a novel
hierarchical transformer-based approach leveraging a learned quantizer of the
space. This quantization enables the training of a simpler zone-conditioned
low-level policy and simplifies planning, which is reduced to discrete
autoregressive prediction. Among other benefits, zone-level reasoning in
planning enables explicit trajectory stitching rather than implicit stitching
based on noisy value function estimates. By combining this transformer-based
planner with recent advancements in offline RL, our proposed approach achieves
state-of-the-art results in complex long-distance navigation environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Code will be released upon acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially Regularized Graph Attention Autoencoder Framework for
  Detecting Rainfall Extremes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Agarwal, Progyan Das, Udit Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Graph Attention Autoencoder (GAE) with spatial
regularization to address the challenge of scalable anomaly detection in
spatiotemporal rainfall data across India from 1990 to 2015. Our model
leverages a Graph Attention Network (GAT) to capture spatial dependencies and
temporal dynamics in the data, further enhanced by a spatial regularization
term ensuring geographic coherence. We construct two graph datasets employing
rainfall, pressure, and temperature attributes from the Indian Meteorological
Department and ERA5 Reanalysis on Single Levels, respectively. Our network
operates on graph representations of the data, where nodes represent geographic
locations, and edges, inferred through event synchronization, denote
significant co-occurrences of rainfall events. Through extensive experiments,
we demonstrate that our GAE effectively identifies anomalous rainfall patterns
across the Indian landscape. Our work paves the way for sophisticated
spatiotemporal anomaly detection methodologies in climate science, contributing
to better climate change preparedness and response strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the loss landscape of regularized neural networks via convex
  duality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyoon Kim, Aaron Mishkin, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss several aspects of the loss landscape of regularized neural
networks: the structure of stationary points, connectivity of optimal
solutions, path with nonincreasing loss to arbitrary global optimum, and the
nonuniqueness of optimal solutions, by casting the problem into an equivalent
convex problem and considering its dual. Starting from two-layer neural
networks with scalar output, we first characterize the solution set of the
convex problem using its dual and further characterize all stationary points.
With the characterization, we show that the topology of the global optima goes
through a phase transition as the width of the network changes, and construct
counterexamples where the problem may have a continuum of optimal solutions.
Finally, we show that the solution set characterization and connectivity
results can be extended to different architectures, including two-layer
vector-valued neural networks and parallel three-layer neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Rate Analysis of LION 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Dong, Huan Li, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The LION (evoLved sIgn mOmeNtum) optimizer for deep neural network training
was found by Google via program search, with the simple sign update yet showing
impressive performance in training large scale networks. Although previous
studies have investigated its convergence properties, a comprehensive analysis,
especially the convergence rate, is still desirable. Recognizing that LION can
be regarded as solving a specific constrained problem, this paper focuses on
demonstrating its convergence to the Karush-Kuhn-Tucker (KKT) point at the rate
of $\cal O(\sqrt{d}K^{-1/4})$ measured by gradient $\ell_1$ norm, where $d$ is
the problem dimension and $K$ is the number of iteration steps. Step further,
we remove the constraint and establish that LION converges to the critical
point of the general unconstrained problem at the same rate. This rate not only
delivers the currently optimal dependence on the problem dimension $d$ but also
tightly matches the theoretical lower bound for nonconvex stochastic
optimization algorithms, which is typically measured using the gradient
$\ell_2$ norm, with respect to the number of iterations $K$. Through extensive
experiments, we not only demonstrate that LION achieves lower loss and higher
performance compared to standard SGD, but also empirically confirm that the
gradient $\ell_1/\ell_2$ norm ratio aligns with $\Theta(\sqrt{d})$, thus
proving that our convergence rate matches the theoretical lower bound with
respect to $d$ in the empirical sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMPERROR: A Flexible <span class="highlight-title">Generative</span> Perception Error Model for Probing
  Self-Driving Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Hanselmann, Simon Doll, Marius Cordts, Hendrik P. A. Lensch, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To handle the complexities of real-world traffic, learning planners for
self-driving from data is a promising direction. While recent approaches have
shown great progress, they typically assume a setting in which the ground-truth
world state is available as input. However, when deployed, planning needs to be
robust to the long-tail of errors incurred by a noisy perception system, which
is often neglected in evaluation. To address this, previous work has proposed
drawing adversarial samples from a perception error model (PEM) mimicking the
noise characteristics of a target object detector. However, these methods use
simple PEMs that fail to accurately capture all failure modes of detection. In
this paper, we present EMPERROR, a novel transformer-based generative PEM,
apply it to stress-test an imitation learning (IL)-based planner and show that
it imitates modern detectors more faithfully than previous work. Furthermore,
it is able to produce realistic noisy inputs that increase the planner's
collision rate by up to 85%, demonstrating its utility as a valuable tool for a
more complete evaluation of self-driving planners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lasnik.github.io/emperror/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OWLed: Outlier-weighed Layerwise Pruning for Efficient Autonomous
  Driving Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Li, Lu Yin, Xilu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) into autonomous driving
systems offers promising enhancements in environmental understanding and
decision-making. However, the substantial computational demands of deploying
LLMs locally on vehicles render this approach unfeasible for real-world
automotive applications. To address this challenge, we introduce OWLed, the
Outlier-Weighed Layerwise Pruning for Efficient Autonomous Driving Framework
that leverages outlier-weighted layerwise sparsity for model compression. Our
method assigns non-uniform sparsity ratios to different layers based on the
distribution of outlier features, significantly reducing the model size without
the need for fine-tuning. To ensure the compressed model adapts well to
autonomous driving tasks, we incorporate driving environment data into both the
calibration and pruning processes. Our empirical studies reveal that the
encoder component is more sensitive to pruning than the LLM, highlighting its
critical role in the system. Experimental results demonstrate that OWLed
outperforms existing methods in perception, action prediction, and language
understanding while substantially lowering computational requirements. These
findings underscore the potential of combining advanced pruning techniques with
LLMs to develop efficient and robust autonomous driving systems capable of
handling complex scenarios. Code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test Where Decisions Matter: Importance-driven Testing for Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Pranger, Hana Chockler, Martin Tappler, Bettina Könighofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many Deep Reinforcement Learning (RL) problems, decisions in a trained
policy vary in significance for the expected safety and performance of the
policy. Since RL policies are very complex, testing efforts should concentrate
on states in which the agent's decisions have the highest impact on the
expected outcome. In this paper, we propose a novel model-based method to
rigorously compute a ranking of state importance across the entire state space.
We then focus our testing efforts on the highest-ranked states. In this paper,
we focus on testing for safety. However, the proposed methods can be easily
adapted to test for performance. In each iteration, our testing framework
computes optimistic and pessimistic safety estimates. These estimates provide
lower and upper bounds on the expected outcomes of the policy execution across
all modeled states in the state space. Our approach divides the state space
into safe and unsafe regions upon convergence, providing clear insights into
the policy's weaknesses. Two important properties characterize our approach.
(1) Optimal Test-Case Selection: At any time in the testing process, our
approach evaluates the policy in the states that are most critical for safety.
(2) Guaranteed Safety: Our approach can provide formal verification guarantees
over the entire state space by sampling only a fraction of the policy. Any
safety properties assured by the pessimistic estimate are formally proven to
hold for the policy. We provide a detailed evaluation of our framework on
several examples, showing that our method discovers unsafe policy behavior with
low testing effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Do Learning Dynamics Reveal About Generalization in <span class="highlight-title">LLM</span> Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable capabilities of modern large language models (LLMs),
the mechanisms behind their problem-solving abilities remain elusive. In this
work, we aim to better understand how the learning dynamics of LLM finetuning
shapes downstream generalization. Our analysis focuses on reasoning tasks,
whose problem structure allows us to distinguish between memorization (the
exact replication of reasoning steps from the training data) and performance
(the correctness of the final solution). We find that a model's generalization
behavior can be effectively characterized by a training metric we call
pre-memorization train accuracy: the accuracy of model samples on training
queries before they begin to copy the exact reasoning steps from the training
set. On the dataset level, this metric is able to reliably predict test
accuracy, achieving $R^2$ of around or exceeding 0.9 across various models
(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On
a per-example level, this metric is also indicative of whether individual model
predictions are robust to perturbations in the training query. By connecting a
model's learning behavior to its generalization, pre-memorization train
accuracy can guide targeted improvements to training strategies. We focus on
data curation as an example, and show that prioritizing examples with low
pre-memorization accuracy leads to 1.5-2x improvements in data efficiency
compared to i.i.d. data scaling, and outperforms other standard data curation
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Exploitative Play with Untrusted Type Beliefs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongxin Li, Tinashe Handina, Shaolei Ren, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of the Bayesian game and learning has a rich history, with
the idea of controlling a single agent in a system composed of multiple agents
with unknown behaviors given a set of types, each specifying a possible
behavior for the other agents. The idea is to plan an agent's own actions with
respect to those types which it believes are most likely to maximize the
payoff. However, the type beliefs are often learned from past actions and
likely to be incorrect. With this perspective in mind, we consider an agent in
a game with type predictions of other components, and investigate the impact of
incorrect beliefs to the agent's payoff. In particular, we formally define a
tradeoff between risk and opportunity by comparing the payoff obtained against
the optimal payoff, which is represented by a gap caused by trusting or
distrusting the learned beliefs. Our main results characterize the tradeoff by
establishing upper and lower bounds on the Pareto front for both normal-form
and stochastic Bayesian games, with numerical results provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Structure Learning For Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Zhuofan Zhang, Ziming Wang, Xiang Li, Sitao Luan, Xiaojiang Peng, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the performance of Graph Neural Networks (GNNs), Graph Structure
Learning (GSL) has been extensively applied to reconstruct or refine original
graph structures, effectively addressing issues like heterophily,
over-squashing, and noisy structures. While GSL is generally thought to improve
GNN performance, it often leads to longer training times and more
hyperparameter tuning. Besides, the distinctions among current GSL methods
remain ambiguous from the perspective of GNN training, and there is a lack of
theoretical analysis to quantify their effectiveness. Recent studies further
suggest that, under fair comparisons with the same hyperparameter tuning, GSL
does not consistently outperform baseline GNNs. This motivates us to ask a
critical question: is GSL really useful for GNNs? To address this question,
this paper makes two key contributions. First, we propose a new GSL framework,
which includes three steps: GSL base (the representation used for GSL)
construction, new structure construction, and view fusion, to better understand
the effectiveness of GSL in GNNs. Second, after graph convolution, we analyze
the differences in mutual information (MI) between node representations derived
from the original topology and those from the newly constructed topology.
Surprisingly, our empirical observations and theoretical analysis show that no
matter which type of graph structure construction methods are used, after
feeding the same GSL bases to the newly constructed graph, there is no MI gain
compared to the original GSL bases. To fairly reassess the effectiveness of
GSL, we conduct ablation experiments and find that it is the pretrained GSL
bases that enhance GNN performance, and in most cases, GSL cannot improve GNN
performance. This finding encourages us to rethink the essential components in
GNNs, such as self-training and structural encoding, in GNN design rather than
GSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Graph Convolution Always Beneficial For Every Feature? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Xiang Li, Sitao Luan, Xiaojiang Peng, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated strong capabilities in
processing structured data. While traditional GNNs typically treat each feature
dimension equally during graph convolution, we raise an important question: Is
the graph convolution operation equally beneficial for each feature? If not,
the convolution operation on certain feature dimensions can possibly lead to
harmful effects, even worse than the convolution-free models. In prior studies,
to assess the impacts of graph convolution on features, people proposed metrics
based on feature homophily to measure feature consistency with the graph
topology. However, these metrics have shown unsatisfactory alignment with GNN
performance and have not been effectively employed to guide feature selection
in GNNs. To address these limitations, we introduce a novel metric, Topological
Feature Informativeness (TFI), to distinguish between GNN-favored and
GNN-disfavored features, where its effectiveness is validated through both
theoretical analysis and empirical observations. Based on TFI, we propose a
simple yet effective Graph Feature Selection (GFS) method, which processes
GNN-favored and GNN-disfavored features separately, using GNNs and non-GNN
models. Compared to original GNNs, GFS significantly improves the extraction of
useful topological information from each feature with comparable computational
costs. Extensive experiments show that after applying GFS to 8 baseline and
state-of-the-art (SOTA) GNN architectures across 10 datasets, 83.75% of the
GFS-augmented cases show significant performance boosts. Furthermore, our
proposed TFI metric outperforms other feature selection methods. These results
validate the effectiveness of both GFS and TFI. Additionally, we demonstrate
that GFS's improvements are robust to hyperparameter tuning, highlighting its
potential as a universal method for enhancing various GNN architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Audiovisual Deepfake Detection: Techniques, Challenges,
  Human Factors and Perceptual Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell
  Lung Cancer <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Sextro, Gabriel Dernbach, Kai Standvoss, Simon Schallenberg, Frederick Klauschen, Klaus-Robert Müller, Maximilian Alber, Lukas Ruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how deep learning models predict oncology patient risk can
provide critical insights into disease progression, support clinical
decision-making, and pave the way for trustworthy and data-driven precision
medicine. Building on recent advances in the spatial modeling of the tumor
microenvironment using graph neural networks, we present an explainable cell
graph (xCG) approach for survival prediction. We validate our model on a public
cohort of imaging mass cytometry (IMC) data for 416 cases of lung
adenocarcinoma. We explain survival predictions in terms of known phenotypes on
the cell level by computing risk attributions over cell graphs, for which we
propose an efficient grid-based layer-wise relevance propagation (LRP) method.
Our ablation studies highlight the importance of incorporating the cancer stage
and model ensembling to improve the quality of risk estimates. Our xCG method,
together with the IMC data, is made publicly available to support further
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Top-$nσ$: Not All Logits Are You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxia Tang, Jianchun Liu, Hongli Xu, Liusheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) typically employ greedy decoding or
low-temperature sampling for reasoning tasks, reflecting a perceived trade-off
between diversity and accuracy. We challenge this convention by introducing
top-$n\sigma$, a novel sampling method that operates directly on pre-softmax
logits by leveraging a statistical threshold. Our key insight is that logits
naturally separate into a Gaussian-distributed noisy region and a distinct
informative region, enabling efficient token filtering without complex
probability manipulations. Unlike existing methods (e.g., top-$p$, min-$p$)
that inadvertently include more noise tokens at higher temperatures,
top-$n\sigma$ maintains a stable sampling space regardless of temperature
scaling. We also provide a theoretical analysis of top-$n\sigma$ to better
understand its behavior. The extensive experimental results across four
reasoning-focused datasets demonstrate that our method not only outperforms
existing sampling approaches but also surpasses greedy decoding, while
maintaining consistent performance even at high temperatures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Multi-<span class="highlight-title">Agent</span> Reinforcement Learning for Unrelated Parallel
  Machine Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Zampella, Urtzi Otamendi, Xabier Belaunzaran, Arkaitz Artetxe, Igor G. Olaizola, Giuseppe Longo, Basilio Sierra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scheduling problems pose significant challenges in resource, industry, and
operational management. This paper addresses the Unrelated Parallel Machine
Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent
Reinforcement Learning (MARL) approach. The study introduces the Reinforcement
Learning environment and conducts empirical analyses, comparing MARL with
Single-Agent algorithms. The experiments employ various deep neural network
policies for single- and Multi-Agent approaches. Results demonstrate the
efficacy of the Maskable extension of the Proximal Policy Optimization (PPO)
algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in
Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced
scenarios, Multi-Agent approaches reveal challenges in cooperative learning but
a scalable capacity. This research contributes insights into applying MARL
techniques to scheduling optimization, emphasizing the need for algorithmic
sophistication balanced with scalability for intelligent scheduling solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 4 tables, article submitted to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CJST: CTC Compressor based Joint Speech and Text Training for
  Decoder-Only ASR <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Junteng Jia, Leda Sari, Jay Mahadeokar, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CTC compressor can be an effective approach to integrate audio encoders to
decoder-only models, which has gained growing interest for different speech
applications. In this work, we propose a novel CTC compressor based joint
speech and text training (CJST) framework for decoder-only ASR. CJST matches
speech and text modalities from both directions by exploring a simple modality
adaptor and several features of the CTC compressor, including sequence
compression, on-the-fly forced peaky alignment and CTC class embeddings.
Experimental results on the Librispeech and TED-LIUM2 corpora show that the
proposed CJST achieves an effective text injection without the need of duration
handling, leading to the best performance for both in-domain and cross-domain
scenarios. We also provide a comprehensive study on CTC compressor, covering
various compression modes, edge case handling and behavior under both clean and
noisy data conditions, which reveals the most robust setting to use CTC
compressor for decoder-only models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegQC: a segmentation network-based framework for multi-metric
  segmentation quality control and segmentation error detection in volumetric
  medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bella Specktor-Fadida, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality control of structures segmentation in volumetric medical images is
important for identifying segmentation errors in clinical practice and for
facilitating model development. This paper introduces SegQC, a novel framework
for segmentation quality estimation and segmentation error detection. SegQC
computes an estimate measure of the quality of a segmentation in volumetric
scans and in their individual slices and identifies possible segmentation error
regions within a slice. The key components include: 1. SegQC-Net, a deep
network that inputs a scan and its segmentation mask and outputs segmentation
error probabilities for each voxel in the scan; 2. three new segmentation
quality metrics, two overlap metrics and a structure size metric, computed from
the segmentation error probabilities; 3. a new method for detecting possible
segmentation errors in scan slices computed from the segmentation error
probabilities. We introduce a new evaluation scheme to measure segmentation
error discrepancies based on an expert radiologist corrections of automatically
produced segmentations that yields smaller observer variability and is closer
to actual segmentation errors. We demonstrate SegQC on three fetal structures
in 198 fetal MRI scans: fetal brain, fetal body and the placenta. To assess the
benefits of SegQC, we compare it to the unsupervised Test Time Augmentation
(TTA)-based quality estimation. Our studies indicate that SegQC outperforms
TTA-based quality estimation in terms of Pearson correlation and MAE for fetal
body and fetal brain structures segmentation. Our segmentation error detection
method achieved recall and precision rates of 0.77 and 0.48 for fetal body, and
0.74 and 0.55 for fetal brain segmentation error detection respectively. SegQC
enhances segmentation metrics estimation for whole scans and individual slices,
as well as provides error regions detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Feedback In-Context Symbol Detection over Block-Fading Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Fan, Jing Yang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Transformers, through in-context learning (ICL), have
demonstrated exceptional capabilities to adapt to new tasks using example
prompts \textit{without model update}. Transformer-based wireless receivers,
where prompts consist of the pilot data in the form of transmitted and received
signal pairs, have shown high estimation accuracy when pilot data are abundant.
However, pilot information is often costly and limited in practice. In this
work, we propose the \underline{DE}cision \underline{F}eedback
\underline{IN}-Cont\underline{E}xt \underline{D}etection (DEFINED) solution as
a new wireless receiver design, which bypasses channel estimation and directly
performs symbol detection using the (sometimes extremely) limited pilot data.
The key innovation in DEFINED is the proposed decision feedback mechanism in
ICL, where we sequentially incorporate the detected symbols into the prompts to
improve the detections for subsequent symbols. Extensive experiments across a
broad range of wireless communication settings demonstrate that DEFINED
achieves significant performance improvements, in some cases only needing a
single pilot pair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Controllable Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming the Curse of Dimensionality in Reinforcement Learning Through
  Approximate Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenbei Lu, Laixi Shi, Zaiwei Chen, Chenye Wu, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) algorithms are known to suffer from the curse of
dimensionality, which refers to the fact that large-scale problems often lead
to exponentially high sample complexity. A common solution is to use deep
neural networks for function approximation; however, such approaches typically
lack theoretical guarantees. To provably address the curse of dimensionality,
we observe that many real-world problems exhibit task-specific model structures
that, when properly leveraged, can improve the sample efficiency of RL.
Building on this insight, we propose overcoming the curse of dimensionality by
approximately factorizing the original Markov decision processes (MDPs) into
smaller, independently evolving MDPs. This factorization enables the
development of sample-efficient RL algorithms in both model-based and
model-free settings, with the latter involving a variant of variance-reduced
Q-learning. We provide improved sample complexity guarantees for both proposed
algorithms. Notably, by leveraging model structure through the approximate
factorization of the MDP, the dependence of sample complexity on the size of
the state-action space can be exponentially reduced. Numerically, we
demonstrate the practicality of our proposed methods through experiments on
both synthetic MDP tasks and a wind farm-equipped storage control problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Tabular Data towards Better One-Class Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Ye, Zhaorui Tan, Yijie Hu, Xi Yang, Guangliang Cheng, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular anomaly detection under the one-class classification setting poses a
significant challenge, as it involves accurately conceptualizing "normal"
derived exclusively from a single category to discern anomalies from normal
data variations. Capturing the intrinsic correlation among attributes within
normal samples presents one promising method for learning the concept. To do
so, the most recent effort relies on a learnable mask strategy with a
reconstruction task. However, this wisdom may suffer from the risk of producing
uniform masks, i.e., essentially nothing is masked, leading to less effective
correlation learning. To address this issue, we presume that attributes related
to others in normal samples can be divided into two non-overlapping and
correlated subsets, defined as CorrSets, to capture the intrinsic correlation
effectively. Accordingly, we introduce an innovative method that disentangles
CorrSets from normal tabular data. To our knowledge, this is a pioneering
effort to apply the concept of disentanglement for one-class anomaly detection
on tabular data. Extensive experiments on 20 tabular datasets show that our
method substantially outperforms the state-of-the-art methods and leads to an
average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Test-Time Adaptation for Inverse Consistent
  Diffeomorphic Lung Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad F. A. Chaudhary, Stephanie M. Aguilera, Arie Nakhmani, Joseph M. Reinhardt, Surya P. Bhatt, Sandeep Bodduluri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffeomorphic deformable image registration ensures smooth invertible
transformations across inspiratory and expiratory chest CT scans. Yet, in
practice, deep learning-based diffeomorphic methods struggle to capture large
deformations between inspiratory and expiratory volumes, and therefore lack
inverse consistency. Existing methods also fail to account for model
uncertainty, which can be useful for improving performance. We propose an
uncertainty-aware test-time adaptation framework for inverse consistent
diffeomorphic lung registration. Our method uses Monte Carlo (MC) dropout to
estimate spatial uncertainty that is used to improve model performance. We
train and evaluate our method for inspiratory-to-expiratory CT registration on
a large cohort of 675 subjects from the COPDGene study, achieving a higher Dice
similarity coefficient (DSC) between the lung boundaries (0.966) compared to
both VoxelMorph (0.953) and TransMorph (0.953). Our method demonstrates
consistent improvements in the inverse registration direction as well with an
overall DSC of 0.966, higher than VoxelMorph (0.958) and TransMorph (0.956).
Paired t-tests indicate statistically significant improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for
  Black-box Multi-modal <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejin Chen, Kaishen Wang, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs)
to output harmful responses, raise significant safety concerns. Among these
methods, gradient-based approaches, which use gradients to generate malicious
prompts, have been widely studied due to their high success rates in white-box
settings, where full access to the model is available. However, these methods
have notable limitations: they require white-box access, which is not always
feasible, and involve high memory usage. To address scenarios where white-box
access is unavailable, attackers often resort to transfer attacks. In transfer
attacks, malicious inputs generated using white-box models are applied to
black-box models, but this typically results in reduced attack performance. To
overcome these challenges, we propose Zer0-Jack, a method that bypasses the
need for white-box access by leveraging zeroth-order optimization. We propose
patch coordinate descent to efficiently generate malicious image inputs to
directly attack black-box MLLMs, which significantly reduces memory usage
further. Through extensive experiments, Zer0-Jack achieves a high attack
success rate across various models, surpassing previous transfer-based methods
and performing comparably with existing white-box jailbreak techniques.
Notably, Zer0-Jack achieves a 95\% attack success rate on MiniGPT-4 with the
Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its
effectiveness. Additionally, we show that Zer0-Jack can directly attack
commercial MLLMs such as GPT-4o. Codes are provided in the supplement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips SafeGenAi Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exogenous Randomness Empowering Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxing Mei, Yingying Fan, Jinchi Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We offer theoretical and empirical insights into the impact of exogenous
randomness on the effectiveness of random forests with tree-building rules
independent of training data. We formally introduce the concept of exogenous
randomness and identify two types of commonly existing randomness: Type I from
feature subsampling, and Type II from tie-breaking in tree-building processes.
We develop non-asymptotic expansions for the mean squared error (MSE) for both
individual trees and forests and establish sufficient and necessary conditions
for their consistency. In the special example of the linear regression model
with independent features, our MSE expansions are more explicit, providing more
understanding of the random forests' mechanisms. It also allows us to derive an
upper bound on the MSE with explicit consistency rates for trees and forests.
Guided by our theoretical findings, we conduct simulations to further explore
how exogenous randomness enhances random forest performance. Our findings
unveil that feature subsampling reduces both the bias and variance of random
forests compared to individual trees, serving as an adaptive mechanism to
balance bias and variance. Furthermore, our results reveal an intriguing
phenomenon: the presence of noise features can act as a "blessing" in enhancing
the performance of random forests thanks to feature subsampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>103 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Gradient Descent Dynamics of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqing Song, Boran Han, Shuai Zhang, Jie Ding, Mingyi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the Transformer architecture has achieved remarkable success across
various domains, a thorough theoretical foundation explaining its optimization
dynamics is yet to be fully developed. In this study, we aim to bridge this
understanding gap by answering the following two core questions: (1) Which
types of Transformer architectures allow Gradient Descent (GD) to achieve
guaranteed convergence? and (2) Under what initial conditions and architectural
specifics does the Transformer achieve rapid convergence during training? By
analyzing the loss landscape of a single Transformer layer using Softmax and
Gaussian attention kernels, our work provides concrete answers to these
questions. Our findings demonstrate that, with appropriate weight
initialization, GD can train a Transformer model (with either kernel type) to
achieve a global optimal solution, especially when the input embedding
dimension is large. Nonetheless, certain scenarios highlight potential
pitfalls: training a Transformer using the Softmax attention kernel may
sometimes lead to suboptimal local solutions. In contrast, the Gaussian
attention kernel exhibits a much favorable behavior. Our empirical study
further validate the theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accident Impact Prediction based on a deep convolutional and recurrent
  neural network model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouyan Sajadi, Mahya Qorbani, Sobhan Moosavi, Erfan Hassannayebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accidents pose a significant threat to public safety, resulting in
numerous fatalities, injuries, and a substantial economic burden each year. The
development of predictive models capable of real-time forecasting of
post-accident impact using readily available data can play a crucial role in
preventing adverse outcomes and enhancing overall safety. However, existing
accident predictive models encounter two main challenges: first, reliance on
either costly or non-real-time data, and second the absence of a comprehensive
metric to measure post-accident impact accurately. To address these
limitations, this study proposes a deep neural network model known as the
cascade model. It leverages readily available real-world data from Los Angeles
County to predict post-accident impacts. The model consists of two components:
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). The LSTM
model captures temporal patterns, while the CNN extracts patterns from the
sparse accident dataset. Furthermore, an external traffic congestion dataset is
incorporated to derive a new feature called the "accident impact" factor, which
quantifies the influence of an accident on surrounding traffic flow. Extensive
experiments were conducted to demonstrate the effectiveness of the proposed
hybrid machine learning method in predicting the post-accident impact compared
to state-of-the-art baselines. The results reveal a higher precision in
predicting minimal impacts (i.e., cases with no reported accidents) and a
higher recall in predicting more significant impacts (i.e., cases with reported
accidents).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Stealing for Any Low-Rank <span class="highlight-title">Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Liu, Ankur Moitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model stealing, where a learner tries to recover an unknown model via
carefully chosen queries, is a critical problem in machine learning, as it
threatens the security of proprietary models and the privacy of data they are
trained on. In recent years, there has been particular interest in stealing
large language models (LLMs). In this paper, we aim to build a theoretical
understanding of stealing language models by studying a simple and
mathematically tractable setting. We study model stealing for Hidden Markov
Models (HMMs), and more generally low-rank language models.
  We assume that the learner works in the conditional query model, introduced
by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient
algorithm in the conditional query model, for learning any low-rank
distribution. In other words, our algorithm succeeds at stealing any language
model whose output distribution is low-rank. This improves upon the previous
result by Kakade, Krishnamurthy, Mahajan and Zhang, which also requires the
unknown distribution to have high "fidelity", a property that holds only in
restricted cases. There are two key insights behind our algorithm: First, we
represent the conditional distributions at each timestep by constructing
barycentric spanners among a collection of vectors of exponentially large
dimension. Second, for sampling from our representation, we iteratively solve a
sequence of convex optimization problems that involve projection in relative
entropy to prevent compounding of errors over the length of the sequence. This
is an interesting example where, at least theoretically, allowing a machine
learning model to solve more complex problems at inference time can lead to
drastic improvements in its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Virtual Reality Teleoperation of an Upper-body Humanoid with
  Modified Task Jacobians and Relaxed Barrier Functions for Self-Collision
  Avoidance <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Jens Jorgensen, Ravi Bhadeshiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for retartgeting off-the-shelf Virtual Reality (VR)
trackers to effectively teleoperate an upper-body humanoid while ensuring
self-collision-free motions. Key to the effectiveness was the proper assignment
of trackers to joint sets via modified task Jacobians and relaxed barrier
functions for self-collision avoidance. The approach was validated on
Apptronik's Astro hardware by demonstrating manipulation capabilities on a
table-top environment with pick-and-place box packing and a two-handed box pick
up and handover task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>XR & Robotics Workshop, IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecEncoder: Logs are All You Need in Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative and Federated Black-box Optimization: A Bayesian
  Optimization Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on collaborative and federated black-box optimization (BBOpt), where
agents optimize their heterogeneous black-box functions through collaborative
sequential experimentation. From a Bayesian optimization perspective, we
address the fundamental challenges of distributed experimentation,
heterogeneity, and privacy within BBOpt, and propose three unifying frameworks
to tackle these issues: (i) a global framework where experiments are centrally
coordinated, (ii) a local framework that allows agents to make decisions based
on minimal shared information, and (iii) a predictive framework that enhances
local surrogates through collaboration to improve decision-making. We
categorize existing methods within these frameworks and highlight key open
questions to unlock the full potential of federated BBOpt. Our overarching goal
is to shift federated learning from its predominantly descriptive/predictive
paradigm to a prescriptive one, particularly in the context of BBOpt - an
inherently sequential decision-making problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Deep Learning Approach for Real-time Lane-based Arrival Curve
  Reconstruction at Intersection using License Plate Recognition Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang He, Chengchuan An, Jiawei Lu, Yao-Jan Wu, Zhenbo Lu, Jingxin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition of real-time and accurate traffic arrival information is of
vital importance for proactive traffic control systems, especially in partially
connected vehicle environments. License plate recognition (LPR) data that
record both vehicle departures and identities are proven to be desirable in
reconstructing lane-based arrival curves in previous works. Existing LPR
databased methods are predominantly designed for reconstructing historical
arrival curves. For real-time reconstruction of multi-lane urban roads, it is
pivotal to determine the lane choice of real-time link-based arrivals, which
has not been exploited in previous studies. In this study, we propose a
Bayesian deep learning approach for real-time lane-based arrival curve
reconstruction, in which the lane choice patterns and uncertainties of
link-based arrivals are both characterized. Specifically, the learning process
is designed to effectively capture the relationship between partially observed
link-based arrivals and lane-based arrivals, which can be physically
interpreted as lane choice proportion. Moreover, the lane choice uncertainties
are characterized using Bayesian parameter inference techniques, minimizing
arrival curve reconstruction uncertainties, especially in low LPR data matching
rate conditions. Real-world experiment results conducted in multiple matching
rate scenarios demonstrate the superiority and necessity of lane choice
modeling in reconstructing arrival curves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Offline Reinforcement Learning for Non-Markovian Decision
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiquan Huang, Yingbin Liang, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust offline reinforcement learning (RL) aims to find a
policy that performs the best under the worst environment within an uncertainty
set using an offline dataset collected from a nominal model. While recent
advances in robust RL focus on Markov decision processes (MDPs), robust
non-Markovian RL is limited to planning problem where the transitions in the
uncertainty set are known. In this paper, we study the learning problem of
robust offline non-Markovian RL. Specifically, when the nominal model admits a
low-rank structure, we propose a new algorithm, featuring a novel dataset
distillation and a lower confidence bound (LCB) design for robust values under
different types of the uncertainty set. We also derive new dual forms for these
robust values in non-Markovian RL, making our algorithm more amenable to
practical implementation. By further introducing a novel type-I concentrability
coefficient tailored for offline low-rank non-Markovian decision processes, we
prove that our algorithm can find an $\epsilon$-optimal robust policy using
$O(1/\epsilon^2)$ offline samples. Moreover, we extend our algorithm to the
case when the nominal model does not have specific structure. With a new
type-II concentrability coefficient, the extended algorithm also enjoys
polynomial sample efficiency under all different types of the uncertainty set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FM-TS: Flow Matching for Time Series Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Hu, Xiao Wang, Lirong Wu, Huatian Zhang, Stan Z. Li, Sheng Wang, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series generation has emerged as an essential tool for analyzing
temporal data across numerous fields. While diffusion models have recently
gained significant attention in generating high-quality time series, they tend
to be computationally demanding and reliant on complex stochastic processes. To
address these limitations, we introduce FM-TS, a rectified Flow Matching-based
framework for Time Series generation, which simplifies the time series
generation process by directly optimizing continuous trajectories. This
approach avoids the need for iterative sampling or complex noise schedules
typically required in diffusion-based models. FM-TS is more efficient in terms
of training and inference. Moreover, FM-TS is highly adaptive, supporting both
conditional and unconditional time series generation. Notably, through our
novel inference design, the model trained in an unconditional setting can
seamlessly generalize to conditional tasks without the need for retraining.
Extensive benchmarking across both settings demonstrates that FM-TS
consistently delivers superior performance compared to existing approaches
while being more efficient in terms of training and inference. For instance, in
terms of discriminative score, FM-TS achieves 0.005, 0.019, 0.011, 0.005,
0.053, and 0.106 on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI
unconditional time series datasets, respectively, significantly outperforming
the second-best method which achieves 0.006, 0.067, 0.061, 0.008, 0.122, and
0.167 on the same datasets. We have achieved superior performance in solar
forecasting and MuJoCo imputation tasks, significantly enhanced by our
innovative $t$ power sampling method. The code is available at
https://github.com/UNITES-Lab/FMTS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search
  in Deep Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wei, Yuekui Yang, Yang Zhang, Haiyang Wu, Meixi Liu, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to
represent various categorical features. Traditional DLRMs adopt unified
embedding size for all features, leading to suboptimal performance and
redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works
focus on obtaining mixed embedding sizes with strong model performance.
However, previous AES works can hardly address several challenges together: (1)
The search results of embedding sizes are unstable; (2) Recommendation effect
with AES results is unsatisfactory; (3) Memory cost of embeddings is
uncontrollable. To address these challenges, we propose a novel one-shot AES
framework called AdaS&S, in which a supernet encompassing various candidate
embeddings is built and AES is performed as searching network architectures
within it. Our framework contains two main stages: In the first stage, we
decouple training parameters from searching embedding sizes, and propose the
Adaptive Sampling method to yield a well-trained supernet, which further helps
to produce stable AES results. In the second stage, to obtain embedding sizes
that benefits the model effect, we design a reinforcement learning search
process which utilizes the supernet trained previously. Meanwhile, to adapt
searching to specific resource constraint, we introduce the resource
competition penalty to balance the model effectiveness and memory cost of
embeddings. We conduct extensive experiments on public datasets to show the
superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving
about 20% of model parameters. Empirical analysis also shows that the stability
of searching results in AdaS&S significantly exceeds other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Automatic Real-time Motion Tracking Method for Magnetic
  Resonance Imaging-guided Radiotherapy: Leveraging the Enhanced
  Tracking-Learning-Detection Framework with Automatic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqi Chen, Zilin Wang, Jianrong Dai, Shirui Qin, Ying Cao, Ruiao Zhao, Jiayun Chen, Guohua Wu, Yuan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Ensuring the precision in motion tracking for MRI-guided
Radiotherapy (MRIgRT) is crucial for the delivery of effective treatments. This
study refined the motion tracking accuracy in MRIgRT through the innovation of
an automatic real-time tracking method, leveraging an enhanced
Tracking-Learning-Detection (ETLD) framework coupled with automatic
segmentation. Methods: We developed a novel MRIgRT motion tracking method by
integrating two primary methods: the ETLD framework and an improved Chan-Vese
model (ICV), named ETLD+ICV. The TLD framework was upgraded to suit real-time
cine MRI, including advanced image preprocessing, no-reference image quality
assessment, an enhanced median-flow tracker, and a refined detector with
dynamic search region adjustments. Additionally, ICV was combined for precise
coverage of the target volume, which refined the segmented region frame by
frame using tracking results, with key parameters optimized. Tested on 3.5D MRI
scans from 10 patients with liver metastases, our method ensures precise
tracking and accurate segmentation vital for MRIgRT. Results: An evaluation of
106,000 frames across 77 treatment fractions revealed sub-millimeter tracking
errors of less than 0.8mm, with over 99% precision and 98% recall for all
subjects, underscoring the robustness and efficacy of the ETLD. Moreover, the
ETLD+ICV yielded a dice global score of more than 82% for all subjects,
demonstrating the proposed method's extensibility and precise target volume
coverage. Conclusions: This study successfully developed an automatic real-time
motion tracking method for MRIgRT that markedly surpasses current methods. The
novel method not only delivers exceptional precision in tracking and
segmentation but also demonstrates enhanced adaptability to clinical demands,
positioning it as an indispensable asset in the quest to augment the efficacy
of radiotherapy treatments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAUREL: Learned Augmented Residual Layer <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Menghani, Ravi Kumar, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Efficient Systems for Foundation Models Workshop
  at the International Conference on Machine Learning (ICML) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMM for Structured Fractional Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganzhao Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a class of structured fractional minimization problems, where the
numerator includes a differentiable function, a simple nonconvex nonsmooth
function, a concave nonsmooth function, and a convex nonsmooth function
composed with a linear operator, while the denominator is a continuous function
that is either weakly convex or has a weakly convex square root. These problems
are widespread and span numerous essential applications in machine learning and
data science. Existing methods are mainly based on subgradient methods and
smoothing proximal gradient methods, which may suffer from slow convergence and
numerical stability issues. In this paper, we introduce {\sf FADMM}, the first
Alternating Direction Method of Multipliers tailored for this class of
problems. {\sf FADMM} decouples the original problem into linearized proximal
subproblems, featuring two variants: one using Dinkelbach's parametric method
({\sf FADMM-D}) and the other using the quadratic transform method ({\sf
FADMM-Q}). By introducing a novel Lyapunov function, we establish that {\sf
FADMM} converges to $\epsilon$-approximate critical points of the problem
within an oracle complexity of $\mathcal{O}(1/\epsilon^{3})$. Our experiments
on synthetic and real-world data for sparse Fisher discriminant analysis,
robust Sharpe ratio minimization, and robust sparse recovery demonstrate the
effectiveness of our approach.
  Keywords: Fractional Minimization, Nonconvex Optimization, Proximal
Linearized ADMM, Nonsmooth Optimization, Convergence Analysis
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Knowledge <span class="highlight-title">Distill</span>ation Using Partial Information
  Decomposition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Qiuyi Zhang, Sanghamitra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation provides an effective method for deploying complex
machine learning models in resource-constrained environments. It typically
involves training a smaller student model to emulate either the probabilistic
outputs or the internal feature representations of a larger teacher model. By
doing so, the student model often achieves substantially better performance on
a downstream task compared to when it is trained independently. Nevertheless,
the teacher's internal representations can also encode noise or additional
information that may not be relevant to the downstream task. This observation
motivates our primary question: What are the information-theoretic limits of
knowledge transfer? To this end, we leverage a body of work in information
theory called Partial Information Decomposition (PID) to quantify the
distillable and distilled knowledge of a teacher's representation corresponding
to a given student and a downstream task. Moreover, we demonstrate that this
metric can be practically used in distillation to address challenges caused by
the complexity gap between the teacher and the student representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Machine Learning and Compression Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Link Prediction with Fuzzy Graph Attention Networks and
  Dynamic Negative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval Augmented Time Series <span class="highlight-title">Forecast</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Tire, Ege Onur Taga, Muhammed Emrullah Ildiz, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a central component of modern LLM
systems, particularly in scenarios where up-to-date information is crucial for
accurately responding to user queries or when queries exceed the scope of the
training data. The advent of time-series foundation models (TSFM), such as
Chronos, and the need for effective zero-shot forecasting performance across
various time-series domains motivates the question: Do benefits of RAG
similarly carry over to time series forecasting? In this paper, we advocate
that the dynamic and event-driven nature of time-series data makes RAG a
crucial component of TSFMs and introduce a principled RAG framework for
time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within
RAF, we develop efficient strategies for retrieving related time-series
examples and incorporating them into forecast. Through experiments and
mechanistic studies, we demonstrate that RAF indeed improves the forecasting
accuracy across diverse time series domains and the improvement is more
significant for larger TSFM sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyao Li, Mingze Ni, Yongshun Gong, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning underpins most of the currently advanced natural language
processing (NLP) tasks such as textual classification, neural machine
translation (NMT), abstractive summarization and question-answering (QA).
However, the robustness of the models, particularly QA models, against
adversarial attacks is a critical concern that remains insufficiently explored.
This paper introduces QA-Attack (Question Answering Attack), a novel word-level
adversarial strategy that fools QA models. Our attention-based attack exploits
the customized attention mechanism and deletion ranking strategy to identify
and target specific words within contextual passages. It creates deceptive
inputs by carefully choosing and substituting synonyms, preserving grammatical
integrity while misleading the model to produce incorrect responses. Our
approach demonstrates versatility across various question types, particularly
when dealing with extensive long textual inputs. Extensive experiments on
multiple benchmark datasets demonstrate that QA-Attack successfully deceives
baseline QA models and surpasses existing adversarial techniques regarding
success rate, semantics changes, BLEU score, fluency and grammar error rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVCiM-PT: An NVCiM-assisted <span class="highlight-title">Prompt</span> Tuning Framework for Edge <span class="highlight-title">LLM</span>s <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Qin, Pengyu Ren, Zheyu Yan, Liu Liu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong, Kai Ni, Sharon Hu, Yiyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) deployed on edge devices, known as edge LLMs,
need to continuously fine-tune their model parameters from user-generated data
under limited resource constraints. However, most existing learning methods are
not applicable for edge LLMs because of their reliance on high resources and
low learning capacity. Prompt tuning (PT) has recently emerged as an effective
fine-tuning method for edge LLMs by only modifying a small portion of LLM
parameters, but it suffers from user domain shifts, resulting in repetitive
training and losing resource efficiency. Conventional techniques to address
domain shift issues often involve complex neural networks and sophisticated
training, which are incompatible for PT for edge LLMs. Therefore, an open
research question is how to address domain shift issues for edge LLMs with
limited resources. In this paper, we propose a prompt tuning framework for edge
LLMs, exploiting the benefits offered by non-volatile computing-in-memory
(NVCiM) architectures. We introduce a novel NVCiM-assisted PT framework, where
we narrow down the core operations to matrix-matrix multiplication, which can
then be accelerated by performing in-situ computation on NVCiM. To the best of
our knowledge, this is the first work employing NVCiM to improve the edge LLM
PT performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Social Outcomes and Priorities centered (SOP) Framework for AI policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid developments in AI and its adoption across various domains have
necessitated a need to build robust guardrails and risk containment plans while
ensuring equitable benefits for the betterment of society. The current
technology-centered approach has resulted in a fragmented, reactive, and
ineffective policy apparatus. This paper highlights the immediate and urgent
need to pivot to a society-centered approach to develop comprehensive,
coherent, forward-looking AI policy. To this end, we present a Social Outcomes
and Priorities centered (SOP) framework for AI policy along with proposals on
implementation of its various components. While the SOP framework is presented
from a US-centric view, the takeaways are general and applicable globally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitation Learning from Observations: An Autoregressive Mixture of
  Experts Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzi Wang, Flavia Sofia Acerbo, Tong Duy Son, Panagiotis Patrinos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to imitation learning from observations,
where an autoregressive mixture of experts model is deployed to fit the
underlying policy. The parameters of the model are learned via a two-stage
framework. By leveraging the existing dynamics knowledge, the first stage of
the framework estimates the control input sequences and hence reduces the
problem complexity. At the second stage, the policy is learned by solving a
regularized maximum-likelihood estimation problem using the estimated control
input sequences. We further extend the learning procedure by incorporating a
Lyapunov stability constraint to ensure asymptotic stability of the identified
model, for accurate multi-step predictions. The effectiveness of the proposed
framework is validated using two autonomous driving datasets collected from
human demonstrations, demonstrating its practical applicability in modelling
complex nonlinear dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQC-Based Reinforcement Learning with Data Re-uploading: Performance and
  Trainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Coelho, André Sequeira, Luís Paulo Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) consists of designing agents that make
intelligent decisions without human supervision. When used alongside function
approximators such as Neural Networks (NNs), RL is capable of solving extremely
complex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, achieved
super-human performance in some specific tasks. Nonetheless, it is also
possible to use Variational Quantum Circuits (VQCs) as function approximators
in RL algorithms. This work empirically studies the performance and
trainability of such VQC-based Deep Q-Learning models in classic control
benchmark environments. More specifically, we research how data re-uploading
affects both these metrics. We show that the magnitude and the variance of the
gradients of these models remain substantial throughout training due to the
moving targets of Deep Q-Learning. Moreover, we empirically show that
increasing the number of qubits does not lead to an exponential vanishing
behavior of the magnitude and variance of the gradients for a PQC approximating
a 2-design, unlike what was expected due to the Barren Plateau Phenomenon. This
hints at the possibility of VQCs being specially adequate for being used as
function approximators in such a context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Utilization of Unique Node Identifiers in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Bechler-Speicher, Moshe Eliasof, Carola-Bibiane Schönlieb, Ran Gilad-Bachrach, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Models for the Electric Power Grid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik F. Hamann, Thomas Brunschwiler, Blazhe Gjorgiev, Leonardo S. A. Martins, Alban Puech, Anna Varbella, Jonas Weiss, Juan Bernabe-Moreno, Alexandre Blondin Massé, Seong Choi, Ian Foster, Bri-Mathias Hodge, Rishabh Jain, Kibaek Kim, Vincent Mai, François Mirallès, Martin De Montigny, Octavio Ramos-Leaños, Hussein Suprême, Le Xie, El-Nasser S. Youssef, Arnaud Zinflou, Alexander J. Belyi, Ricardo J. Bessa, Bishnu Prasad Bhattarai, Johannes Schmude, Stanislav Sobolevsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) currently dominate news headlines. They employ
advanced deep learning architectures to extract structural information
autonomously from vast datasets through self-supervision. The resulting rich
representations of complex systems and dynamics can be applied to many
downstream applications. Therefore, FMs can find uses in electric power grids,
challenged by the energy transition and climate change. In this paper, we call
for the development of, and state why we believe in, the potential of FMs for
electric grids. We highlight their strengths and weaknesses amidst the
challenges of a changing grid. We argue that an FM learning from diverse grid
data and topologies could unlock transformative capabilities, pioneering a new
approach in leveraging AI to redefine how we manage complexity and uncertainty
in the electric grid. Finally, we discuss a power grid FM concept, namely
GridFM, based on graph neural networks and show how different downstream tasks
benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major equal contributors: H.F.H., T.B., B.G., L.S.A.M., A.P., A.V.,
  J.W.; Significant equal contributors: J.B., A.B.M., S.C., I.F., B.H., R.J.,
  K.K., V.M., F.M., M.D.M., O.R., H.S., L.X., E.S.Y., A.Z.; Other equal
  contributors: A.J.B., R.J.B., B.P.B., J.S., S.S; Lead contact: H.F.H</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LE-PDE++: Mamba for accelerating PDEs Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoming Liang, Zhaoyang Mu, Qi liu, Ruipeng Li, Mingming Ge, Dixia Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Differential Equations are foundational in modeling science and
natural systems such as fluid dynamics and weather forecasting. The Latent
Evolution of PDEs method is designed to address the computational intensity of
classical and deep learning-based PDE solvers by proposing a scalable and
efficient alternative. To enhance the efficiency and accuracy of LE-PDE, we
incorporate the Mamba model, an advanced machine learning model known for its
predictive efficiency and robustness in handling complex dynamic systems with a
progressive learning strategy. The LE-PDE was tested on several benchmark
problems. The method demonstrated a marked reduction in computational time
compared to traditional solvers and standalone deep learning models while
maintaining high accuracy in predicting system behavior over time. Our method
doubles the inference speed compared to the LE-PDE while retaining the same
level of parameter efficiency, making it well-suited for scenarios requiring
long-term predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and
  Tabnet with SMOTEENN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yixin Jin, Qianwen Xing, Ye Zhang, Shaobo Guo, Shuchen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bank credit risk is a significant challenge in modern financial transactions,
and the ability to identify qualified credit card holders among a large number
of applicants is crucial for the profitability of a bank'sbank's credit card
business. In the past, screening applicants'applicants' conditions often
required a significant amount of manual labor, which was time-consuming and
labor-intensive. Although the accuracy and reliability of previously used ML
models have been continuously improving, the pursuit of more reliable and
powerful AI intelligent models is undoubtedly the unremitting pursuit by major
banks in the financial industry. In this study, we used a dataset of over
40,000 records provided by a commercial bank as the research object. We
compared various dimensionality reduction techniques such as PCA and T-SNE for
preprocessing high-dimensional datasets and performed in-depth adaptation and
tuning of distributed models such as LightGBM and XGBoost, as well as deep
models like Tabnet. After a series of research and processing, we obtained
excellent research results by combining SMOTEENN with these techniques. The
experiments demonstrated that LightGBM combined with PCA and SMOTEENN
techniques can assist banks in accurately predicting potential high-quality
customers, showing relatively outstanding performance compared to other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pagess on IEEE ICPICS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced Payment Security System:XGBoost, LightGBM and SMOTE Integrated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zheng, Chang Yu, Jin Cao, Yongshun Xu, Qianwen Xing, Yinxin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of various online and mobile payment systems, transaction fraud
has become a significant threat to financial security. This study explores the
application of advanced machine learning models, specifically based on XGBoost
and LightGBM, for developing a more accurate and robust Payment Security
Protection Model. To enhance data reliability, we meticulously processed the
data sources and applied SMOTE (Synthetic Minority Over-sampling Technique) to
address class imbalance and improve data representation. By selecting highly
correlated features, we aimed to strengthen the training process and boost
model performance. We conducted thorough performance evaluations of our
proposed models, comparing them against traditional methods including Random
Forest, Neural Network, and Logistic Regression. Using metrics such as
Precision, Recall, and F1 Score, we rigorously assessed their effectiveness.
Our detailed analyses and comparisons reveal that the combination of SMOTE with
XGBoost and LightGBM offers a highly efficient and powerful mechanism for
payment security protection. Moreover, the integration of XGBoost and LightGBM
in a Local Ensemble model further demonstrated outstanding performance. After
incorporating SMOTE, the new combined model achieved a significant improvement
of nearly 6\% over traditional models and around 5\% over its sub-models,
showcasing remarkable results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is received by https://ieee-metacom.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credit Card Fraud Detection Using Advanced <span class="highlight-title">Transformer</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yongshun Xu, Jin Cao, Ye Zhang, Yinxin Jin, Mengran Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of various online and mobile payment systems, credit
card fraud has emerged as a significant threat to financial security. This
study focuses on innovative applications of the latest Transformer models for
more robust and precise fraud detection. To ensure the reliability of the data,
we meticulously processed the data sources, balancing the dataset to address
the issue of data sparsity significantly. We also selected highly correlated
vectors to strengthen the training process.To guarantee the reliability and
practicality of the new Transformer model, we conducted performance comparisons
with several widely adopted models, including Support Vector Machine (SVM),
Random Forest, Neural Network, and Logistic Regression. We rigorously compared
these models using metrics such as Precision, Recall, and F1 Score. Through
these detailed analyses and comparisons, we present to the readers a highly
efficient and powerful anti-fraud mechanism with promising prospects. The
results demonstrate that the Transformer model not only excels in traditional
applications but also shows great potential in niche areas like fraud
detection, offering a substantial advancement in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper have been received by https://ieee-metacom.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Credit Score Prediction Using Ensemble Deep Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianwen Xing, Chang Yu, Sining Huang, Qi Zheng, Xingyu Mu, Mengying Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contemporary economic society, credit scores are crucial for every
participant. A robust credit evaluation system is essential for the
profitability of core businesses such as credit cards, loans, and investments
for commercial banks and the financial sector. This paper combines
high-performance models like XGBoost and LightGBM, already widely used in
modern banking systems, with the powerful TabNet model. We have developed a
potent model capable of accurately determining credit score levels by
integrating Random Forest, XGBoost, and TabNet, and through the stacking
technique in ensemble modeling. This approach surpasses the limitations of
single models and significantly advances the precise credit score prediction.
In the following sections, we will explain the techniques we used and
thoroughly validate our approach by comprehensively comparing a series of
metrics such as Precision, Recall, F1, and AUC. By integrating Random Forest,
XGBoost, and with the TabNet deep learning architecture, these models
complement each other, demonstrating exceptionally strong overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper have been accepted by sci of AI Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Levin Tree Search with Context Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Orseau, Marcus Hutter, Levi H. S. Lelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a
probability distribution over actions) and comes with a theoretical guarantee
on the number of expansions before reaching a goal node, depending on the
quality of the policy. This guarantee can be used as a loss function, which we
call the LTS loss, to optimize neural networks representing the policy
(LTS+NN). In this work we show that the neural network can be substituted with
parameterized context models originating from the online compression literature
(LTS+CM). We show that the LTS loss is convex under this new model, which
allows for using standard convex optimization tools, and obtain convergence
guarantees to the optimal parameters in an online setting for a given set of
solution trajectories -- guarantees that cannot be provided for neural
networks. The new LTS+CM algorithm compares favorably against LTS+NN on several
benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle
(STP). The difference is particularly large on STP, where LTS+NN fails to solve
most of the test instances while LTS+CM solves each test instance in a fraction
of a second. Furthermore, we show that LTS+CM is able to learn a policy that
solves the Rubik's cube in only a few hundred expansions, which considerably
improves upon previous machine learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piecewise Linearity of Min-Norm Solution Map of a Nonconvexly
  Regularized Convex Sparse Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Isao Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well known that the minimum $\ell_2$-norm solution of the convex LASSO
model, say $\mathbf{x}_{\star}$, is a continuous piecewise linear function of
the regularization parameter $\lambda$, and its signed sparsity pattern is
constant within each linear piece. The current study is an extension of this
classic result, proving that the aforementioned properties extend to the
min-norm solution map $\mathbf{x}_{\star}(\mathbf{y},\lambda)$, where
$\mathbf{y}$ is the observed signal, for a generalization of LASSO termed the
scaled generalized minimax concave (sGMC) model. The sGMC model adopts a
nonconvex debiased variant of the $\ell_1$-norm as sparse regularizer, but its
objective function is overall-convex. Based on the geometric properties of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we propose an extension of the least
angle regression (LARS) algorithm, which iteratively computes the closed-form
expression of $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ in each linear zone.
Under suitable conditions, the proposed algorithm provably obtains the whole
solution map $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ within finite iterations.
Notably, our proof techniques for establishing continuity and piecewise
linearity of $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ are novel, and they lead
to two side contributions: (a) our proofs establish continuity of the sGMC
solution set as a set-valued mapping of $(\mathbf{y},\lambda)$; (b) to prove
piecewise linearity and piecewise constant sparsity pattern of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we do not require any assumption that
previous work relies on (whereas to prove some additional properties of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we use a different set of assumptions
from previous work).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages. Submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RiNALMo: General-Purpose RNA <span class="highlight-title">Language Model</span>s Can Generalize Well on
  Structure Prediction Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Josip Penić, Tin Vlašić, Roland G. Huber, Yue Wan, Mile Šikić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While RNA has recently been recognized as an interesting small-molecule drug
target, many challenges remain to be addressed before we take full advantage of
it. This emphasizes the necessity to improve our understanding of its
structures and functions. Over the years, sequencing technologies have produced
an enormous amount of unlabeled RNA data, which hides a huge potential.
Motivated by the successes of protein language models, we introduce RiboNucleic
Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the
largest RNA language model to date, with 650M parameters pre-trained on 36M
non-coding RNA sequences from several databases. It can extract hidden
knowledge and capture the underlying structure information implicitly embedded
within the RNA sequences. RiNALMo achieves state-of-the-art results on several
downstream tasks. Notably, we show that its generalization capabilities
overcome the inability of other deep learning methods for secondary structure
prediction to generalize on unseen RNA families.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Compositional Generalization for Object-Centric Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations that generalize to novel compositions of known
concepts is crucial for bridging the gap between human and machine perception.
One prominent effort is learning object-centric representations, which are
widely conjectured to enable compositional generalization. Yet, it remains
unclear when this conjecture will be true, as a principled theoretical or
empirical understanding of compositional generalization is lacking. In this
work, we investigate when compositional generalization is guaranteed for
object-centric representations through the lens of identifiability theory. We
show that autoencoders that satisfy structural assumptions on the decoder and
enforce encoder-decoder consistency will learn object-centric representations
that provably generalize compositionally. We validate our theoretical result
and highlight the practical relevance of our assumptions through experiments on
synthetic image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral at ICLR 2024. The first four authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional and Deep Learning based techniques for Time Series Ordinal
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10084v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10084v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Ayllón-Gavilán, David Guijo-Rubio, Pedro Antonio Gutiérrez, Anthony Bagnall, César Hervás-Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Classification (TSC) covers the supervised learning problem where
input data is provided in the form of series of values observed through
repeated measurements over time, and whose objective is to predict the category
to which they belong. When the class values are ordinal, classifiers that take
this into account can perform better than nominal classifiers. Time Series
Ordinal Classification (TSOC) is the field covering this gap, yet unexplored in
the literature. There are a wide range of time series problems showing an
ordered label structure, and TSC techniques that ignore the order relationship
discard useful information. Hence, this paper presents a first benchmarking of
TSOC methodologies, exploiting the ordering of the target labels to boost the
performance of current TSC state-of-the-art. Both convolutional- and deep
learning-based methodologies (among the best performing alternatives for
nominal TSC) are adapted for TSOC. For the experiments, a selection of 29
ordinal problems from two well-known archives has been made. In this way, this
paper contributes to the establishment of the state-of-the-art in TSOC. The
results obtained by ordinal versions are found to be significantly better than
current nominal TSC techniques in terms of ordinal performance metrics,
outlining the importance of considering the ordering of the labels when dealing
with this kind of problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Basis-to-Basis Operator Learning Using Function Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Ingebrand, Adam J. Thorpe, Somdatta Goswami, Krishna Kumar, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Basis-to-Basis (B2B) operator learning, a novel approach for
learning operators on Hilbert spaces of functions based on the foundational
ideas of function encoders. We decompose the task of learning operators into
two parts: learning sets of basis functions for both the input and output
spaces and learning a potentially nonlinear mapping between the coefficients of
the basis functions. B2B operator learning circumvents many challenges of prior
works, such as requiring data to be at fixed locations, by leveraging classic
techniques such as least squares to compute the coefficients. It is especially
potent for linear operators, where we compute a mapping between bases as a
single matrix transformation with a closed-form solution. Furthermore, with
minimal modifications and using the deep theoretical connections between
function encoders and functional analysis, we derive operator learning
algorithms that are directly analogous to eigen-decomposition and singular
value decomposition. We empirically validate B2B operator learning on seven
benchmark operator learning tasks and show that it demonstrates a
two-orders-of-magnitude improvement in accuracy over existing approaches on
several benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Manifold Perspective on the Statistical Generalization of Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05225v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05225v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyang Wang, Juan Cervino, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) extend convolutional neural networks to operate
on graphs. Despite their impressive performances in various graph learning
tasks, the theoretical understanding of their generalization capability is
still lacking. Previous GNN generalization bounds ignore the underlying graph
structures, often leading to bounds that increase with the number of nodes -- a
behavior contrary to the one experienced in practice. In this paper, we take a
manifold perspective to establish the statistical generalization theory of GNNs
on graphs sampled from a manifold in the spectral domain. As demonstrated
empirically, we prove that the generalization bounds of GNNs decrease linearly
with the size of the graphs in the logarithmic scale, and increase linearly
with the spectral continuity constants of the filter functions. Notably, our
theory explains both node-level and graph-level tasks. Our result has two
implications: i) guaranteeing the generalization of GNNs to unseen data over
manifolds; ii) providing insights into the practical design of GNNs, i.e.,
restrictions on the discriminability of GNNs are necessary to obtain a better
generalization performance. We demonstrate our generalization bounds of GNNs
using synthetic and multiple real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages,25 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic planning in hierarchical active inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Priorelli, Ivilin Peev Stoianov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behaviors could be explained in terms of active
inference - either as discrete decision-making or continuous motor control -
inspiring innovative solutions in robotics and artificial intelligence. Still,
the literature lacks a comprehensive outlook on effectively planning realistic
actions in changing environments. Setting ourselves the goal of modeling
complex tasks such as tool use, we delve into the topic of dynamic planning in
active inference, keeping in mind two crucial aspects of biological behavior:
the capacity to understand and exploit affordances for object manipulation, and
to learn the hierarchical interactions between the self and the environment,
including other agents. We start from a simple unit and gradually describe more
advanced structures, comparing recently proposed design choices and providing
basic examples. This study distances itself from traditional views centered on
neural networks and reinforcement learning, and points toward a yet unexplored
direction in active inference: hybrid representations in hierarchical models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visuomotor policies for agile quadrotor flight presents significant
difficulties, primarily from inefficient policy exploration caused by
high-dimensional visual inputs and the need for precise and low-latency
control. To address these challenges, we propose a novel approach that combines
the performance of Reinforcement Learning (RL) and the sample efficiency of
Imitation Learning (IL) in the task of vision-based autonomous drone racing.
While RL provides a framework for learning high-performance controllers through
trial and error, it faces challenges with sample efficiency and computational
demands due to the high dimensionality of visual inputs. Conversely, IL
efficiently learns from visual expert demonstrations, but it remains limited by
the expert's performance and state distribution. To overcome these limitations,
our policy learning framework integrates the strengths of both approaches. Our
framework contains three phases: training a teacher policy using RL with
privileged state information, distilling it into a student policy via IL, and
adaptive fine-tuning via RL. Testing in both simulated and real-world scenarios
shows our approach can not only learn in scenarios where RL from scratch fails
but also outperforms existing IL methods in both robustness and performance,
successfully navigating a quadrotor through a race course using only visual
information. Videos of the experiments are available at
https://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8th Annual Conference on Robot Learning (CoRL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bandits with Abstention under Expert Advice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Pasteris, Alberto Rumi, Maximilian Thiessen, Shota Saito, Atsushi Miyauchi, Fabio Vitale, Mark Herbster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the classic problem of prediction with expert advice under bandit
feedback. Our model assumes that one action, corresponding to the learner's
abstention from play, has no reward or loss on every trial. We propose the CBA
algorithm, which exploits this assumption to obtain reward bounds that can
significantly improve those of the classical Exp4 algorithm. We can view our
problem as the aggregation of confidence-rated predictors when the learner has
the option of abstention from play. Importantly, we are the first to achieve
bounds on the expected cumulative reward for general confidence-rated
predictors. In the special case of specialists we achieve a novel reward bound,
significantly improving previous bounds of SpecialistExp (treating abstention
as another action). As an example application, we discuss learning unions of
balls in a finite metric space. In this contextual setting, we devise an
efficient implementation of CBA, reducing the runtime from quadratic to almost
linear in the number of contexts. Preliminary experiments show that CBA
improves over existing bandit algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistRL: An Asynchronous Distributed Reinforcement Learning Framework for
  On-Device Control <span class="highlight-title">Agent</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Appendix, 25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Can Evolve <span class="highlight-title">Continual</span>ly on Modality for X-Modal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazuo Yu, Haomiao Xiong, Lu Zhang, Haiwen Diao, Yunzhi Zhuge, Lanqing Hong, Dong Wang, Huchuan Lu, You He, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have gained significant attention
due to their impressive capabilities in multimodal understanding. However,
existing methods rely heavily on extensive modal-specific pretraining and
joint-modal tuning, leading to significant computational burdens when expanding
to new modalities. In this paper, we propose PathWeave, a flexible and scalable
framework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs
to continually EVolve on modalities for $\mathbb{X}$-modal reasoning. We
leverage the concept of Continual Learning and develop an incremental training
strategy atop pre-trained MLLMs, enabling their expansion to new modalities
using uni-modal data, without executing joint-modal pretraining. In detail, a
novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and
cross-modal adapters are seamlessly integrated to facilitate efficient modality
alignment and collaboration. Additionally, an MoE-based gating module is
applied between two types of adapters to further enhance the multimodal
interaction. To investigate the proposed method, we establish a challenging
benchmark called Continual Learning of Modality (MCL), which consists of
high-quality QA data from five distinct modalities: image, video, audio, depth
and point cloud. Extensive experiments demonstrate the effectiveness of the
proposed AnA framework on learning plasticity and memory stability during
continual learning. Furthermore, PathWeave performs comparably to
state-of-the-art MLLMs while concurrently reducing parameter training burdens
by 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTE: A <span class="highlight-title">Survey</span> and Unified Pipeline for <span class="highlight-title">Pre-train</span>ing Spatio<span class="highlight-title">temporal</span>
  Trajectory Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Lin, Zeyu Zhou, Yicheng Liu, Haochen Lv, Haomin Wen, Tianyi Li, Yushuai Li, Christian S. Jensen, Shengnan Guo, Youfang Lin, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal trajectories are sequences of timestamped locations, which
enable a variety of analyses that in turn enable important real-world
applications. It is common to map trajectories to vectors, called embeddings,
before subsequent analyses. Thus, the qualities of embeddings are very
important. Methods for pre-training embeddings, which leverage unlabeled
trajectories for training universal embeddings, have shown promising
applicability across different tasks, thus attracting considerable interest.
However, research progress on this topic faces two key challenges: a lack of a
comprehensive overview of existing methods, resulting in several related
methods not being well-recognized, and the absence of a unified pipeline,
complicating the development of new methods and the analysis of methods.
  We present UniTE, a survey and a unified pipeline for this domain. In doing
so, we present a comprehensive list of existing methods for pre-training
trajectory embeddings, which includes methods that either explicitly or
implicitly employ pre-training techniques. Further, we present a unified and
modular pipeline with publicly available underlying code, simplifying the
process of constructing and evaluating methods for pre-training trajectory
embeddings. Additionally, we contribute a selection of experimental results
using the proposed pipeline on real-world datasets. Implementation of the
pipeline is publicly available at https://github.com/Logan-Lin/UniTE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Hamiltonian, structure and trace distance learning of Gaussian
  states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Fanizza, Cambyse Rouzé, Daniel Stilck França
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we initiate the study of Hamiltonian learning for positive
temperature bosonic Gaussian states, the quantum generalization of the widely
studied problem of learning Gaussian graphical models. We obtain efficient
protocols, both in sample and computational complexity, for the task of
inferring the parameters of their underlying quadratic Hamiltonian under the
assumption of bounded temperature, squeezing, displacement and maximal degree
of the interaction graph. Our protocol only requires heterodyne measurements,
which are often experimentally feasible, and has a sample complexity that
scales logarithmically with the number of modes. Furthermore, we show that it
is possible to learn the underlying interaction graph in a similar setting and
sample complexity. Taken together, our results put the status of the quantum
Hamiltonian learning problem for continuous variable systems in a much more
advanced state when compared to spins, where state-of-the-art results are
either unavailable or quantitatively inferior to ours. In addition, we use our
techniques to obtain the first results on learning Gaussian states in trace
distance with a quadratic scaling in precision and polynomial in the number of
modes, albeit imposing certain restrictions on the Gaussian states. Our main
technical innovations are several continuity bounds for the covariance and
Hamiltonian matrix of a Gaussian state, which are of independent interest,
combined with what we call the local inversion technique. In essence, the local
inversion technique allows us to reliably infer the Hamiltonian of a Gaussian
state by only estimating in parallel submatrices of the covariance matrix whose
size scales with the desired precision, but not the number of modes. This way
we bypass the need to obtain precise global estimates of the covariance matrix,
controlling the sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 1 figure. Corrections to Lemma 4.1. Main results are
  unchanged</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUMO: Search-Based Uncertainty Estimation for Model-Based Offline
  Reinforcement Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjian Qiao, Jiafei Lyu, Kechen Jiao, Qi Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of offline reinforcement learning (RL) suffers from the
limited size and quality of static datasets. Model-based offline RL addresses
this issue by generating synthetic samples through a dynamics model to enhance
overall performance. To evaluate the reliability of the generated samples,
uncertainty estimation methods are often employed. However, model ensemble, the
most commonly used uncertainty estimation method, is not always the best
choice. In this paper, we propose a \textbf{S}earch-based \textbf{U}ncertainty
estimation method for \textbf{M}odel-based \textbf{O}ffline RL (SUMO) as an
alternative. SUMO characterizes the uncertainty of synthetic samples by
measuring their cross entropy against the in-distribution dataset samples, and
uses an efficient search-based method for implementation. In this way, SUMO can
achieve trustworthy uncertainty estimation. We integrate SUMO into several
model-based offline RL algorithms including MOPO and Adapted MOReL (AMOReL),
and provide theoretical analysis for them. Extensive experimental results on
D4RL datasets demonstrate that SUMO can provide more accurate uncertainty
estimation and boost the performance of base algorithms. These indicate that
SUMO could be a better uncertainty estimator for model-based offline RL when
used in either reward penalty or trajectory truncation. Our code is available
and will be open-source for further research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pessimistic Iterative Planning for Robust POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maris F. L. Galesloot, Marnix Suilen, Thiago D. Simão, Steven Carr, Matthijs T. J. Spaan, Ufuk Topcu, Nils Jansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust POMDPs extend classical POMDPs to handle model uncertainty.
Specifically, robust POMDPs exhibit so-called uncertainty sets on the
transition and observation models, effectively defining ranges of
probabilities. Policies for robust POMDPs must be (1) memory-based to account
for partial observability and (2) robust against model uncertainty to account
for the worst-case instances from the uncertainty sets. To compute such robust
memory-based policies, we propose the pessimistic iterative planning (PIP)
framework, which alternates between two main steps: (1) selecting a pessimistic
(non-robust) POMDP via worst-case probability instances from the uncertainty
sets; and (2) computing a finite-state controller (FSC) for this pessimistic
POMDP. We evaluate the performance of this FSC on the original robust POMDP and
use this evaluation in step (1) to select the next pessimistic POMDP. Within
PIP, we propose the rFSCNet algorithm. In each iteration, rFSCNet finds an FSC
through a recurrent neural network by using supervision policies optimized for
the pessimistic POMDP. The empirical evaluation in four benchmark environments
showcases improved robustness against several baseline methods and competitive
performance compared to a state-of-the-art robust POMDP solver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cross-Domain Benchmark for Active Learning <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorben Werner, Johannes Burchert, Maximilian Stubbemann, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) deals with identifying the most informative samples for
labeling to reduce data annotation costs for supervised learning tasks. AL
research suffers from the fact that lifts from literature generalize poorly and
that only a small number of repetitions of experiments are conducted. To
overcome these obstacles, we propose CDALBench, the first active learning
benchmark which includes tasks in computer vision, natural language processing
and tabular learning. Furthermore, by providing an efficient, greedy oracle,
CDALBench can be evaluated with 50 runs for each experiment. We show, that both
the cross-domain character and a large amount of repetitions are crucial for
sophisticated evaluation of AL research. Concretely, we show that the
superiority of specific methods varies over the different domains, making it
important to evaluate Active Learning with a cross-domain benchmark.
Additionally, we show that having a large amount of runs is crucial. With only
conducting three runs as often done in the literature, the superiority of
specific methods can strongly vary with the specific runs. This effect is so
strong, that, depending on the seed, even a well-established method's
performance can be significantly better and significantly worse than random for
the same dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 24 in the Benchmarks and Datasets Track. Updated
  version of paper "Toward Comparable Active Learning" (arXiv:2311.18356).
  "Toward Comparable Active Learning" is deprecated, please use this version.
  arXiv admin note: text overlap with arXiv:2311.18356; text overlap with
  arXiv:2301.10625 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Activation Sparsity with Dense to Dynamic-k
  Mixture-of-Experts Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04361v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04361v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Szatkowski, Bartosz Wójcik, Mikołaj Piórczyński, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models can face practical limitations due to their high
computational requirements. At the same time, such models exhibit significant
activation sparsity, which can be leveraged to reduce the inference cost by
converting parts of the network into equivalent Mixture-of-Experts (MoE)
layers. Despite the crucial role played by activation sparsity, its impact on
this process remains unexplored. We demonstrate that the efficiency of the
conversion can be significantly enhanced by a proper regularization of the
activation sparsity of the base model. Moreover, motivated by the high variance
of the number of activated neurons for different inputs, we introduce a more
effective dynamic-$k$ expert selection rule that adjusts the number of executed
experts on a per-token basis. To achieve further savings, we extend this
approach to multi-head attention projections. Finally, we develop an efficient
implementation that translates these computational savings into actual
wall-clock speedup. The proposed method, Dense to Dynamic-$k$
Mixture-of-Experts (D2DMoE), outperforms existing approaches on common NLP and
vision tasks, reducing inference cost by up to 60% without significantly
impacting performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Integrated Sensing, Communication, and Computation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzhu Wen, Yong Zhou, Xiaoyang Li, Yuanming Shi, Kaibin Huang, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The forthcoming generation of wireless technology, 6G, aims to usher in an
era of ubiquitous intelligent services, where everything is interconnected and
intelligent. This vision requires the seamless integration of three fundamental
modules: Sensing for information acquisition, communication for information
sharing, and computation for information processing and decision-making. These
modules are intricately linked, especially in complex tasks such as edge
learning and inference. However, the performance of these modules is
interdependent, creating a resource competition for time, energy, and
bandwidth. Existing techniques like integrated communication and computation
(ICC), integrated sensing and computation (ISC), and integrated sensing and
communication (ISAC) have made partial strides in addressing this challenge,
but they fall short of meeting the extreme performance requirements. To
overcome these limitations, it is essential to develop new techniques that
comprehensively integrate sensing, communication, and computation. This
integrated approach, known as Integrated Sensing, Communication, and
Computation (ISCC), offers a systematic perspective for enhancing task
performance. This paper begins with a comprehensive survey of historic and
related techniques such as ICC, ISC, and ISAC, highlighting their strengths and
limitations. It then discusses the benefits, functions, and challenges of ISCC.
Subsequently, the state-of-the-art signal designs for ISCC, along with network
resource management strategies specifically tailored for ISCC are explored.
Furthermore, this paper discusses the exciting research opportunities that lie
ahead for implementing ISCC in future advanced networks, and the unresolved
issues requiring further investigation. ISCC is expected to unlock the full
potential of intelligent connectivity, paving the way for groundbreaking
applications and services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, a series of discussions have been added.The
  benefits, functions, and challenges of ISCC are investigated using a new
  section. Moreover, the unresolved issues of ISCC have been discussed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably <span class="highlight-title">Transformer</span>s Harness Multi-Concept Word Semantics for Efficient
  In-Context Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalist Robot Learning from Internet Video: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep learning to massive, diverse internet data has yielded
remarkably general capabilities in visual and natural language understanding
and generation. However, data has remained scarce and challenging to collect in
robotics, seeing robot learning struggle to obtain similarly general
capabilities. Promising Learning from Videos (LfV) methods aim to address the
robotics data bottleneck by augmenting traditional robot data with large-scale
internet video data. This video data offers broad foundational information
regarding physical behaviour and the underlying physics of the world, and thus
can be highly informative for a generalist robot.
  In this survey, we present a thorough overview of the emerging field of LfV.
We outline fundamental concepts, including the benefits and challenges of LfV.
We provide a comprehensive review of current methods for extracting knowledge
from large-scale internet video, addressing key challenges in LfV, and boosting
downstream robot and reinforcement learning via the use of video data. The
survey concludes with a critical discussion of challenges and opportunities in
LfV. Here, we advocate for scalable foundation model approaches that can
leverage the full range of available internet video to improve the learning of
robot policies and dynamics models. We hope this survey can inform and catalyse
further LfV research, driving progress towards the development of
general-purpose robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Viti, Franz Thaler, Kathrin Lisa Kapper, Martin Urschler, Martin Holler, Elias Karabelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of cardiac magnetic resonance images (MRI) is crucial for the
analysis and assessment of cardiac function, helping to diagnose and treat
various cardiovascular diseases. Most recent techniques rely on deep learning
and usually require an extensive amount of labeled data. To overcome this
problem, few-shot learning has the capability of reducing data dependency on
labeled data. In this work, we introduce a new method that merges few-shot
learning with a U-Net architecture and Gaussian Process Emulators (GPEs),
enhancing data integration from a support set for improved performance. GPEs
are trained to learn the relation between the support images and the
corresponding masks in latent space, facilitating the segmentation of unseen
query images given only a small labeled support set at inference. We test our
model with the M&Ms-2 public dataset to assess its ability to segment the heart
in cardiac magnetic resonance imaging from different orientations, and compare
it with state-of-the-art unsupervised and few-shot methods. Our architecture
shows higher DICE coefficients compared to these methods, especially in the
more challenging setups where the size of the support set is considerably
small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Statistical Atlases and Computational Modeling of the
  Heart (STACOM) Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCMDP: Observation-Constrained Markov Decision Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Jianheng Liu, Bryan Lee, Zhihao Wu, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications, decision-making processes must balance the
costs of acquiring information with the benefits it provides. Traditional
control systems often assume full observability, an unrealistic assumption when
observations are expensive. We tackle the challenge of simultaneously learning
observation and control strategies in such cost-sensitive environments by
introducing the Observation-Constrained Markov Decision Process (OCMDP), where
the policy influences the observability of the true state. To manage the
complexity arising from the combined observation and control actions, we
develop an iterative, model-free deep reinforcement learning algorithm that
separates the sensing and control components of the policy. This decomposition
enables efficient learning in the expanded action space by focusing on when and
what to observe, as well as determining optimal control actions, without
requiring knowledge of the environment's dynamics. We validate our approach on
a simulated diagnostic task and a realistic healthcare environment using
HeartPole. Given both scenarios, the experimental results demonstrate that our
model achieves a substantial reduction in observation costs on average,
significantly outperforming baseline methods by a notable margin in efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper, 14 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on
  Supervised Regression (Preprint) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12308v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12308v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansel Gonzalez Tejeda, Helmut A. Mayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this tutorial, we present a compact and holistic discussion of Deep
Learning with a focus on Convolutional Neural Networks (CNNs) and supervised
regression. While there are numerous books and articles on the individual
topics we cover, comprehensive and detailed tutorials that address Deep
Learning from a foundational yet rigorous and accessible perspective are rare.
Most resources on CNNs are either too advanced, focusing on cutting-edge
architectures, or too narrow, addressing only specific applications like image
classification.This tutorial not only summarizes the most relevant concepts but
also provides an in-depth exploration of each, offering a complete yet agile
set of ideas. Moreover, we highlight the powerful synergy between learning
theory, statistic, and machine learning, which together underpin the Deep
Learning and CNN frameworks. We aim for this tutorial to serve as an optimal
resource for students, professors, and anyone interested in understanding the
foundations of Deep Learning. Upon acceptance we will provide an accompanying
repository under
\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}
  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine
Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the journal Machine Learning and Knowledge Extraction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLHF Workflow: From Reward Modeling to Online RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (09/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Overview</span> frequency principle/spectral bias in deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07395v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07395v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding deep learning is increasingly emergent as it penetrates more
and more into industry and science. In recent years, a research line from
Fourier analysis sheds lights on this magical "black box" by showing a
Frequency Principle (F-Principle or spectral bias) of the training behavior of
deep neural networks (DNNs) -- DNNs often fit functions from low to high
frequency during the training. The F-Principle is first demonstrated by
onedimensional synthetic data followed by the verification in high-dimensional
real datasets. A series of works subsequently enhance the validity of the
F-Principle. This low-frequency implicit bias reveals the strength of neural
network in learning low-frequency functions as well as its deficiency in
learning high-frequency functions. Such understanding inspires the design of
DNN-based algorithms in practical problems, explains experimental phenomena
emerging in various scenarios, and further advances the study of deep learning
from the frequency perspective. Although incomplete, we provide an overview of
F-Principle and propose some open problems for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mr.Steve: Instruction-Following <span class="highlight-title">Agent</span>s in Minecraft with What-Where-When
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyeong Park, Junmo Cho, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in developing general-purpose embodied AI
in environments like Minecraft through the adoption of LLM-augmented
hierarchical approaches. While these approaches, which combine high-level
planners with low-level controllers, show promise, low-level controllers
frequently become performance bottlenecks due to repeated failures. In this
paper, we argue that the primary cause of failure in many low-level controllers
is the absence of an episodic memory system. To address this, we introduce Mr.
Steve (Memory Recall Steve-1), a novel low-level controller equipped with Place
Event Memory (PEM), a form of episodic memory that captures what, where, and
when information from episodes. This directly addresses the main limitation of
the popular low-level controller, Steve-1. Unlike previous models that rely on
short-term memory, PEM organizes spatial and event-based data, enabling
efficient recall and navigation in long-horizon tasks. Additionally, we propose
an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing
agents to alternate between exploration and task-solving based on recalled
events. Our approach significantly improves task-solving and exploration
efficiency compared to existing methods. We will release our code and demos on
the project page: https://sites.google.com/view/mr-steve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalar Function Topology Divergence: Comparing Topology of 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Trofimov, Daria Voronkova, Eduard Tulchinskii, Evgeny Burnaev, Serguei Barannikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new topological tool for computer vision - Scalar Function
Topology Divergence (SFTD), which measures the dissimilarity of multi-scale
topology between sublevel sets of two functions having a common domain.
Functions can be defined on an undirected graph or Euclidean space of any
dimensionality. Most of the existing methods for comparing topology are based
on Wasserstein distance between persistence barcodes and they don't take into
account the localization of topological features. The minimization of SFTD
ensures that the corresponding topological features of scalar functions are
located in the same places. The proposed tool provides useful visualizations
depicting areas where functions have topological dissimilarities. We provide
applications of the proposed method to 3D computer vision. In particular,
experiments demonstrate that SFTD as an additional loss improves the
reconstruction of cellular 3D shapes from 2D fluorescence microscopy images,
and helps to identify topological errors in 3D segmentation. Additionally, we
show that SFTD outperforms Betti matching loss in 2D segmentation problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiCoEval: Evaluating <span class="highlight-title">LLM</span>s on License Compliance in Code Generation <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Xu, Kai Gao, Hao He, Minghui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have revolutionized code
generation, leading to widespread adoption of AI coding tools by developers.
However, LLMs can generate license-protected code without providing the
necessary license information, leading to potential intellectual property
violations during software production. This paper addresses the critical, yet
underexplored, issue of license compliance in LLM-generated code by
establishing a benchmark to evaluate the ability of LLMs to provide accurate
license information for their generated code. To establish this benchmark, we
conduct an empirical study to identify a reasonable standard for "striking
similarity" that excludes the possibility of independent creation, indicating a
copy relationship between the LLM output and certain open-source code. Based on
this standard, we propose LiCoEval, to evaluate the license compliance
capabilities of LLMs, i.e., the ability to provide accurate license or
copyright information when they generate code with striking similarity to
already existing copyrighted code. Using LiCoEval, we evaluate 14 popular LLMs,
finding that even top-performing LLMs produce a non-negligible proportion
(0.88% to 2.01%) of code strikingly similar to existing open-source
implementations. Notably, most LLMs fail to provide accurate license
information, particularly for code under copyleft licenses. These findings
underscore the urgent need to enhance LLM compliance capabilities in code
generation tasks. Our study provides a foundation for future research and
development to improve license compliance in AI-assisted software development,
contributing to both the protection of open-source software copyrights and the
mitigation of legal risks for LLM users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 47th International Conference on Software Engineering(ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Human-AI Complementarity with Prediction Sets <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni De Toni, Nastaran Okati, Suhas Thejaswi, Eleni Straitouri, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision support systems based on prediction sets have proven to be effective
at helping human experts solve classification tasks. Rather than providing
single-label predictions, these systems provide sets of label predictions
constructed using conformal prediction, namely prediction sets, and ask human
experts to predict label values from these sets. In this paper, we first show
that the prediction sets constructed using conformal prediction are, in
general, suboptimal in terms of average accuracy. Then, we show that the
problem of finding the optimal prediction sets under which the human experts
achieve the highest average accuracy is NP-hard. More strongly, unless P = NP,
we show that the problem is hard to approximate to any factor less than the
size of the label set. However, we introduce a simple and efficient greedy
algorithm that, for a large class of expert models and non-conformity scores,
is guaranteed to find prediction sets that provably offer equal or greater
performance than those constructed using conformal prediction. Further, using a
simulation study with both synthetic and real expert predictions, we
demonstrate that, in practice, our greedy algorithm finds near-optimal
prediction sets offering greater performance than conformal prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenUAS: Embeddings of Cities in Japan with Anchor Data for Cross-city
  Analysis of Area Usage Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Tamura, Kazuyuki Shoji, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We publicly release OpenUAS, a dataset of area embeddings based on urban
usage patterns, including embeddings for over 1.3 million 50-meter square
meshes covering a total area of 3,300 square kilometers. This dataset is
valuable for analyzing area functions in fields such as market analysis, urban
planning, transportation infrastructure, and infection prediction. It captures
the characteristics of each area in the city, such as office districts and
residential areas, by employing an area embedding technique that utilizes
location information typically obtained by GPS. Numerous area embedding
techniques have been proposed, and while the public release of such embedding
datasets is technically feasible, it has not been realized. One reason for this
is that previous methods could not embed areas from different cities and
periods into the same embedding space without sharing raw location data. We
address this issue by developing an anchoring method that establishes anchors
within a shared embedding space. We publicly release this anchor dataset along
with area embedding datasets from several periods in eight major Japanese
cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Functional Structured Data Generators Rooted in
  Out-of-Equilibrium Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandra Carbone, Aurélien Decelle, Lorenzo Rosset, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address the challenge of using energy-based models to
produce high-quality, label-specific data in complex structured datasets, such
as population genetics, RNA or protein sequences data. Traditional training
methods encounter difficulties due to inefficient Markov chain Monte Carlo
mixing, which affects the diversity of synthetic data and increases generation
times. To address these issues, we use a novel training algorithm that exploits
non-equilibrium effects. This approach, applied on the Restricted Boltzmann
Machine, improves the model's ability to correctly classify samples and
generate high-quality synthetic data in only a few sampling steps. The
effectiveness of this method is demonstrated by its successful application to
four different types of data: handwritten digits, mutations of human genomes
classified by continental origin, functionally characterized sequences of an
enzyme protein family, and homologous RNA sequences from specific taxonomies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Clustering on High-Dimensional Data with Stochastic Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kozyriev, Vladimir Norkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of conventional vector quantization
algorithms, particularly K-Means and its variant K-Means++, and investigates
the Stochastic Quantization (SQ) algorithm as a scalable alternative for
high-dimensional unsupervised and semi-supervised learning tasks. Traditional
clustering algorithms often suffer from inefficient memory utilization during
computation, necessitating the loading of all data samples into memory, which
becomes impractical for large-scale datasets. While variants such as Mini-Batch
K-Means partially mitigate this issue by reducing memory usage, they lack
robust theoretical convergence guarantees due to the non-convex nature of
clustering problems. In contrast, the Stochastic Quantization algorithm
provides strong theoretical convergence guarantees, making it a robust
alternative for clustering tasks. We demonstrate the computational efficiency
and rapid convergence of the algorithm on an image classification problem with
partially labeled data, comparing model accuracy across various ratios of
labeled to unlabeled data. To address the challenge of high dimensionality, we
employ a Triplet Network to encode images into low-dimensional representations
in a latent space, which serve as a basis for comparing the efficiency of both
the Stochastic Quantization algorithm and traditional quantization algorithms.
Furthermore, we enhance the algorithm's convergence speed by introducing
modifications with an adaptive learning rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, to be published in the International Scientific
  Technical Journal "Problems of Control and Informatics"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Jiahao Xu, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-independent cosmological inference post DESI DR1 BAO measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purba Mukherjee, Anjan Ananda Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we implement Gaussian process regression to reconstruct the
expansion history of the universe in a model-agnostic manner, using the
Pantheon-Plus SN-Ia compilation in combination with two different BAO
measurements (SDSS-IV and DESI DR1). In both the reconstructions, the
$\Lambda$CDM model is always included in the 95\% confidence intervals. We find
evidence that the DESI LRG data at $z_{\text{eff}} = 0.51$ is not an outlier
within our model-independent framework. We study the $\mathcal{O}m$-diagnostics
and the evolution of the total equation of state (EoS) of our universe, which
hint towards the possibility of a quintessence-like dark energy scenario with a
very slowly varying EoS, and a phantom-crossing in higher $z$. The entire
exercise is later complemented by considering two more SN-Ia compilations -
DES-5YR and Union3 - in combination with DESI BAO. Reconstruction with the DESI
BAO + DES-5YR SN data sets predicts that the $\Lambda$CDM model lies outside
the 3$\sigma$ confidence levels, whereas with DESI BAO + Union3 data, the
$\Lambda$CDM model is always included within 1$\sigma$. We also report
constraints on $H_0 r_d$ from our model-agnostic analysis, independent of the
pre-recombination physics. Our results point towards an $\approx$ 2$\sigma$
discrepancy between the DESI + Pantheon-Plus and DESI + DES-5YR data sets,
which calls for further investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 sets of figures. Accepted for publication in PRD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule
  Lists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Ly, Julien Ferry, Marie-José Huguet, Sébastien Gambs, Ulrich Aivodji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially-private (DP) mechanisms can be embedded into the design of a
machine learning algorithm to protect the resulting model against privacy
leakage. However, this often comes with a significant loss of accuracy due to
the noise added to enforce DP. In this paper, we aim at improving this
trade-off for a popular class of machine learning algorithms leveraging the
Gini impurity as an information gain criterion to greedily build interpretable
models such as decision trees or rule lists. To this end, we establish the
smooth sensitivity of the Gini impurity, which can be used to obtain thorough
DP guarantees while adding noise scaled with tighter magnitude. We illustrate
the applicability of this mechanism by integrating it within a greedy algorithm
producing rule list models, motivated by the fact that such models remain
understudied in the DP literature. Our theoretical analysis and experimental
results confirm that the DP rule lists models integrating smooth sensitivity
have higher accuracy that those using other DP frameworks based on global
sensitivity, for identical privacy budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARTAN: A Sparse <span class="highlight-title">Transformer</span> Learning Local Causation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anson Lei, Bernhard Schölkopf, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal structures play a central role in world models that flexibly adapt to
changes in the environment. While recent works motivate the benefits of
discovering local causal graphs for dynamics modelling, in this work we
demonstrate that accurately capturing these relationships in complex settings
remains challenging for the current state-of-the-art. To remedy this
shortcoming, we postulate that sparsity is a critical ingredient for the
discovery of such local causal structures. To this end we present the SPARse
TrANsformer World model (SPARTAN), a Transformer-based world model that learns
local causal structures between entities in a scene. By applying sparsity
regularisation on the attention pattern between object-factored tokens, SPARTAN
identifies sparse local causal models that accurately predict future object
states. Furthermore, we extend our model to capture sparse interventions with
unknown targets on the dynamics of the environment. This results in a highly
interpretable world model that can efficiently adapt to changes. Empirically,
we evaluate SPARTAN against the current state-of-the-art in object-centric
world models on observation-based environments and demonstrate that our model
can learn accurate local causal graphs and achieve significantly improved
few-shot adaptation to changes in the dynamics of the environment as well as
robustness against removing irrelevant distractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot NAS via the Suppression of Local Entropy Decrease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Wu, Han Huang, Yueting Xu, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architecture performance evaluation is the most time-consuming part of neural
architecture search (NAS). Zero-Shot NAS accelerates the evaluation by
utilizing zero-cost proxies instead of training. Though effective, existing
zero-cost proxies require invoking backpropagations or running networks on
input data, making it difficult to further accelerate the computation of
proxies. To alleviate this issue, architecture topologies are used to evaluate
the performance of networks in this study. We prove that particular
architectural topologies decrease the local entropy of feature maps, which
degrades specific features to a bias, thereby reducing network performance.
Based on this proof, architectural topologies are utilized to quantify the
suppression of local entropy decrease (SED) as a data-free and running-free
proxy. Experimental results show that SED outperforms most state-of-the-art
proxies in terms of architecture selection on five benchmarks, with computation
time reduced by three orders of magnitude. We further compare the SED-based NAS
with state-of-the-art proxies. SED-based NAS selects the architecture with
higher accuracy and fewer parameters in only one second. The theoretical
analyses of local entropy and experimental results demonstrate that the
suppression of local entropy decrease facilitates selecting optimal
architectures in Zero-Shot NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures. Corrected typos and latex template</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CALoR: Towards Comprehensive Model Inversion Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, Sijin Yu, Bin Wang, Shu-Tao Xia, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training
data from the knowledge encoded in the released machine learning models. Recent
advances in the MIA field have significantly enhanced the attack performance
under multiple scenarios, posing serious privacy risks of Deep Neural Networks
(DNNs). However, the development of defense strategies against MIAs is
relatively backward to resist the latest MIAs and existing defenses fail to
achieve further trade-off between model utility and model robustness. In this
paper, we provide an in-depth analysis from the perspective of intrinsic
vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in
the basic pipeline, which are partially investigated in the previous defenses.
Building upon these new insights, we propose a robust defense mechanism,
integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method
includes a novel robustness-enhanced classification loss specially-designed for
model inversion defenses and reveals the extraordinary effectiveness of
compressing the classification header. With CALoR, we can mislead the
optimization objective, reduce the leaked information and impede the
backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive
experimental results demonstrate that our method achieves state-of-the-art
(SOTA) defense performance against MIAs and exhibits superior generalization to
existing defenses across various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Super-resolution of Cosmological Simulations with Denoising
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Schanz, Florian List, Oliver Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning models have been successfully employed for
augmenting low-resolution cosmological simulations with small-scale
information, a task known as "super-resolution". So far, these cosmological
super-resolution models have relied on generative adversarial networks (GANs),
which can achieve highly realistic results, but suffer from various
shortcomings (e.g. low sample diversity). We introduce denoising diffusion
models as a powerful generative model for super-resolving cosmic large-scale
structure predictions (as a first proof-of-concept in two dimensions). To
obtain accurate results down to small scales, we develop a new "filter-boosted"
training approach that redistributes the importance of different scales in the
pixel-wise training objective. We demonstrate that our model not only produces
convincing super-resolution images and power spectra consistent at the percent
level, but is also able to reproduce the diversity of small-scale features
consistent with a given low-resolution simulation. This enables uncertainty
quantification for the generated small-scale features, which is critical for
the usefulness of such super-resolution models as a viable surrogate model for
cosmic structure formation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, to be submitted to OJA, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Iterative Reinforcement Learning from Human Feedback with General
  Preference Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong, Nan Jiang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate Reinforcement Learning from Human Feedback (RLHF) in the
context of a general preference oracle. In particular, we do not assume the
existence of a reward function and an oracle preference signal drawn from the
Bradley-Terry model as most of the prior works do. We consider a standard
mathematical formulation, the reverse-KL regularized minimax game between two
LLMs for RLHF under general preference oracle. The learning objective of this
formulation is to find a policy so that it is consistently preferred by the
KL-regularized preference oracle over any competing LLMs. We show that this
framework is strictly more general than the reward-based one, and propose
sample-efficient algorithms for both the offline learning from a pre-collected
preference dataset and online learning where we can query the preference oracle
along the way of training. Empirical studies verify the effectiveness of the
proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLHF, Preference Learning, Alignment for LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKVQ: Sliding-window Key and Value Cache Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can now handle longer sequences of tokens,
enabling complex tasks like book understanding and generating lengthy novels.
However, the key-value (KV) cache required for LLMs consumes substantial memory
as context length increasing, becoming the bottleneck for deployment. In this
paper, we present a strategy called SKVQ, which stands for sliding-window KV
cache quantization, to address the issue of extremely low bitwidth KV cache
quantization. To achieve this, SKVQ rearranges the channels of the KV cache in
order to improve the similarity of channels in quantization groups, and applies
clipped dynamic quantization at the group level. Additionally, SKVQ ensures
that the most recent window tokens in the KV cache are preserved with high
precision. This helps maintain the accuracy of a small but important portion of
the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.
Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization
approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit
values with minimal loss of accuracy. With SKVQ, it is possible to process
context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7
times faster decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced <span class="highlight-title">LLM</span>
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing Zhao, Andi Chen, Panfeng Li, Shiyao Zhang, Qifu Yin, Yuanqing Wang, Yuxi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Add-it: Training-Free Object Insertion in Images With <span class="highlight-title">Pretrain</span>ed
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adding Object into images based on text instructions is a challenging task in
semantic image editing, requiring a balance between preserving the original
scene and seamlessly integrating the new object in a fitting location. Despite
extensive efforts, existing models often struggle with this balance,
particularly with finding a natural location for adding an object in complex
scenes. We introduce Add-it, a training-free approach that extends diffusion
models' attention mechanisms to incorporate information from three key sources:
the scene image, the text prompt, and the generated image itself. Our weighted
extended-attention mechanism maintains structural consistency and fine details
while ensuring natural object placement. Without task-specific fine-tuning,
Add-it achieves state-of-the-art results on both real and generated image
insertion benchmarks, including our newly constructed "Additing Affordance
Benchmark" for evaluating object placement plausibility, outperforming
supervised methods. Human evaluations show that Add-it is preferred in over 80%
of cases, and it also demonstrates improvements in various automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is at https://research.nvidia.com/labs/par/addit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Exchange Rate <span class="highlight-title">Forecast</span>ing with Explainable Deep Learning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Meng, Andi Chen, Chihang Wang, Mengyao Zheng, Fangyu Wu, Xupeng Chen, Haowei Ni, Panfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate exchange rate prediction is fundamental to financial stability and
international trade, positioning it as a critical focus in economic and
financial research. Traditional forecasting models often falter when addressing
the inherent complexities and non-linearities of exchange rate data. This study
explores the application of advanced deep learning models, including LSTM, CNN,
and transformer-based architectures, to enhance the predictive accuracy of the
RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis
identifies TSMixer as the most effective model for this task. A rigorous
feature selection process emphasizes the inclusion of key economic indicators,
such as China-U.S. trade volumes and exchange rates of other major currencies
like the euro-RMB and yen-dollar pairs. The integration of grad-CAM
visualization techniques further enhances model interpretability, allowing for
clearer identification of the most influential features and bolstering the
credibility of the predictions. These findings underscore the pivotal role of
fundamental economic data in exchange rate forecasting and highlight the
substantial potential of machine learning models to deliver more accurate and
reliable predictions, thereby serving as a valuable tool for financial analysis
and decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Machine Learning and
  Computer Application</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Enhancing Prediction in Social Network
  Advertisement through Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qikai Yang, Panfeng Li, Xinhe Xu, Zhicheng Ding, Wenjing Zhou, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of social network advertising, the volume and
accuracy of data play a critical role in the performance of predictive models.
However, the development of robust predictive algorithms is often hampered by
the limited size and potential bias present in real-world datasets. This study
presents and explores a generative augmentation framework of social network
advertising data. Our framework explores three generative models for data
augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders
(VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and
diversity in the context of social network advertising analytics effectiveness.
By performing synthetic extensions of the feature space, we find that through
data augmentation, the performance of various classifiers has been
quantitatively improved. Furthermore, we compare the relative performance gains
brought by each data augmentation technique, providing insights for
practitioners to select appropriate techniques to enhance model performance.
This paper contributes to the literature by showing that synthetic data
augmentation alleviates the limitations imposed by small or imbalanced datasets
in the field of social network advertising. At the same time, this article also
provides a comparative perspective on the practicality of different data
augmentation methods, thereby guiding practitioners to choose appropriate
techniques to enhance model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 4th International Conference on Machine Learning and
  Intelligent Systems Engineering (MLISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time Series Modeling for Heart Rate Prediction: From ARIMA to
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li, Xupeng Chen, Xiaotong Wang, Shiyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular disease (CVD) is a leading cause of death globally,
necessitating precise forecasting models for monitoring vital signs like heart
rate, blood pressure, and ECG. Traditional models, such as ARIMA and Prophet,
are limited by their need for manual parameter tuning and challenges in
handling noisy, sparse, and highly variable medical data. This study
investigates advanced deep learning models, including LSTM, and
transformer-based architectures, for predicting heart rate time series from the
MIT-BIH Database. Results demonstrate that deep learning models, particularly
PatchTST, significantly outperform traditional models across multiple metrics,
capturing complex patterns and dependencies more effectively. This research
underscores the potential of deep learning to enhance patient monitoring and
CVD management, suggesting substantial clinical benefits. Future work should
extend these findings to larger, more diverse datasets and real-world clinical
applications to further validate and optimize model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Electronic
  Engineering and Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Diverse Methods in Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph <span class="highlight-title">Agent</span> Network: Empowering Nodes with Inference Capabilities for
  Adversarial Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liu, Wenshan Li, Tao Li, Beibei Li, Guangquan Xu, Pan Zhou, Wengang Ma, Hanyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end training with global optimization have popularized graph neural
networks (GNNs) for node classification, yet inadvertently introduced
vulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit
the inherent opened interfaces of GNNs' input and output, perturbing critical
edges and thus manipulating the classification results. Current defenses, due
to their persistent utilization of global-optimization-based end-to-end
training schemes, inherently encapsulate the vulnerabilities of GNNs. This is
specifically evidenced in their inability to defend against targeted secondary
attacks. In this paper, we propose the Graph Agent Network (GAgN) to address
the aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent
network in which each node is designed as an 1-hop-view agent. Through the
decentralized interactions between agents, they can learn to infer global
perceptions to perform tasks including inferring embeddings, degrees and
neighbor relationships for given nodes. This empowers nodes to filtering
adversarial edges while carrying out classification tasks. Furthermore, agents'
limited view prevents malicious messages from propagating globally in GAgN,
thereby resisting global-optimization-based secondary attacks. We prove that
single-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient
to achieve these functionalities. Experimental results show that GAgN
effectively implements all its intended capabilities and, compared to
state-of-the-art defenses, achieves optimal classification accuracy on the
perturbed datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Backdoored Graphs in Graph Neural Network Training: An
  Explanation-Based Approach with Novel Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Recognizing the challenge in
detecting such intrusions, we devised a novel detection method that creatively
leverages graph-level explanations. By extracting and transforming secondary
outputs from GNN explanation mechanisms, we developed seven innovative metrics
for effective detection of backdoor attacks on GNNs. Additionally, we develop
an adaptive attack to rigorously evaluate our approach. We test our method on
multiple benchmark datasets and examine its efficacy against various attack
models. Our results show that our method can achieve high detection
performance, marking a significant advancement in safeguarding GNNs against
backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Selection Based on Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel feature selection method leveraging the
Wasserstein distance to improve feature selection in machine learning. Unlike
traditional methods based on correlation or Kullback-Leibler (KL) divergence,
our approach uses the Wasserstein distance to assess feature similarity,
inherently capturing class relationships and making it robust to noisy labels.
We introduce a Markov blanket-based feature selection algorithm and demonstrate
its effectiveness. Our analysis shows that the Wasserstein distance-based
feature selection method effectively reduces the impact of noisy labels without
relying on specific noise models. We provide a lower bound on its
effectiveness, which remains meaningful even in the presence of noise.
Experimental results across multiple datasets demonstrate that our approach
consistently outperforms traditional methods, particularly in noisy settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game-theoretic <span class="highlight-title">LLM</span>: <span class="highlight-title">Agent</span> Workflow for Negotiation Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medication Recommendation via Dual Molecular Modalities and Multi-Step
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20358v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20358v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mu, Chen Li, Xiang Li, Shunpan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works based on molecular knowledge neglect the 3D geometric
structure of molecules and fail to learn the high-dimensional information of
medications, leading to structural confusion. Additionally, it does not extract
key substructures from a single patient visit, resulting in the failure to
identify medication molecules suitable for the current patient visit. To
address the above limitations, we propose a bimodal molecular recommendation
framework named BiMoRec, which introduces 3D molecular structures to obtain
atomic 3D coordinates and edge indices, overcoming the inherent lack of
high-dimensional molecular information in 2D molecular structures. To retain
the fast training and prediction efficiency of the recommendation system, we
use bimodal graph contrastive pretraining to maximize the mutual information
between the two molecular modalities, achieving the fusion of 2D and 3D
molecular graphs. Additionally, we designed a molecular multi-step enhancement
mechanism to re-calibrate the molecular weights. Specifically, we employ a
pre-training method that captures both 2D and 3D molecular structure
representations, along with substructure representations, and leverages
contrastive learning to extract mutual information. We then use the pre-trained
encoder to generate molecular representations, enhancing them through a
three-step process: intra-visit, molecular per-visit, and latest-visit.
Finally, we apply temporal information aggregation to generate the final
medication combinations. Our implementation on the MIMIC-III and MIMIC-IV
datasets demonstrates that our method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MicroScopiQ: Accelerating Foundational Models through Outlier-Aware
  Microscaling Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Ramachandran, Souvik Kundu, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization of foundational models (FMs) is significantly more challenging
than traditional DNNs due to the emergence of large magnitude features called
outliers. Existing outlier-aware algorithm/architecture co-design techniques
either use mixed-precision, retaining outliers at high precision but compromise
hardware efficiency, or quantize inliers and outliers at the same precision,
improving hardware efficiency at the cost of accuracy. To address this mutual
exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique
that leverages pruning to complement outlier-aware quantization. MicroScopiQ
retains outliers at higher precision while pruning a certain fraction of least
important weights to distribute the additional outlier bits; ensuring high
accuracy, aligned memory and hardware efficiency. We design a high-throughput,
low overhead accelerator architecture composed of simple multi-precision INT
processing elements and a novel network-on-chip called ReCoN that efficiently
abstracts the complexity of supporting high-precision outliers. Additionally,
unlike existing alternatives, MicroScopiQ does not assume any locality of
outlier weights, enabling applicability to a broad range of FMs. Extensive
experiments across various quantization settings show that MicroScopiQ achieves
SoTA quantization performance while simultaneously improving inference
performance by 3x and reducing energy by 2x over existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-in-the-Loop Segmentation of Multi-species Coral Imagery <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Niko Suenderhauf, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marine surveys by robotic underwater and surface vehicles result in
substantial quantities of coral reef imagery, however labeling these images is
expensive and time-consuming for domain experts. Point label propagation is a
technique that uses existing images labeled with sparse points to create
augmented ground truth data, which can be used to train a semantic segmentation
model. In this work, we show that recent advances in large foundation models
facilitate the creation of augmented ground truth masks using only features
extracted by the denoised version of the DINOv2 foundation model and K-Nearest
Neighbors (KNN), without any pre-training. For images with extremely sparse
labels, we present a labeling method based on human-in-the-loop principles,
which greatly enhances annotation efficiency: in the case that there are 5
point labels per image, our human-in-the-loop method outperforms the prior
state-of-the-art by 14.2% for pixel accuracy and 19.7% for mIoU; and by 8.9%
and 18.3% if there are 10 point labels. When human-in-the-loop labeling is not
available, using the denoised DINOv2 features with a KNN still improves on the
prior state-of-the-art by 2.7% for pixel accuracy and 5.8% for mIoU (5 grid
points). On the semantic segmentation task, we outperform the prior
state-of-the-art by 8.8% for pixel accuracy and by 13.5% for mIoU when only 5
point labels are used for point label propagation. Additionally, we perform a
comprehensive study into the impacts of the point label placement style and the
number of points on the point label propagation quality, and make several
recommendations for improving the efficiency of labeling images with points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal article preprint of extended paper, 30 pages, 11 figures.
  Original conference paper (v2) accepted at the CVPR2024 3rd Workshop on
  Learning with Limited Labelled Data for Image and Video Understanding
  (L3D-IVU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Data <span class="highlight-title">Distill</span>ation for Recovering Quality in Pruned Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithursan Thangarasa, Ganesh Venkatesh, Mike Lasby, Nish Sinnadurai, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 6 Tables (Main Paper) + 5 pages (Supplementary
  Material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Privacy-aware Split Learning Framework for Satellite
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Sun, Cong Wu, Shahid Mumtaz, Junyi Tao, Mingsheng Cao, Mei Wang, Valerio Frascolla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving domain of satellite communications, integrating
advanced machine learning techniques, particularly split learning, is crucial
for enhancing data processing and model training efficiency across satellites,
space stations, and ground stations. Traditional ML approaches often face
significant challenges within satellite networks due to constraints such as
limited bandwidth and computational resources. To address this gap, we propose
a novel framework for more efficient SL in satellite communications. Our
approach, Dynamic Topology Informed Pruning, namely DTIP, combines differential
privacy with graph and model pruning to optimize graph neural networks for
distributed learning. DTIP strategically applies differential privacy to raw
graph data and prunes GNNs, thereby optimizing both model size and
communication load across network tiers. Extensive experiments across diverse
datasets demonstrate DTIP's efficacy in enhancing privacy, accuracy, and
computational efficiency. Specifically, on Amazon2M dataset, DTIP maintains an
accuracy of 0.82 while achieving a 50% reduction in floating-point operations
per second. Similarly, on ArXiv dataset, DTIP achieves an accuracy of 0.85
under comparable conditions. Our framework not only significantly improves the
operational efficiency of satellite communications but also establishes a new
benchmark in privacy-aware distributed learning, potentially revolutionizing
data handling in space-based networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Framework for Causal Analysis of Recurrent <span class="highlight-title">Event</span>s with Timing
  Misalignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Oganisian, Anthony Girard, Jon A. Steingrimsson, Patience Moyo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Observational studies of recurrent event rates are common in biomedical
statistics. Broadly, the goal is to estimate differences in event rates under
two treatments within a defined target population over a specified followup
window. Estimation with observational data is challenging because, while
membership in the target population is defined in terms of eligibility
criteria, treatment is rarely observed exactly at the time of eligibility.
Ad-hoc solutions to this timing misalignment can induce bias by incorrectly
attributing prior event counts and person-time to treatment. Even if
eligibility and treatment are aligned, a terminal event process (e.g. death)
often stops the recurrent event process of interest. In practice, both
processes can be censored so that events are not observed over the entire
followup window. Our approach addresses misalignment by casting it as a
time-varying treatment problem: some patients are on treatment at eligibility
while others are off treatment but may switch to treatment at a specified time
- if they survive long enough. We define and identify an average causal effect
estimand under right-censoring. Estimation is done using a g-computation
procedure with a joint semiparametric Bayesian model for the death and
recurrent event processes. We apply the method to contrast hospitalization
rates among patients with different opioid treatments using Medicare insurance
claims data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ vTune: Verifiable Fine-Tuning for <span class="highlight-title">LLM</span>s Through Backdooring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Zhang, Arka Pal, Akilesh Potti, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As fine-tuning large language models (LLMs) becomes increasingly prevalent,
users often rely on third-party services with limited visibility into their
fine-tuning processes. This lack of transparency raises the question: how do
consumers verify that fine-tuning services are performed correctly? For
instance, a service provider could claim to fine-tune a model for each user,
yet simply send all users back the same base model. To address this issue, we
propose vTune, a simple method that uses a small number of backdoor data points
added to the training data to provide a statistical test for verifying that a
provider fine-tuned a custom model on a particular user's dataset. Unlike
existing works, vTune is able to scale to verification of fine-tuning on
state-of-the-art LLMs, and can be used both with open-source and closed-source
models. We test our approach across several model families and sizes as well as
across multiple instruction-tuning datasets, and find that the statistical test
is satisfied with p-values on the order of $\sim 10^{-40}$, with no negative
impact on downstream task performance. Further, we explore several attacks that
attempt to subvert vTune and demonstrate the method's robustness to these
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoldMark: Protecting Protein <span class="highlight-title">Generative</span> Models with Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20354v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20354v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaixi Zhang, Ruofan Jin, Kaidi Fu, Le Cong, Marinka Zitnik, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structure is key to understanding protein function and is essential
for progress in bioengineering, drug discovery, and molecular biology.
Recently, with the incorporation of generative AI, the power and accuracy of
computational protein structure prediction/design have been improved
significantly. However, ethical concerns such as copyright protection and
harmful content generation (biosecurity) pose challenges to the wide
implementation of protein generative models. Here, we investigate whether it is
possible to embed watermarks into protein generative models and their outputs
for copyright authentication and the tracking of generated structures. As a
proof of concept, we propose a two-stage method FoldMark as a generalized
watermarking strategy for protein generative models. FoldMark first pretrain
watermark encoder and decoder, which can minorly adjust protein structures to
embed user-specific information and faithfully recover the information from the
encoded structure. In the second step, protein generative models are fine-tuned
with watermark-conditioned Low-Rank Adaptation (LoRA) modules to preserve
generation quality while learning to generate watermarked structures with high
recovery rates. Extensive experiments are conducted on open-source protein
structure prediction models (e.g., ESMFold and MultiFlow) and de novo structure
design models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method
is effective across all these generative models. Meanwhile, our watermarking
framework only exerts a negligible impact on the original protein structure
quality and is robust under potential post-processing and adaptive attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks
  against "Truly Anonymous" Synthetic <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgi Ganev, Emiliano De Cristofaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models producing synthetic data are meant to provide a
privacy-friendly approach to releasing data. However, their privacy guarantees
are only considered robust when models satisfy Differential Privacy (DP). Alas,
this is not a ubiquitous standard, as many leading companies (and, in fact,
research papers) use ad-hoc privacy metrics based on testing the statistical
similarity between synthetic and real data. In this paper, we examine the
privacy metrics used in real-world synthetic data deployments and demonstrate
their unreliability in several ways. First, we provide counter-examples where
severe privacy violations occur even if the privacy tests pass and instantiate
accurate membership and attribute inference attacks with minimal cost. We then
introduce ReconSyn, a reconstruction attack that generates multiple synthetic
datasets that are considered private by the metrics but actually leak
information unique to individual records. We show that ReconSyn recovers
78-100% of the outliers in the train data with only black-box access to a
single fitted generative model and the privacy metrics. In the process, we show
that applying DP only to the model does not mitigate this attack, as using
privacy metrics breaks the end-to-end DP pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Block Coordinate Descent Method for Nonsmooth Composite Optimization
  under Orthogonality Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganzhao Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonsmooth composite optimization with orthogonality constraints is crucial in
statistical learning and data science, but it presents challenges due to its
nonsmooth objective and computationally expensive, non-convex constraints. In
this paper, we propose a new approach called \textbf{OBCD}, which leverages
Block Coordinate Descent (BCD) to address these challenges. \textbf{OBCD} is a
feasible method with a small computational footprint. In each iteration, it
updates $k$ rows of the solution matrix, where $k \geq 2$, while globally
solving a small nonsmooth optimization problem under orthogonality constraints.
We prove that \textbf{OBCD} converges to block-$k$ stationary points, which
offer stronger optimality than standard critical points. Notably, \textbf{OBCD}
is the first greedy descent method with monotonicity for this problem class.
Under the Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point
convergence. We also extend \textbf{OBCD} with breakpoint searching methods for
subproblem solving and greedy strategies for working set selection.
Comprehensive experiments demonstrate the superior performance of our approach
across various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Chen, Qiaobo Li, Arindam Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining gradient compression methods (e.g., CountSketch, quantization) and
adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated
learning (FL), with potential benefits on both fewer communication rounds and
less per-round communication. In spite of the preliminary empirical success of
sketched adaptive methods, existing convergence analyses show the communication
cost to have a linear dependence on the ambient dimension, i.e., number of
parameters, which is prohibitively high for modern deep learning models. In
this work, we introduce specific sketched adaptive federated learning (SAFL)
algorithms and, as our main contribution, provide theoretical convergence
analyses in different FL settings with guarantees on communication cost
depending only logarithmically (instead of linearly) on the ambient dimension.
Unlike existing analyses, we show that the entry-wise sketching noise existent
in the preconditioners and the first moments of SAFL can be implicitly
addressed by leveraging the recently-popularized anisotropic curvatures in deep
learning losses, e.g., fast decaying loss Hessian eigen-values. In the i.i.d.
client setting of FL, we show that SAFL achieves asymptotic $O(1/\sqrt{T})$
convergence, and converges faster in the initial epochs. In the non-i.i.d.
client setting, where non-adaptive methods lack convergence guarantees, we show
that SACFL (SAFL with clipping) algorithms can provably converge in spite of
the additional heavy-tailed noise. Our theoretical claims are supported by
empirical studies on vision and language tasks, and in both fine-tuning and
training-from-scratch regimes. Surprisingly, as a by-product of our analysis,
the proposed SAFL methods are competitive with the state-of-the-art
communication-efficient federated learning algorithms based on error feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wonderful Matrices: More Efficient and Effective Architecture for
  <span class="highlight-title">Language Model</span>ing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16958v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16958v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Bingheng Wu, Lu He, Luchang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove the availability of inner product form position encoding in the
state space dual algorithm and study the effectiveness of different position
embeddings in the hybrid quadratic causal self-attention and state space dual
algorithms. We propose inner function attention with dynamic mask, which can
improve the expressiveness of the attention algorithm and avoid the sequence
noise significantly affecting the accuracy of the attention score. We also
design cross domain mixture of experts, which can improve the granularity of
the sparse activation feedforward network while maintaining the efficiency of
parameter utilization and retrieval. The combination of these methods
constitutes our foundation model architecture: Wonderful Matrices. We conduct
experiments on the language modeling task and find that Wonderful Matrices are
more efficient and effective in handling complex language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in
  Variational AutoEncoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee-Jun Jung, Jaehyoung Jeong, Kangil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs. However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
the training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information. CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry code-book 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing a group equivariant encoder and decoder in training
VAEs with the two conditions. In addition, we propose an extended evaluation
metric for multi-factor changes in comparison to disentanglement evaluation in
VAEs. In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in TMLR 25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-overlapping, Schwarz-type Domain Decomposition Method for Physics
  and Equality Constrained Artificial Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifeng Hu, Shamsulhaq Basir, Inanc Senocak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a non-overlapping, Schwarz-type domain decomposition method with a
generalized interface condition, designed for physics-informed machine learning
of partial differential equations (PDEs) in both forward and inverse contexts.
Our approach employs physics and equality-constrained artificial neural
networks (PECANN) within each subdomain. Unlike the original PECANN method,
which relies solely on initial and boundary conditions to constrain PDEs, our
method uses both boundary conditions and the governing PDE to constrain a
unique interface loss function for each subdomain. This modification improves
the learning of subdomain-specific interface parameters while reducing
communication overhead by delaying information exchange between neighboring
subdomains. To address the constrained optimization in each subdomain, we apply
an augmented Lagrangian method with a conditionally adaptive update strategy,
transforming the problem into an unconstrained dual optimization. A distinct
advantage of our domain decomposition method is its ability to learn solutions
to both Poisson's and Helmholtz equations, even in cases with high-wavenumber
and complex-valued solutions. Through numerical experiments with up to 64
subdomains, we demonstrate that our method consistently generalizes well as the
number of subdomains increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEaRL: Personalized Privacy of Human-Centric Systems using Early-Exit
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Taherisadr, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving landscape of human-centric systems, personalized privacy
solutions are becoming increasingly crucial due to the dynamic nature of human
interactions. Traditional static privacy models often fail to meet the diverse
and changing privacy needs of users. This paper introduces PEaRL, a system
designed to enhance privacy preservation by tailoring its approach to
individual behavioral patterns and preferences. While incorporating
reinforcement learning (RL) for its adaptability, PEaRL primarily focuses on
employing an early-exit strategy that dynamically balances privacy protection
and system utility. This approach addresses the challenges posed by the
variability and evolution of human behavior, which static privacy models
struggle to handle effectively. We evaluate PEaRL in two distinct contexts:
Smart Home environments and Virtual Reality (VR) Smart Classrooms. The
empirical results demonstrate PEaRL's capability to provide a personalized
tradeoff between user privacy and application utility, adapting effectively to
individual user preferences. On average, across both systems, PEaRL enhances
privacy protection by 31%, with a corresponding utility reduction of 24%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Analysis on the Subgradient Upper Bounds for the Subgradient
  Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoli Zhu, Lei Zhao, Shuzhong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a unified analysis for the proximal subgradient method
(Prox-SubGrad) type approach to minimize an overall objective of $f(x)+r(x)$,
subject to convex constraints, where both $f$ and $r$ are weakly convex,
nonsmooth, and non-Lipschitz. Leveraging on the properties of the Moreau
envelope of weakly convex functions, we are able to relate error-bound
conditions, the growth conditions of the subgradients of the objective, and the
behavior of the proximal subgradient iterates on some remarkably broad classes
of objective functions. Various existing as well as new bounding conditions are
studied, leading to novel iteration complexity results. The terrain of our
exploration expands to stochastic proximal subgradient algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-11T00:00:00Z">2024-11-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">120</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding <span class="highlight-title">Thought</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Qingping Yang, Runtao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and adaptability. This
paper introduces the UTMath Benchmark, which robustly evaluates the models
through extensive unit tests. It consists of 1,053 problems across 9
mathematical domains, with over 68 test cases per problem.We propose an
innovative evaluation framework inspired by unit testing in software
development, focusing on both accuracy and reliability of results. Furthermore,
we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which
encourages LLMs to perform explicit reasoning before generating code, leading
to generating more advanced solution and improved performance. Furthermore, we
are releasing not only the UTMath benchmark but also the UTMath-Train training
dataset (more than 70k samples), to support the community in further exploring
mathematical reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenThai<span class="highlight-title">GPT</span> 1.5: A Thai-Centric Open Source <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextualized Evaluations: Taking the Guesswork Out of <span class="highlight-title">Language Model</span>
  Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model users often issue queries that lack specification, where the
context under which a query was issued -- such as the user's identity, the
query's intent, and the criteria for a response to be useful -- is not
explicit. For instance, a good response to a subjective query like "What book
should I read next?" would depend on the user's preferences, and a good
response to an open-ended query like "How do antibiotics work against
bacteria?" would depend on the user's expertise. This makes evaluation of
responses to such queries an ill-posed task, as evaluators may make arbitrary
judgments about the response quality. To remedy this, we present contextualized
evaluations, a protocol that synthetically constructs context surrounding an
underspecified query and provides it during evaluation. We find that the
presence of context can 1) alter conclusions drawn from evaluation, even
flipping win rates between model pairs, 2) nudge evaluators to make fewer
judgments based on surface-level criteria, like style, and 3) provide new
insights about model behavior across diverse contexts. Specifically, our
procedure uncovers an implicit bias towards WEIRD contexts in models' "default"
responses and we find that models are not equally sensitive to following
different contexts, even when they are provided in prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code & data available at https://github.com/allenai/ContextEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on
  <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Simão, Fabiano Prado, Omar Abdul Wahab, Anderson Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread of digital environments, reliable authentication and
continuous access control has become crucial. It can minimize cyber attacks and
prevent frauds, specially those associated with identity theft. A particular
interest lies on keystroke dynamics (KD), which refers to the task of
recognizing individuals' identity based on their unique typing style. In this
work, we propose the use of pre-trained language models (PLMs) to recognize
such patterns. Although PLMs have shown high performance on multiple NLP
benchmarks, the use of these models on specific tasks requires customization.
BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot
be directly applied to KD, which requires temporal-character information to
recognize users. Recent character-aware PLMs are able to process both subwords
and character-level information and can be an alternative solution.
Notwithstanding, they are still not suitable to be directly fine-tuned for KD
as they are not optimized to account for user's temporal typing information
(e.g., hold time and flight time). To overcome this limitation, we propose
TempCharBERT, an architecture that incorporates temporal-character information
in the embedding layer of CharBERT. This allows modeling keystroke dynamics for
the purpose of user identification and authentication. Our results show a
significant improvement with this customization. We also showed the feasibility
of training TempCharBERT on a federated learning settings in order to foster
data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WIFS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeCoders: Trees of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Colonna D'Istria, Abdulrahman Altahhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce TreeCoders, a novel family of transformer trees.
We moved away from traditional linear transformers to complete k-ary trees.
Transformer blocks serve as nodes, and generic classifiers learn to select the
best child and route the sequence of tokens to a specific leaf. The selectors,
moved outside the transformer blocks, allow for the use of a variety of
architecture without further modifications. Furthermore, our proposed
architecture supports sparse node activation due to the logarithmic complexity
of a tree search. We validate our idea by testing a series of decoder-only tree
transformers, achieving competitive results across a diverse range of language
datasets. Our study demonstrates that the proposed tree transformer model
outperforms a size-equivalent linear transformer model 76\% of the time over a
wide range of tree architectures. Furthermore, our proposed model naturally
lends itself to distributed implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Super Weight in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown a surprising result: a small fraction of Large
Language Model (LLM) parameter outliers are disproportionately important to the
quality of the model. LLMs contain billions of parameters, so these small
fractions, such as 0.01%, translate to hundreds of thousands of parameters. In
this work, we present an even more surprising finding: Pruning as few as a
single parameter can destroy an LLM's ability to generate text -- increasing
perplexity by 3 orders of magnitude and reducing zero-shot accuracy to
guessing. We propose a data-free method for identifying such parameters, termed
super weights, using a single forward pass through the model. We additionally
find that these super weights induce correspondingly rare and large activation
outliers, termed super activations. When preserved with high precision, super
activations can improve simple round-to-nearest quantization to become
competitive with state-of-the-art methods. For weight quantization, we
similarly find that by preserving the super weight and clipping other weight
outliers, round-to-nearest quantization can scale to much larger block sizes
than previously considered. To facilitate further research into super weights,
we provide an index of super weight coordinates for common, openly available
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Generation from <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to intervene on
these models. To understand the impact of interventions precisely, it is useful
to examine counterfactuals -- e.g., how a given sentence would have appeared
had it been generated by the model following a specific intervention. We
highlight that counterfactual reasoning is conceptually distinct from
interventions, as articulated in Pearl's causal hierarchy. Based on this
observation, we propose a framework for generating true string counterfactuals
by reformulating language models as Generalized Structural-equation. Models
using the Gumbel-max trick. This allows us to model the joint distribution over
original strings and their counterfactuals resulting from the same
instantiation of the sampling noise. We develop an algorithm based on hindsight
Gumbel sampling that allows us to infer the latent noise variables and generate
counterfactuals of observed strings. Our experiments demonstrate that the
approach produces meaningful counterfactuals while at the same time showing
that commonly used intervention techniques have considerable undesired side
effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Continual</span> Memorization of Factoids in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, <span class="highlight-author">Danqi Chen</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can absorb a massive amount of knowledge through
pretraining, but pretraining is inefficient for acquiring long-tailed or
specialized facts. Therefore, fine-tuning on specialized or new knowledge that
reflects changes in the world has become popular, though it risks disrupting
the model's original capabilities. We study this fragility in the context of
continual memorization, where the model is trained on a small set of long-tail
factoids (factual associations) and must retain these factoids after multiple
stages of subsequent training on other datasets. Through extensive experiments,
we show that LLMs suffer from forgetting across a wide range of subsequent
tasks, and simple replay techniques do not fully prevent forgetting, especially
when the factoid datasets are trained in the later stages. We posit that there
are two ways to alleviate forgetting: 1) protect the memorization process as
the model learns the factoids, or 2) reduce interference from training in later
stages. With this insight, we develop an effective mitigation strategy: REMIX
(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic
data sampled from pretraining corpora or even randomly generated word sequences
during each stage, despite being unrelated to the memorized factoids in the
first stage. REMIX can recover performance from severe forgetting, often
outperforming replay-based methods that have access to the factoids from the
first stage. We then analyze how REMIX alters the learning process and find
that successful forgetting prevention is associated with a pattern: the model
stores factoids in earlier layers than usual and diversifies the set of layers
that store these factoids. The efficacy of REMIX invites further investigation
into the underlying dynamics of memorization and forgetting, opening exciting
possibilities for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Primer on Word Embeddings: AI Techniques for Text Analysis in Social
  Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian E. Perron, Kelley A. Rivenburgh, Bryan G. Victor, Zia Qi, Hui Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embeddings represent a transformative technology for analyzing text data
in social work research, offering sophisticated tools for understanding case
notes, policy documents, research literature, and other text-based materials.
This methodological paper introduces word embeddings to social work
researchers, explaining how these mathematical representations capture meaning
and relationships in text data more effectively than traditional keyword-based
approaches. We discuss fundamental concepts, technical foundations, and
practical applications, including semantic search, clustering, and retrieval
augmented generation. The paper demonstrates how embeddings can enhance
research workflows through concrete examples from social work practice, such as
analyzing case notes for housing instability patterns and comparing social work
licensing examinations across languages. While highlighting the potential of
embeddings for advancing social work research, we acknowledge limitations
including information loss, training data constraints, and potential biases. We
conclude that successfully implementing embedding technologies in social work
requires developing domain-specific models, creating accessible tools, and
establishing best practices aligned with social work's ethical principles. This
integration can enhance our ability to analyze complex patterns in text data
while supporting more effective services and interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingbo Mo, Shun Jiang, Akash Maharaj, Bernard Hishamunda, Yunyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-Oriented Dialogue (TOD) systems assist users in completing tasks through
natural language interactions, often relying on a single-layered workflow
structure for slot-filling in public tasks, such as hotel bookings. However, in
enterprise environments, which involve rich domain-specific knowledge, TOD
systems face challenges due to task complexity and the lack of standardized
documentation. In this work, we introduce HierTOD, an enterprise TOD system
driven by hierarchical goals and can support composite workflows. By focusing
on goal-driven interactions, our system serves a more proactive role,
facilitating mixed-initiative dialogue and improving task completion. Equipped
with components for natural language understanding, composite goal retriever,
dialogue management, and response generation, backed by a well-organized data
service with domain knowledge base and retrieval engine, HierTOD delivers
efficient task assistance. Furthermore, our system implementation unifies two
TOD paradigms: slot-filling for information collection and step-by-step
guidance for task execution. Our human study demonstrates the effectiveness and
helpfulness of HierTOD in performing both paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text
  Embeddings Must Adapt <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, Charlie Flanagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial documents are filled with specialized terminology, arcane jargon,
and curious acronyms that pose challenges for general-purpose text embeddings.
Yet, few text embeddings specialized for finance have been reported in the
literature, perhaps in part due to a lack of public datasets and benchmarks. We
present BAM embeddings, a set of text embeddings finetuned on a carefully
constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of
domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a
held-out test set, vs. only 39.2% for the best general-purpose text embedding
from OpenAI. Further, BAM embeddings increase question answering accuracy by 8%
on FinanceBench and show increased sensitivity to the finance-specific elements
that are found in detailed, forward-looking and company and date-specific
queries. To support further research we describe our approach in detail,
quantify the importance of hard negative mining and dataset scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New LLM evaluation benchmarks are important to align with the rapid
development of Large Language Models (LLMs). In this work, we present Chinese
SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality
ability of language models to answer short questions, and Chinese SimpleQA
mainly has five properties (i.e., Chinese, Diverse, High-quality, Static,
Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6
major topics with 99 diverse subtopics. Second, we conduct a comprehensive
quality control process to achieve high-quality questions and answers, where
the reference answers are static and cannot be changed over time. Third,
following SimpleQA, the questions and answers are very short, and the grading
process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we
perform a comprehensive evaluation on the factuality abilities of existing
LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to
better understand the Chinese factuality abilities of their models and
facilitate the growth of foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stronger Models are NOT Stronger Teachers for Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval or Global Context Understanding? On Many-Shot In-Context
  Learning for Long-Context Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijian Zou, Muhammad Khalifa, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have demonstrated an improved capacity to handle
long-context information, yet existing long-context benchmarks primarily
measure LMs' retrieval abilities with extended inputs, e.g., pinpointing a
short phrase from long-form text. Therefore, they may fall short when
evaluating models' global context understanding capacity, such as synthesizing
and reasoning over content across input to generate the response. In this
paper, we study long-context language model (LCLM) evaluation through many-shot
in-context learning (ICL). Concretely, we identify the skills each ICL task
requires, and examine models' long-context capabilities on them. We first ask:
What types of ICL tasks benefit from additional demonstrations, and are these
tasks effective at evaluating LCLMs? We find that classification and
summarization tasks show notable performance improvements with additional
demonstrations, while translation and reasoning tasks do not exhibit clear
trends. This suggests the classification tasks predominantly test models'
retrieval skills. Next, we ask: To what extent does each task require retrieval
skills versus global context understanding from LCLMs? We develop metrics to
categorize ICL tasks into two groups: (i) retrieval tasks that require strong
retrieval ability to pinpoint relevant examples, and (ii) global context
understanding tasks that necessitate a deeper comprehension of the full input.
We find that not all datasets can effectively evaluate these long-context
capabilities. To address this gap, we introduce a new many-shot ICL benchmark,
MANYICLBENCH, designed to characterize LCLMs' retrieval and global context
understanding capabilities separately. Benchmarking 11 open-weight LCLMs with
MANYICLBENCH, we find that while state-of-the-art models perform well in
retrieval tasks up to 64k tokens, many show significant drops in global context
tasks at just 16k tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">LLM</span>s' Judgments with No Gold Standard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the GEM (Generative Estimator for Mutual Information), an
evaluation metric for assessing language generation by Large Language Models
(LLMs), particularly in generating informative judgments, without the need for
a gold standard reference. GEM broadens the scenarios where we can benchmark
LLM generation performance-from traditional ones, like machine translation and
summarization, where gold standard references are readily available, to
subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate
and reference responses, without requiring the reference to be a gold standard.
In experiments on a human-annotated dataset, GEM demonstrates competitive
correlations with human scores compared to the state-of-the-art GPT-4o
Examiner, and outperforms all other baselines. Additionally, GEM is more robust
against strategic manipulations, such as rephrasing or elongation, which can
artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which
evaluates LLMs based on how well they can generate high-quality peer reviews
for academic research papers. Because GRE-bench is based upon GEM, it inherits
its robustness properties. Additionally, GRE-bench circumvents data
contamination problems (or data leakage) by using the continuous influx of new
open-access research papers and peer reviews each year. We show GRE-bench
results of various popular LLMs on their peer review capabilities using the
ICLR2023 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering
  in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating human-like text, but their output may not be aligned with the user
or even produce harmful content. This paper presents a novel approach to detect
and steer concepts such as toxicity before generation. We introduce the Sparse
Conditioned Autoencoder (SCAR), a single trained module that extends the
otherwise untouched LLM. SCAR ensures full steerability, towards and away from
concepts (e.g., toxic content), without compromising the quality of the model's
text generation on standard evaluation benchmarks. We demonstrate the effective
application of our approach through a variety of concepts, including toxicity,
safety, and writing style alignment. As such, this work establishes a robust
framework for controlling LLM generations, ensuring their ethical and safe
deployment in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a Taiwanese Mandarin Spoken <span class="highlight-title">Language Model</span>: A First Attempt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report presents our initial attempt to build a spoken large
language model (LLM) for Taiwanese Mandarin, specifically tailored to enable
real-time, speech-to-speech interaction in multi-turn conversations. Our
end-to-end model incorporates a decoder-only transformer architecture and aims
to achieve seamless interaction while preserving the conversational flow,
including full-duplex capabilities allowing simultaneous speaking and
listening. The paper also details the training process, including data
preparation with synthesized dialogues and adjustments for real-time
interaction. We also developed a platform to evaluate conversational fluency
and response coherence in multi-turn dialogues. We hope the release of the
report can contribute to the future development of spoken LLMs in Taiwanese
Mandarin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Neural Networks as Recognizers of Formal Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the computational power of neural network architectures in
terms of formal language theory remains a crucial line of research, as it
describes lower and upper bounds on the reasoning capabilities of modern AI.
However, when empirically testing these bounds, existing work often leaves a
discrepancy between experiments and the formal claims they are meant to
support. The problem is that formal language theory pertains specifically to
recognizers: machines that receive a string as input and classify whether it
belongs to a language. On the other hand, it is common to instead use proxy
tasks that are similar in only an informal sense, such as language modeling or
sequence-to-sequence transduction. We correct this mismatch by training and
evaluating neural networks directly as binary classifiers of strings, using a
general method that can be applied to a wide variety of languages. As part of
this, we extend an algorithm recently proposed by Sn{\ae}bjarnarson et al.
(2024) to do length-controlled sampling of strings from regular languages, with
much better asymptotic time complexity than previous methods. We provide
results on a variety of languages across the Chomsky hierarchy for three neural
architectures: a simple RNN, an LSTM, and a causally-masked transformer. We
find that the RNN and LSTM often outperform the transformer, and that auxiliary
training objectives such as language modeling can help, although no single
objective uniformly improves performance across languages and architectures.
Our contributions will facilitate theoretically sound empirical testing of
language recognition claims in future work. We have released our datasets as a
benchmark called FLaRe (Formal Language Recognition), along with our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 2 figures. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> verbatim in-context retrieval across time and scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristijan Armeni, Marko Pranjić, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To predict upcoming text, language models must in some cases retrieve
in-context information verbatim. In this report, we investigated how the
ability of language models to retrieve arbitrary in-context nouns developed
during training (across time) and as language models trained on the same
dataset increase in size (across scale). We then asked whether learning of
in-context retrieval correlates with learning of more challenging zero-shot
benchmarks. Furthermore, inspired by semantic effects in human short-term
memory, we evaluated the retrieval with respect to a major semantic component
of target nouns, namely whether they denote a concrete or abstract entity, as
rated by humans. We show that verbatim in-context retrieval developed in a
sudden transition early in the training process, after about 1% of the training
tokens. This was observed across model sizes (from 14M and up to 12B
parameters), and the transition occurred slightly later for the two smallest
models. We further found that the development of verbatim in-context retrieval
is positively correlated with the learning of zero-shot benchmarks. Around the
transition point, all models showed the advantage of retrieving concrete nouns
as opposed to abstract nouns. In all but two smallest models, the advantage
dissipated away toward the end of training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Conference on Natural Language Learning 2024
  (https://www.conll.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Response and Emergence of Induction in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niclas Luick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While induction is considered a key mechanism for in-context learning in
LLMs, understanding its precise circuit decomposition beyond toy models remains
elusive. Here, we study the emergence of induction behavior within LLMs by
probing their response to weak single-token perturbations of the residual
stream. We find that LLMs exhibit a robust, universal regime in which their
response remains scale-invariant under changes in perturbation strength,
thereby allowing us to quantify the build-up of token correlations throughout
the model. By applying our method, we observe signatures of induction behavior
within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across
all models, we find that these induction signatures gradually emerge within
intermediate layers and identify the relevant model sections composing this
behavior. Our results provide insights into the collective interplay of
components within LLMs and serve as a benchmark for large-scale circuit
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Active Privacy Auditing in Supervised Fine-tuning for White-Box
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Sun, Hanpeng Wu, Xi Sheryl Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretraining and fine-tuning approach has become the leading technique for
various NLP applications. However, recent studies reveal that fine-tuning data,
due to their sensitive nature, domain-specific characteristics, and
identifiability, pose significant privacy concerns. To help develop more
privacy-resilient fine-tuning models, we introduce a novel active privacy
auditing framework, dubbed Parsing, designed to identify and quantify privacy
leakage risks during the supervised fine-tuning (SFT) of language models (LMs).
The framework leverages improved white-box membership inference attacks (MIAs)
as the core technology, utilizing novel learning objectives and a two-stage
pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the
exposure of privacy risks. Additionally, we have improved the effectiveness of
MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our
research aims to provide the SFT community of LMs with a reliable, ready-to-use
privacy auditing tool, and to offer valuable insights into safeguarding privacy
during the fine-tuning process. Experimental results confirm the framework's
efficiency across various models and tasks, emphasizing notable privacy
concerns in the fine-tuning process. Project code available for
https://github.com/mapleleavesss/PARSING.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network pruning is a set of computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has focused on pruning and re-training, which
nowadays is inconvenient due to the vast amount of pre-trained models, which
are in any case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs, that
modifies the block-wise and row-wise sparsity ratios to maximize the
\emph{neuron alignment} among activations. Moreover, differently from existing
methods, our approach adaptively selects the best parameters for the block-wise
and row-wise sparsity ratios w.r.t. to the model and the desired sparsity
(given as input), and requires \emph{no re-training}. We test our method on 4
different LLM families and 3 different sparsity ratios, showing how it
consistently outperforms the latest state-of-the-art techniques. The code is
available at https://github.com/eliacunegatti/NeuroAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minion: A Technology Probe for Resolving Value Conflicts through
  Expert-Driven and User-Driven Strategies in AI Companion Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzhe Fan, Qing Xiao, Xuhui Zhou, Yuran Su, Zhicong Lu, Maarten Sap, Hong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI companions based on large language models can role-play and converse very
naturally. When value conflicts arise between the AI companion and the user, it
may offend or upset the user. Yet, little research has examined such conflicts.
We first conducted a formative study that analyzed 151 user complaints about
conflicts with AI companions, providing design implications for our study.
Based on these, we created Minion, a technology probe to help users resolve
human-AI value conflicts. Minion applies a user-empowerment intervention method
that provides suggestions by combining expert-driven and user-driven conflict
resolution strategies. We conducted a technology probe study, creating 40 value
conflict scenarios on Character.AI and Talkie. 22 participants completed 274
tasks and successfully resolved conflicts 94.16% of the time. We summarize user
responses, preferences, and needs in resolving value conflicts, and propose
design implications to reduce conflicts and empower users to resolve them more
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIFBench: Evaluating the Instruction Following Performance and Stability
  of <span class="highlight-title">Large Language Model</span>s in Long-Context Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in natural language
processing (NLP), their ability to stably follow instructions in long-context
inputs has become crucial for real-world applications. While existing
benchmarks assess various LLM capabilities, they rarely focus on
instruction-following in long-context scenarios or stability on different
inputs. In response, we introduce the Long-context Instruction-Following
Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs'
instruction-following capabilities and stability across long contexts. LIFBench
comprises three long-context scenarios and eleven diverse tasks, supported by
2,766 instructions generated through an automated expansion method across three
dimensions: length, expression, and variables. For evaluation, we propose
LIFEval, a rubric-based assessment framework that provides precise, automated
scoring of complex LLM responses without relying on LLM-assisted evaluations or
human judgments. This approach facilitates a comprehensive analysis of model
performance and stability across various perspectives. We conduct extensive
experiments on 20 notable LLMs across six length intervals, analyzing their
instruction-following capabilities and stability. Our work contributes LIFBench
and LIFEval as robust tools for assessing LLM performance in complex,
long-context settings, providing insights that can inform future LLM
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniHR: Hierarchical Representation Learning for Unified Knowledge Graph
  Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Liu, Mingyang Chen, Yin Hua, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond-triple fact representations including hyper-relational facts with
auxiliary key-value pairs, temporal facts with additional timestamps, and
nested facts implying relationships between facts, are gaining significant
attention. However, existing link prediction models are usually designed for
one specific type of facts, making it difficult to generalize to other fact
representations. To overcome this limitation, we propose a Unified Hierarchical
Representation learning framework (UniHR) for unified knowledge graph link
prediction. It consists of a unified Hierarchical Data Representation (HiDR)
module and a unified Hierarchical Structure Learning (HiSL) module as graph
encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested
factual KGs into triple-based representations. Then HiSL incorporates
intra-fact and inter-fact message passing, focusing on enhancing the semantic
information within individual facts and enriching the structural information
between facts. Experimental results across 7 datasets from 3 types of KGs
demonstrate that our UniHR outperforms baselines designed for one specific kind
of KG, indicating strong generalization capability of HiDR form and the
effectiveness of HiSL module. Code and data are available at
https://github.com/Lza12a/UniHR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token2Wave 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides an in-depth analysis of Token2Wave, a novel token
representation method derived from the Wave Network, designed to capture both
global and local semantics of input text through wave-inspired complex vectors.
In Token2Wave, each token is represented with a magnitude component, capturing
the global semantics of the entire input text, and a phase component, encoding
the relationships between individual tokens and the global semantics. Building
on prior research that demonstrated the effectiveness of wave-like operations,
such as interference and modulation, during forward propagation, this study
investigates the convergence behavior, backpropagation characteristics, and
embedding independence within the Token2Wave framework. A detailed
computational complexity analysis shows that Token2Wave can significantly
reduce video memory usage and training time compared to BERT. Gradient
comparisons for the [CLS] token, total input text, and classifier parameters
further highlight Token2Wave's unique characteristics. This research offers new
insights into wave-based token representations, demonstrating their potential
to enable efficient and computationally friendly language model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring <span class="highlight-title">LLM</span>'s Perceptual
  Alignment with Human Smell Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning AI with human intent is important, yet perceptual alignment-how AI
interprets what we see, hear, or smell-remains underexplored. This work focuses
on olfaction, human smell experiences. We conducted a user study with 40
participants to investigate how well AI can interpret human descriptions of
scents. Participants performed "sniff and describe" interactive tasks, with our
designed AI system attempting to guess what scent the participants were
experiencing based on their descriptions. These tasks evaluated the Large
Language Model's (LLMs) contextual understanding and representation of scent
relationships within its internal states - high-dimensional embedding space.
Both quantitative and qualitative methods were used to evaluate the AI system's
performance. Results indicated limited perceptual alignment, with biases
towards certain scents, like lemon and peppermint, and continued failing to
identify others, like rosemary. We discuss these findings in light of human-AI
alignment advancements, highlighting the limitations and opportunities for
enhancing HCI systems with multisensory experience integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer-Answer: Empowering Cancer Care with Advanced Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastrointestinal (GI) tract cancers account for a substantial portion of the
global cancer burden, where early diagnosis is critical for improved management
and patient outcomes. The complex aetiologies and overlapping symptoms across
GI cancers often delay diagnosis, leading to suboptimal treatment strategies.
Cancer-related queries are crucial for timely diagnosis, treatment, and patient
education, as access to accurate, comprehensive information can significantly
influence outcomes. However, the complexity of cancer as a disease, combined
with the vast amount of available data, makes it difficult for clinicians and
patients to quickly find precise answers. To address these challenges, we
leverage large language models (LLMs) such as GPT-3.5 Turbo to generate
accurate, contextually relevant responses to cancer-related queries.
Pre-trained with medical data, these models provide timely, actionable insights
that support informed decision-making in cancer diagnosis and care, ultimately
improving patient outcomes. We calculate two metrics: A1 (which represents the
fraction of entities present in the model-generated answer compared to the gold
standard) and A2 (which represents the linguistic correctness and
meaningfulness of the model-generated answer with respect to the gold
standard), achieving maximum values of 0.546 and 0.881, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Conversational System for Differential
  Diagnosis of GI Cancer)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electroencephalogram-based Multi-class Decoding of Attended Speakers'
  Direction with Audio Spatial Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanming Zhang, Jing Lu, Zhibin Lin, Fei Chen, Haoliang Du, Xia Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding the directional focus of an attended speaker from listeners'
electroencephalogram (EEG) signals is essential for developing brain-computer
interfaces to improve the quality of life for individuals with hearing
impairment. Previous works have concentrated on binary directional focus
decoding, i.e., determining whether the attended speaker is on the left or
right side of the listener. However, a more precise decoding of the exact
direction of the attended speaker is necessary for effective speech processing.
Additionally, audio spatial information has not been effectively leveraged,
resulting in suboptimal decoding results. In this paper, we observe that, on
our recently presented dataset with 15-class directional focus, models relying
exclusively on EEG inputs exhibits significantly lower accuracy when decoding
the directional focus in both leave-one-subject-out and leave-one-trial-out
scenarios. By integrating audio spatial spectra with EEG features, the decoding
accuracy can be effectively improved. We employ the CNN, LSM-CNN, and
EEG-Deformer models to decode the directional focus from listeners' EEG signals
with the auxiliary audio spatial spectra. The proposed Sp-Aux-Deformer model
achieves notable 15-class decoding accuracies of 57.48% and 61.83% in
leave-one-subject-out and leave-one-trial-out scenarios, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVQAScore: Efficient Video Question Answering Data Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Zirong Chen, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question-answering (QA) is a core task in video understanding.
Evaluating the quality of video QA and video caption data quality for training
video large language models (VideoLLMs) is an essential challenge. Although
various methods have been proposed for assessing video caption quality, there
remains a lack of dedicated evaluation methods for Video QA. To address this
gap, we introduce EVQAScore, a reference-free method that leverages keyword
extraction to assess both video caption and video QA data quality.
Additionally, we incorporate frame sampling and rescaling techniques to enhance
the efficiency and robustness of our evaluation, this enables our score to
evaluate the quality of extremely long videos. Our approach achieves
state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for
Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on
the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using
EVQAScore for data selection, we achieved SOTA results with only 12.5\% of the
original data volume, outperforming the previous SOTA method PAC-S and 100\% of
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LongSafetyBench: Long-Context <span class="highlight-title">LLM</span>s Struggle with Safety Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, <span class="highlight-author">Xipeng Qiu</span>, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), the sequence length of
these models continues to increase, drawing significant attention to
long-context language models. However, the evaluation of these models has been
primarily limited to their capabilities, with a lack of research focusing on
their safety. Existing work, such as ManyShotJailbreak, has to some extent
demonstrated that long-context language models can exhibit safety concerns.
However, the methods used are limited and lack comprehensiveness. In response,
we introduce \textbf{LongSafetyBench}, the first benchmark designed to
objectively and comprehensively evaluate the safety of long-context models.
LongSafetyBench consists of 10 task categories, with an average length of
41,889 words. After testing eight long-context language models on
LongSafetyBench, we found that existing models generally exhibit insufficient
safety capabilities. The proportion of safe responses from most mainstream
long-context LLMs is below 50\%. Moreover, models' safety performance in
long-context scenarios does not always align with that in short-context
scenarios. Further investigation revealed that long-context models tend to
overlook harmful content within lengthy texts. We also proposed a simple yet
effective solution, allowing open-source models to achieve performance
comparable to that of top-tier closed-source models. We believe that
LongSafetyBench can serve as a valuable benchmark for evaluating the safety
capabilities of long-context language models. We hope that our work will
encourage the broader community to pay attention to the safety of long-context
models and contribute to the development of solutions to improve the safety of
long-context LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense
  Question Answering <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boci Peng, Yongchao Liu, Xiaohe Bo, Sheng Tian, Baokun Wang, Chuntao Hong, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense question answering is a crucial task that requires machines to
employ reasoning according to commonsense. Previous studies predominantly
employ an extracting-and-modeling paradigm to harness the information in KG,
which first extracts relevant subgraphs based on pre-defined rules and then
proceeds to design various strategies aiming to improve the representations and
fusion of the extracted structural knowledge. Despite their effectiveness,
there are still two challenges. On one hand, subgraphs extracted by rule-based
methods may have the potential to overlook critical nodes and result in
uncontrollable subgraph size. On the other hand, the misalignment between graph
and text modalities undermines the effectiveness of knowledge fusion,
ultimately impacting the task performance. To deal with the problems above, we
propose a novel framework: \textbf{S}ubgraph R\textbf{E}trieval Enhanced by
Gra\textbf{P}h-\textbf{T}ext \textbf{A}lignment, named \textbf{SEPTA}. Firstly,
we transform the knowledge graph into a database of subgraph vectors and
propose a BFS-style subgraph sampling strategy to avoid information loss,
leveraging the analogy between BFS and the message-passing mechanism. In
addition, we propose a bidirectional contrastive learning approach for
graph-text alignment, which effectively enhances both subgraph retrieval and
knowledge fusion. Finally, all the retrieved information is combined for
reasoning in the prediction module. Extensive experiments on five datasets
demonstrate the effectiveness and robustness of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Multi-Task Learning Architecture for Hate Detection Leveraging
  User-Based Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Kapil, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech, offensive language, aggression, racism, sexism, and other
abusive language are common phenomena in social media. There is a need for
Artificial Intelligence(AI)based intervention which can filter hate content at
scale. Most existing hate speech detection solutions have utilized the features
by treating each post as an isolated input instance for the classification.
This paper addresses this issue by introducing a unique model that improves
hate speech identification for the English language by utilising intra-user and
inter-user-based information. The experiment is conducted over single-task
learning (STL) and multi-task learning (MTL) paradigms that use deep neural
networks, such as convolutional neural networks (CNN), gated recurrent unit
(GRU), bidirectional encoder representations from the transformer (BERT), and A
Lite BERT (ALBERT). We use three benchmark datasets and conclude that combining
certain user features with textual features gives significant improvements in
macro-F1 and weighted-F1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, and two tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating <span class="highlight-title">Large Language Model</span>s on Financial Report <span class="highlight-title">Summarization</span>: An
  Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
versatility across various applications, including natural language
understanding, domain-specific knowledge tasks, etc. However, applying LLMs to
complex, high-stakes domains like finance requires rigorous evaluation to
ensure reliability, accuracy, and compliance with industry standards. To
address this need, we conduct a comprehensive and comparative study on three
state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their
effectiveness in generating automated financial reports. Our primary motivation
is to explore how these models can be harnessed within finance, a field
demanding precision, contextual relevance, and robustness against erroneous or
misleading information. By examining each model's capabilities, we aim to
provide an insightful assessment of their strengths and limitations. Our paper
offers benchmarks for financial report analysis, encompassing proposed metrics
such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative
evaluation framework that integrates both quantitative metrics (e.g.,
precision, recall) and qualitative analyses (e.g., contextual fit, consistency)
to provide a holistic view of each model's output quality. Additionally, we
make our financial dataset publicly available, inviting researchers and
practitioners to leverage, scrutinize, and enhance our findings through broader
community engagement and collaborative improvement. Our dataset is available on
huggingface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of
  Language, Hate Speech, and Targets using <span class="highlight-title">LLM</span>s <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jebish Purbey, Siddartha Pullakhandam, Kanwal Mehreen, Muhammad Arham, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a detailed system description of our entry for the
CHiPSAL 2025 shared task, focusing on language detection, hate speech
identification, and target detection in Devanagari script languages. We
experimented with a combination of large language models and their ensembles,
including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like
focal loss to address challenges in the natural understanding of Devanagari
languages, such as multilingual processing and class imbalance. Our approach
achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804
for Sub-tasks A, B, and C respectively. This work provides insights into the
effectiveness of transformer models in tasks with domain-specific and
linguistic challenges, as well as areas for potential improvement in future
iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Submitted to CHIPSAL workshop @ COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Neo: Parameter Efficient Knowledge <span class="highlight-title">Distill</span>ation for Large Language
  Models <span class="chip">ICASSP 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel LLM-Neo framework that efficiently
transfers knowledge from a large language model (LLM) teacher to a compact
student. Initially, we revisit the knowledge distillation (KD) and low-rank
adaption (LoRA), and argue that they share the same paradigm. Inspired by this
observation, we explore the strategy that combines LoRA and KD to enhance the
efficiency of knowledge transfer. We first summarize some guidelines for this
design and further develop the LLM-Neo. Experimental results on compressing
Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further
analysis demonstrates the robustness of the proposed LLM-Neo on variants of
LoRA. The trained models have been available at
\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this
repository}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 25' under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persuasion with <span class="highlight-title">Large Language Model</span>s: a <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rogiers, Sander Noels, Maarten Buyl, Tijl De Bie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid rise of Large Language Models (LLMs) has created new disruptive
possibilities for persuasive communication, by enabling fully-automated
personalized and interactive content generation at an unprecedented scale. In
this paper, we survey the research field of LLM-based persuasion that has
emerged as a result. We begin by exploring the different modes in which LLM
Systems are used to influence human attitudes and behaviors. In areas such as
politics, marketing, public health, e-commerce, and charitable giving, such LLM
Systems have already achieved human-level or even super-human persuasiveness.
We identify key factors influencing their effectiveness, such as the manner of
personalization and whether the content is labelled as AI-generated. We also
summarize the experimental designs that have been used to evaluate progress.
Our survey suggests that the current and future potential of LLM-based
persuasion poses profound ethical and societal risks, including the spread of
misinformation, the magnification of biases, and the invasion of privacy. These
risks underscore the urgent need for ethical guidelines and updated regulatory
frameworks to avoid the widespread deployment of irresponsible and harmful LLM
Systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of
  Quantization on Model Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of the transformers architecture, LLMs have
revolutionized the NLP field with ever more powerful models. Nevertheless,
their development came up with several challenges. The exponential growth in
computational power and reasoning capabilities of language models has
heightened concerns about their security. As models become more powerful,
ensuring their safety has become a crucial focus in research. This paper aims
to address gaps in the current literature on jailbreaking techniques and the
evaluation of LLM vulnerabilities. Our contributions include the creation of a
novel dataset designed to assess the harmfulness of model outputs across
multiple harm levels, as well as a focus on fine-grained harm-level analysis.
Using this framework, we provide a comprehensive benchmark of state-of-the-art
jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.
Additionally, we examine how quantization techniques, such as AWQ and GPTQ,
influence the alignment and robustness of models, revealing trade-offs between
enhanced robustness with regards to transfer attacks and potential increases in
vulnerability on direct ones. This study aims to demonstrate the influence of
harmful input queries on the complexity of jailbreaking techniques, as well as
to deepen our understanding of LLM vulnerabilities and improve methods for
assessing model robustness when confronted with harmful content, particularly
in the context of compression strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Safe Generative Artificial Intelligence
  (SafeGenAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AssistRAG: Boosting the Potential of <span class="highlight-title">Large Language Model</span>s with an
  Intelligent Information Assistant <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhou, Zheng Liu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as "hallucination". Initial retrieval-augmented
generation (RAG) methods like the "Retrieve-Read" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LA4SR: illuminating the dark proteome with <span class="highlight-title">generative</span> AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Nelson, Ashish Kumar Jaiswal, Noha Ismail, Alexandra Mystikou, Kourosh Salehi-Ashtiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI language models (LMs) show promise for biological sequence analysis. We
re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba,
ranging from 70M to 12B parameters) for microbial sequence classification. The
models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the
recall of BLASTP. They effectively classified the algal dark proteome -
uncharacterized proteins comprising about 65% of total proteins - validated on
new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger
(>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2%
of available data, rapidly achieving strong generalization capacity. High
accuracy was achieved when training data had intact or scrambled terminal
information, demonstrating robust generalization to incomplete sequences.
Finally, we provide custom AI explainability software tools for attributing
amino acid patterns to AI generative processes and interpret their outputs in
evolutionary and biophysical contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale moral machine experiment on <span class="highlight-title">large language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Shahrul Zaim bin Ahmad, Kazuhiro Takemoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) and their potential
integration into autonomous driving systems necessitates understanding their
moral decision-making capabilities. While our previous study examined four
prominent LLMs using the Moral Machine experimental framework, the dynamic
landscape of LLM development demands a more comprehensive analysis. Here, we
evaluate moral judgments across 51 different LLMs, including multiple versions
of proprietary models (GPT, Claude, Gemini) and open-source alternatives
(Llama, Gemma), to assess their alignment with human moral preferences in
autonomous driving scenarios. Using a conjoint analysis framework, we evaluated
how closely LLM responses aligned with human preferences in ethical dilemmas
and examined the effects of model size, updates, and architecture. Results
showed that proprietary models and open-source models exceeding 10 billion
parameters demonstrated relatively close alignment with human judgments, with a
significant negative correlation between model size and distance from human
judgments in open-source models. However, model updates did not consistently
improve alignment with human preferences, and many LLMs showed excessive
emphasis on specific ethical principles. These findings suggest that while
increasing model size may naturally lead to more human-like moral judgments,
practical implementation in autonomous driving systems requires careful
consideration of the trade-off between judgment quality and computational
efficiency. Our comprehensive analysis provides crucial insights for the
ethical design of autonomous systems and highlights the importance of
considering cultural contexts in AI moral decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDC & DM-SFT: A Road for <span class="highlight-title">LLM</span> SQL Bug-Fix Enhancing <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Duan, Yonghong Yu, Xiaoming Zhao, Yichang Wu, Wenbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code Large Language Models (Code LLMs), such as Code llama and
DeepSeek-Coder, have demonstrated exceptional performance in the code
generation tasks. However, most existing models focus on the abilities of
generating correct code, but often struggle with bug repair. We introduce a
suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are
mainly consisted of two parts: A Progressive Dataset Construction (PDC) from
scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data
expansion methods from the perspectives of breadth first and depth first
respectively. DM-SFT introduces an efficient bug-fixing supervised learning
approach, which effectively reduce the total training steps and mitigate the
"disorientation" in SQL code bug-fixing training. In our evaluation, the code
LLM models trained with two methods have exceeds all current best performing
model which size is much larger.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING-Industry 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse <span class="highlight-title">Prompt</span> Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Li, Diego Klabjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a new black-box, zero-shot language model inversion
problem and proposes an innovative framework for prompt reconstruction using
only text outputs from a language model. Leveraging a large language model
alongside an optimization algorithm, the proposed method effectively recovers
prompts with minimal resources. Experimental results on several datasets
derived from public sources indicate that the proposed approach achieves
high-quality prompt recovery and generates prompts more similar to the
originals than current state-of-the-art methods. Additionally, the use-case
study demonstrates the method's strong potential for generating high-quality
text data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Fusion through Bayesian Optimization in <span class="highlight-title">Language Model</span> Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaeyun Jang, Hyungi Lee, Jungtaek Kim, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained models for downstream tasks is a widely adopted
technique known for its adaptability and reliability across various domains.
Despite its conceptual simplicity, fine-tuning entails several troublesome
engineering choices, such as selecting hyperparameters and determining
checkpoints from an optimization trajectory. To tackle the difficulty of
choosing the best model, one effective solution is model fusion, which combines
multiple models in a parameter space. However, we observe a large discrepancy
between loss and metric landscapes during the fine-tuning of pre-trained
language models. Building on this observation, we introduce a novel model
fusion technique that optimizes both the desired metric and loss through
multi-objective Bayesian optimization. In addition, to effectively select
hyperparameters, we establish a two-stage procedure by integrating Bayesian
optimization processes into our framework. Experiments across various
downstream tasks show considerable performance improvements using our Bayesian
optimization-guided method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Should Baby Models Read? Exploring Sample-Efficient Data
  Composition on Model Performance <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Meng Yam, Nathan J Paek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of pre-training data composition on the performance of
small language models in a sample-efficient setting. Using datasets limited to
10 million words, we evaluate several dataset sources, including child-directed
speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and
a mix of these (Mix) across different model sizes ranging from 18 million to
705 million parameters. Our experiments show that smaller models (e.g.,
GPT2-97M, GPT2-705M, Llama-360M) perform better when trained on more complex
and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories
datasets underperformed across all model sizes. These findings suggest that the
optimal dataset for sample efficient training depends on the model size, and
that neither child-directed speech nor simplified stories are optimal for
language models of all sizes. We highlight the importance of considering both
dataset composition and model capacity for effective sample efficient language
model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, CoNLL 2024 (Shared Task) Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge: A Unified Framework to Knowledge Graph Completion via Language
  Models and Knowledge Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Renaissance: Investigating the <span class="highlight-title">Pretrain</span>ing of Vision-Language Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Fields, Casey Kennington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past several years there has been an explosion of available models for
vision-language tasks. Unfortunately, the literature still leaves open a number
of questions related to best practices in designing and training such models.
In this paper we seek to answer several questions related to the pretraining of
vision-language encoders through meta-analysis. In our first set of
experiments, we show that we can save significant compute at no cost to
downstream performance, by freezing large parts of vision-language models
during pretraining. In our second set of experiments we examine the effect of
basing a VL transformer on a vision model versus a text model. Additionally, we
introduce a VL modeling platform called Renaissance that we use to conduct all
of the experiments. This program offers a great deal of flexibility in
creating, training and evaluating transformer encoders for VL modeling. The
source code for Renaissance can be found at
https://github.com/bsu-slim/renaissance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore the Reasoning Capability of <span class="highlight-title">LLM</span>s in the Chess Testbed <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is a central capability of human intelligence. In recent years,
with the advent of large-scale datasets, pretrained large language models have
emerged with new capabilities, including reasoning. However, these models still
struggle with long-term, complex reasoning tasks, such as playing chess. Based
on the observation that expert chess players employ a dual approach combining
long-term strategic play with short-term tactical play along with language
explanation, we propose improving the reasoning capability of large language
models in chess by integrating annotated strategy and tactic. Specifically, we
collect a dataset named MATE, which consists of 1 million chess positions with
candidate moves annotated by chess experts for strategy and tactics. We
finetune the LLaMA-3-8B model and compare it against state-of-the-art
commercial language models in the task of selecting better chess moves. Our
experiments show that our models perform better than GPT, Claude, and Gemini
models. We find that language explanations can enhance the reasoning capability
of large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to NAACL2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Scaling Laws with Statistical and Approximation Theory for
  <span class="highlight-title">Transformer</span> Neural Networks on Intrinsically Low-dimensional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Havrilla, Wenjing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training deep neural networks, a model's generalization error is often
observed to follow a power scaling law dependent both on the model size and the
data size. Perhaps the best known example of such scaling laws are for
transformer-based large language models, where networks with billions of
parameters are trained on trillions of tokens of text. Yet, despite sustained
widespread interest, a rigorous understanding of why transformer scaling laws
exist is still missing. To answer this question, we establish novel statistical
estimation and mathematical approximation theories for transformers when the
input data are concentrated on a low-dimensional manifold. Our theory predicts
a power law between the generalization error and both the training data size
and the network size for transformers, where the power depends on the intrinsic
dimension $d$ of the training data. Notably, the constructed model architecture
is shallow, requiring only logarithmic depth in $d$. By leveraging
low-dimensional data structures under a manifold hypothesis, we are able to
explain transformer scaling laws in a way which respects the data geometry.
Moreover, we test our theory with empirical observation by training LLMs on
natural language datasets. We find the observed empirical data scaling laws
closely agree with our theoretical predictions. Taken together, these results
rigorously show the intrinsic dimension of data to be a crucial quantity
affecting transformer scaling laws in both theory and practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Editing for <span class="highlight-title">LLM</span>s4Code: How Far are We? <span class="chip">ICSE2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Bin Ji, Weimin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models for Code (LLMs4Code) have been found to exhibit
outstanding performance in the software engineering domain, especially the
remarkable performance in coding tasks. However, even the most advanced
LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to
the high cost of training LLMs4Code, it is impractical to re-train the models
for fixing these problematic code knowledge. Model editing is a new technical
field for effectively and efficiently correcting erroneous knowledge in LLMs,
where various model editing techniques and benchmarks have been proposed
recently. Despite that, a comprehensive study that thoroughly compares and
analyzes the performance of the state-of-the-art model editing techniques for
adapting the knowledge within LLMs4Code across various code-related tasks is
notably absent. To bridge this gap, we perform the first systematic study on
applying state-of-the-art model editing approaches to repair the inaccuracy of
LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists
of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and
CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help
of CLMEEval, we evaluate six advanced model editing techniques on three
LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings
include that the external memorization-based GRACE approach achieves the best
knowledge editing effectiveness and specificity (the editing does not influence
untargeted knowledge), while generalization (whether the editing can generalize
to other semantically-identical inputs) is a universal challenge for existing
techniques. Furthermore, building on in-depth case analysis, we introduce an
enhanced version of GRACE called A-GRACE, which incorporates contrastive
learning to better capture the semantics of the inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE2025. The code is available at:
  https://github.com/xpq-tech/code-llmedit.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Untangling Hate Speech Definitions: A Semantic Componential Analysis
  Across Cultures and Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katerina Korre, Arianna Muti, Federico Ruggeri, Alberto Barrón-Cedeño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech relies heavily on cultural influences, leading to varying
individual interpretations. For that reason, we propose a Semantic Componential
Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate
speech definitions. We create the first dataset of definitions derived from
five domains: online dictionaries, research papers, Wikipedia articles,
legislation, and online platforms, which are later analyzed into semantic
components. Our analysis reveals that the components differ from definition to
definition, yet many domains borrow definitions from one another without taking
into account the target culture. We conduct zero-shot model experiments using
our proposed dataset, employing three popular open-sourced LLMs to understand
the impact of different definitions on hate speech detection. Our findings
indicate that LLMs are sensitive to definitions: responses for hate speech
detection change according to the complexity of definitions used in the prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using <span class="highlight-title">Generative</span> AI and Multi-<span class="highlight-title">Agent</span>s to Provide Automatic Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Guo, Ehsan Latif, Yifan Zhou, Xuan Huang, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the use of generative AI and multi-agent systems to
provide automatic feedback in educational contexts, particularly for student
constructed responses in science assessments. The research addresses a key gap
in the field by exploring how multi-agent systems, called AutoFeedback, can
improve the quality of GenAI-generated feedback, overcoming known issues such
as over-praise and over-inference that are common in single-agent large
language models (LLMs). The study developed a multi-agent system consisting of
two AI agents: one for generating feedback and another for validating and
refining it. The system was tested on a dataset of 240 student responses, and
its performance was compared to that of a single-agent LLM. Results showed that
AutoFeedback significantly reduced the occurrence of over-praise and
over-inference errors, providing more accurate and pedagogically sound
feedback. The findings suggest that multi-agent systems can offer a more
reliable solution for generating automated feedback in educational settings,
highlighting their potential for scalable and personalized learning support.
These results have important implications for educators and researchers seeking
to leverage AI in formative assessments, offering a pathway to more effective
feedback mechanisms that enhance student learning outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Context Sensitivity and the Knob Behind It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical
  Concern-related App <span class="highlight-title">Review</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Sorathiya, Gouri Ginde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing proliferation of mobile applications in our everyday
experiences, the concerns surrounding ethics have surged significantly. Users
generally communicate their feedback, report issues, and suggest new
functionalities in application (app) reviews, frequently emphasizing safety,
privacy, and accountability concerns. Incorporating these reviews is essential
to developing successful products. However, app reviews related to ethical
concerns generally use domain-specific language and are expressed using a more
varied vocabulary. Thus making automated ethical concern-related app review
extraction a challenging and time-consuming effort.
  This study proposes a novel Natural Language Processing (NLP) based approach
that combines Natural Language Inference (NLI), which provides a deep
comprehension of language nuances, and a decoder-only (LLaMA-like) Large
Language Model (LLM) to extract ethical concern-related app reviews at scale.
Utilizing 43,647 app reviews from the mental health domain, the proposed
methodology 1) Evaluates four NLI models to extract potential privacy reviews
and compares the results of domain-specific privacy hypotheses with generic
privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to
privacy concerns; and 3) Uses the best NLI and LLM models further to extract
new privacy reviews from the dataset. Results show that the
DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses
yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the
classification of app reviews. Then, using NLI+LLM, an additional 1,008 new
privacy-related reviews were extracted that were not identified through the
keyword-based approach in previous research, thus demonstrating the
effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Optimal Search and Retrieval for RAG <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandria Leto, Cecilia Aguerrebere, Ishwar Bhati, Ted Willke, Mariano Tepper, Vy Ai Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Workshop ATTRIB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isochrony-Controlled Speech-to-Text Translation: A study on translating
  from Sino-Tibetan to Indo-European Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Midia Yousefi, Yao Qian, Junkun Chen, Gang Wang, Yanqing Liu, Dongmei Wang, Xiaofei Wang, Jian Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end speech translation (ST), which translates source language speech
directly into target language text, has garnered significant attention in
recent years. Many ST applications require strict length control to ensure that
the translation duration matches the length of the source audio, including both
speech and pause segments. Previous methods often controlled the number of
words or characters generated by the Machine Translation model to approximate
the source sentence's length without considering the isochrony of pauses and
speech segments, as duration can vary between languages. To address this, we
present improvements to the duration alignment component of our
sequence-to-sequence ST model. Our method controls translation length by
predicting the duration of speech and pauses in conjunction with the
translation process. This is achieved by providing timing information to the
decoder, ensuring it tracks the remaining duration for speech and pauses while
generating the translation. The evaluation on the Zh-En test set of CoVoST 2,
demonstrates that the proposed Isochrony-Controlled ST achieves 0.92 speech
overlap and 8.9 BLEU, which has only a 1.4 BLEU drop compared to the ST
baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeeManc at the PLABA Track of TAC-2024: RoBERTa for task 1 and LLaMA3.1
  and <span class="highlight-title">GPT</span>-4o for task 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhidong Ling, Zihao Li, Pablo Romeo, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report is the system description of the BeeManc team for shared task
Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024. This report
contains two sections corresponding to the two sub-tasks in PLABA 2024. In task
one, we applied fine-tuned ReBERTa-Base models to identify and classify the
difficult terms, jargon and acronyms in the biomedical abstracts and reported
the F1 score. Due to time constraints, we didn't finish the replacement task.
In task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot
prompts to complete the abstract adaptation and reported the scores in BLEU,
SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024
on Task 1A and 1B, our \textbf{much smaller fine-tuned RoBERTa-Base} model
ranked 3rd and 2nd respectively on the two sub-task, and the \textbf{1st on
averaged F1 scores across the two tasks} from 9 evaluated systems. Our share
our fine-tuned models and related resources at
\url{https://github.com/HECTA-UoM/PLABA2024}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work - system report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Span-based Detector for AI-generated Fragments in Scientific
  Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        German Gritsai, Ildar Khabutdinov, Andrey Grabovoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a system designed to distinguish between AI-generated
and human-written scientific excerpts in the DAGPap24 competition hosted within
the Fourth Workshop on Scientific Document Processing. In this competition the
task is to find artificially generated token-level text fragments in documents
of a scientific domain. Our work focuses on the use of a multi-task learning
architecture with two heads. The application of this approach is justified by
the specificity of the task, where class spans are continuous over several
hundred characters. We considered different encoder variations to obtain a
state vector for each token in the sequence, as well as a variation in
splitting fragments into tokens to further feed into the input of a
transform-based encoder. This approach allows us to achieve a 9% quality
improvement relative to the baseline solution score on the development set
(from 0.86 to 0.95) using the average macro F1-score, as well as a score of
0.96 on a closed test part of the dataset from the competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and
  Semantic Robustness of <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bardiya Akhbari, Manish Gawali, Nicholas A. Dronen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Set theory is foundational to mathematics and, when sets are finite, to
reasoning about the world. An intelligent system should perform set operations
consistently, regardless of superficial variations in the operands. Initially
designed for semantically-oriented NLP tasks, large language models (LLMs) are
now being evaluated on algorithmic tasks. Because sets are comprised of
arbitrary symbols (e.g. numbers, words), they provide an opportunity to test,
systematically, the invariance of LLMs' algorithmic abilities under simple
lexical or semantic variations. To this end, we present the SetLexSem
Challenge, a synthetic benchmark that evaluates the performance of LLMs on set
operations. SetLexSem assesses the robustness of LLMs' instruction-following
abilities under various conditions, focusing on the set operations and the
nature and construction of the set members. Evaluating seven LLMs with
SetLexSem, we find that they exhibit poor robustness to variation in both
operation and operands. We show -- via the framework's systematic sampling of
set members along lexical and semantic dimensions -- that LLMs are not only not
robust to variation along these dimensions but demonstrate unique failure modes
in particular, easy-to-create semantic groupings of "deceptive" sets. We find
that rigorously measuring language model robustness to variation in frequency
and length is challenging and present an analysis that measures them
independently. The code for reproducing the results of this paper, and for
generating the SetLexSem Challenge dataset, is available at
\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, NeurIPS 2024 Datasets and Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Richer Output for Richer Countries: Uncovering Geographical Disparities
  in Generated Stories and Travel Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirti Bhagat, Kinshuk Vasisht, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a large body of work inspects language models for biases concerning
gender, race, occupation and religion, biases of geographical nature are
relatively less explored. Some recent studies benchmark the degree to which
large language models encode geospatial knowledge. However, the impact of the
encoded geographical knowledge (or lack thereof) on real-world applications has
not been documented. In this work, we examine large language models for two
common scenarios that require geographical knowledge: (a) travel
recommendations and (b) geo-anchored story generation. Specifically, we study
four popular language models, and across about $100$K travel requests, and
$200$K story generations, we observe that travel recommendations corresponding
to poorer countries are less unique with fewer location references, and stories
from these regions more often convey emotions of hardship and sadness compared
to those from wealthier nations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR - October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Surprising Effectiveness of Test-Time Training for Abstract
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown impressive performance on tasks within their
training distribution, but often struggle with novel problems requiring complex
reasoning. We investigate the effectiveness of test-time training (TTT) --
updating model parameters temporarily during inference using a loss derived
from input data -- as a mechanism for improving models' reasoning capabilities,
using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through
systematic experimentation, we identify three crucial components for successful
TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and
augmentations (3) per-instance training. TTT significantly improves performance
on ARC tasks, achieving up to 6x improvement in accuracy compared to base
fine-tuned models; applying TTT to an 8B-parameter language model, we achieve
53% accuracy on the ARC's public validation set, improving the state-of-the-art
by nearly 25% for public and purely neural approaches. By ensembling our method
with recent program generation approaches, we get SoTA public validation
accuracy of 61.9%, matching the average human score. Our findings suggest that
explicit symbolic search is not the only path to improved abstract reasoning in
neural language models; additional test-time applied to continued training on
few-shot examples can also be extremely effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding <span class="highlight-title">Thought</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Qingping Yang, Runtao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and adaptability. This
paper introduces the UTMath Benchmark, which robustly evaluates the models
through extensive unit tests. It consists of 1,053 problems across 9
mathematical domains, with over 68 test cases per problem. We propose an
innovative evaluation framework inspired by unit testing in software
development, focusing on both accuracy and reliability of results. Furthermore,
we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which
encourages LLMs to perform explicit reasoning before generating code, leading
to generating more advanced solution and improved performance. Furthermore, we
are releasing not only the UTMath benchmark but also the UTMath-Train training
dataset (more than 70k samples), to support the community in further exploring
mathematical reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Random Baselines for In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Yauney, David Mimno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the in-context learning classification performance of language
models poses challenges due to small dataset sizes, extensive prompt-selection
using the validation set, and intentionally difficult tasks that lead to
near-random performance. The standard random baseline--the expected accuracy of
guessing labels uniformly at random--is stable when the evaluation set is used
only once or when the dataset is large. We account for the common practice of
validation set reuse and existing small datasets with a stronger random
baseline: the expected maximum accuracy across multiple random classifiers.
When choosing the best prompt demonstrations across six quantized language
models applied to 16 BIG-bench Lite tasks, more than 20% of the few-shot
results that exceed the standard baseline do not exceed this stronger random
baseline. When held-out test sets are available, this stronger baseline is also
a better predictor of held-out performance than the standard baseline, avoiding
unnecessary test set evaluations. This maximum random baseline provides an
easily calculated drop-in replacement for the standard baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving to the Future: Unseen <span class="highlight-title">Event</span> Adaptive Fake News Detection on
  Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Zhixun Li, Qiang Liu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of social media, the wide dissemination of fake
news on social media is increasingly threatening both individuals and society.
One of the unique challenges for fake news detection on social media is how to
detect fake news on future events. Recently, numerous fake news detection
models that utilize textual information and the propagation structure of posts
have been proposed. Unfortunately, most of the existing approaches can hardly
handle this challenge since they rely heavily on event-specific features for
prediction and cannot generalize to unseen events. To address this, we
introduce \textbf{F}uture \textbf{AD}aptive \textbf{E}vent-based Fake news
Detection (FADE) framework. Specifically, we train a target predictor through
an adaptive augmentation strategy and graph contrastive learning to obtain
higher-quality features and make more accurate overall predictions.
Simultaneously, we independently train an event-only predictor to obtain biased
predictions. We further mitigate event bias by subtracting the event-only
predictor's output from the target predictor's output to obtain the final
prediction. Encouraging results from experiments designed to emulate real-world
social media conditions validate the effectiveness of our method in comparison
to existing state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reminding Multimodal <span class="highlight-title">Large Language Model</span>s of Object-aware Knowledge
  with Retrieved Tags 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in the general visual instruction-following ability
of Multimodal Large Language Models (MLLMs), they still struggle with critical
problems when required to provide a precise and detailed response to a visual
instruction: (1) failure to identify novel objects or entities, (2) mention of
non-existent objects, and (3) neglect of object's attributed details. Intuitive
solutions include improving the size and quality of data or using larger
foundation models. They show effectiveness in mitigating these issues, but at
an expensive cost of collecting a vast amount of new data and introducing a
significantly larger model. Standing at the intersection of these approaches,
we examine the three object-oriented problems from the perspective of the
image-to-text mapping process by the multimodal connector. In this paper, we
first identify the limitations of multimodal connectors stemming from
insufficient training data. Driven by this, we propose to enhance the mapping
with retrieval-augmented tag tokens, which contain rich object-aware
information such as object names and attributes. With our Tag-grounded visual
instruction tuning with retrieval Augmentation (TUNA), we outperform baselines
that share the same language model and training data on 12 benchmarks.
Furthermore, we show the zero-shot capability of TUNA when provided with
specific datastores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2.5-Coder Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we introduce the Qwen2.5-Coder series, a significant upgrade
from its predecessor, CodeQwen1.5. This series includes six models:
Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,
Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained
on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,
scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder
demonstrates impressive code generation capabilities while retaining general
and math skills. These models have been evaluated on a wide range of
code-related tasks, achieving state-of-the-art (SOTA) performance across more
than 10 benchmarks, including code generation, completion, reasoning, and
repair, consistently outperforming larger models of the same model size. We
believe that the release of the Qwen2.5-Coder series will advance research in
code intelligence and, with its permissive licensing, support wider adoption by
developers in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDR: Customizable Density Ratios of Strong-over-weak <span class="highlight-title">LLM</span>s for Preference
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference tuning of large language models (LLMs) relies on high-quality
human preference data, which is often expensive and time-consuming to gather.
While existing methods can use trained reward models or proprietary model as
judges for preference annotation, they have notable drawbacks: training reward
models remain dependent on initial human data, and using proprietary model
imposes license restrictions that inhibits commercial usage. In this paper, we
introduce customized density ratio (CDR), a training-free and highly effective
method that leverages off-the-shelf LLMs for preference data annotation. Our
approach uses the log-density ratio between a better-aligned LLM and a less
aligned LLM as a reward signal. We explores 221 different LLMs pairs and
empirically demonstrate that increasing the performance gap between paired LLMs
correlates with better reward generalization. Furthermore, we show that
tailoring the density ratio reward function with specific criteria and
preference exemplars enhances performance across domains and within target
areas.
  In our experiment using density ratio from a pair of Mistral-7B models, CDR
achieves a RewardBench score of 82.6, outperforming the best trained reward
functions from same model class and demonstrating competitive performance
against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR
to annotate an on-policy preference dataset with which we preference tune
Llama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak
models, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on
ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,
along with a score of 8.0 on MT-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hire Me or Not? Examining <span class="highlight-title">Language Model</span>'s Behavior with Occupation
  Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the impressive performance in various downstream tasks, large language
models (LLMs) have been widely integrated into production pipelines, like
recruitment and recommendation systems. A known issue of models trained on
natural language data is the presence of human biases, which can impact the
fairness of the system. This paper investigates LLMs' behavior with respect to
gender stereotypes, in the context of occupation decision making. Our framework
is designed to investigate and quantify the presence of gender stereotypes in
LLMs' behavior via multi-round question answering. Inspired by prior works, we
construct a dataset by leveraging a standard occupation classification
knowledge base released by authoritative agencies. We tested three LLMs
(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models
exhibit gender stereotypes analogous to human biases, but with different
preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may
imply the current alignment methods are insufficient for debiasing and could
introduce new biases contradicting the traditional gender stereotypes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to
  Address Shortcut Shifts in Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ukyo Honda, Tatsushi Oka, Peinan Zhang, Masato Mita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent models for natural language understanding are inclined to exploit
simple patterns in datasets, commonly known as shortcuts. These shortcuts hinge
on spurious correlations between labels and latent features existing in the
training data. At inference time, shortcut-dependent models are likely to
generate erroneous predictions under distribution shifts, particularly when
some latent features are no longer correlated with the labels. To avoid this,
previous studies have trained models to eliminate the reliance on shortcuts. In
this study, we explore a different direction: pessimistically aggregating the
predictions of a mixture-of-experts, assuming each expert captures relatively
different latent features. The experimental results demonstrate that our
post-hoc control over the experts significantly enhances the model's robustness
to the distribution shift in shortcuts. Besides, we show that our approach has
some practical advantages. We also analyze our model and provide results to
support the assumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures (the layout differs from the MIT Press
  publication version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Augmentation: Self-Supervised Learning with Transformations in
  Activation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Deep Augmentation, an approach to implicit data augmentation
using dropout or PCA to transform a targeted layer within a neural network to
improve performance and generalization. We demonstrate Deep Augmentation
through extensive experiments on contrastive learning tasks in NLP, computer
vision, and graph learning. We observe substantial performance gains with
Transformers, ResNets, and Graph Neural Networks as the underlying models in
contrastive learning, but observe inverse effects on the corresponding
supervised problems. Our analysis suggests that Deep Augmentation alleviates
co-adaptation between layers, a problem exhibited by self-supervised learning
where ground truth labels are not available. We use this observation to
formulate a method for selecting which layer to target; in particular, our
experimentation reveals that targeting deeper layers with Deep Augmentation
outperforms augmenting the input data. The simple network- and
modality-agnostic nature of this approach enables its integration into various
machine learning pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances in Named Entity Recognition: A Comprehensive <span class="highlight-title">Survey</span> and
  Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imed Keraghel, Stanislas Morbieu, Mohamed Nadif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition seeks to extract substrings within a text that name
real-world objects and to determine their type (for example, whether they refer
to persons or organizations). In this survey, we first present an overview of
recent popular approaches, including advancements in Transformer-based methods
and Large Language Models (LLMs) that have not had much coverage in other
surveys. In addition, we discuss reinforcement learning and graph-based
approaches, highlighting their role in enhancing NER performance. Second, we
focus on methods designed for datasets with scarce annotations. Third, we
evaluate the performance of the main NER implementations on a variety of
datasets with differing characteristics (as regards their domain, their size,
and their number of classes). We thus provide a deep comparison of algorithms
that have never been considered together. Our experiments shed some light on
how the characteristics of datasets affect the behavior of the methods we
compare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic and Textual Graph Generation Via Large-Scale <span class="highlight-title">LLM</span>-based <span class="highlight-title">Agent</span>
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09824v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09824v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph generation is a fundamental task that has been extensively studied in
social, technological, and scientific analysis. For modeling the dynamic graph
evolution process, traditional rule-based methods struggle to capture community
structures within graphs, while deep learning methods only focus on fitting
training graphs. This limits existing graph generators to producing graphs that
adhere to predefined rules or closely resemble training datasets, achieving
poor performance in dynamic graph generation. Given that graphs are abstract
representations arising from pairwise interactions in human activities, a
realistic simulation of human-wise interaction could provide deeper insights
into the graph evolution mechanism. With the increasing recognition of large
language models (LLMs) in simulating human behavior, we introduce
GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic
graph generation. Without training or fine-tuning process of LLM, our framework
effectively replicates seven macro-level structural characteristics in
established network science theories while surpassing existing baselines in
graph expansion tasks by 31\% on specific evaluation metrics. Through node
classification task, we validate GAG effectively preserves characteristics of
real-world network for node-wise textual features in generated text-rich graph.
Furthermore, by incorporating parallel acceleration, GAG supports generating
graphs with up to nearly 100,000 nodes or 10 million edges through large-scale
LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code
is available at https://anonymous.4open.science/r/GraphAgent-2206.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pediatrics<span class="highlight-title">GPT</span>: <span class="highlight-title">Large Language Model</span>s as Chinese Medical Assistants for
  Pediatric Applications <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19266v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19266v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical
  Large Language Model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning <span class="highlight-title">LLM</span>s for FL-free Program Repair <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs can locate and repair bugs in certain functions using the related
artifacts (e.g., test cases), existing methods still depend on statement-level
fault localization methods to provide a list of buggy hunks for repair. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first identifying faulty statements. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-buggy-hunks-then-repair
workflow with direct debugging is more effective for LLM-based APR methods.
Thus, we believe this paper introduces a new mindset for harnessing LLMs in
APR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UstanceBR: a social media language resource for stance prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06374v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06374v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camila Pereira, Matheus Pavan, Sungwon Yoon, Ricelli Ramos, Pablo Costa, Lais Cavalheiro, Ivandre Paraboni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Diversity Matters for Robust Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo Zhao, Haoming Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that by curating high quality and diverse instruction
tuning datasets, we can significantly improve instruction-following
capabilities. However, creating such datasets is difficult and most works rely
on manual curation or proprietary language models. Automatic data curation is
difficult as it is still not clear how we can define diversity for instruction
tuning, how diversity and quality depend on one other, and how we can optimize
dataset quality and diversity. To resolve these issue, we propose a new
algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple
method to simultaneously control dataset diversity and quality, allowing us to
conduct an in-depth study on the effect of diversity and quality on instruction
tuning performance. From this study we draw two key insights (1) there is a
natural tradeoff between data diversity and quality and (2) increasing data
diversity significantly improves the worst case instruction following
performance, therefore improving robustness. We validate the performance of
QDIT on several large scale instruction tuning datasets, where we find it can
substantially improve worst and average case performance compared to
quality-driven data selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small <span class="highlight-title">Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Lexical Representation for Interpretable Visual-Language
  Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's
groundbreaking work. Although CLIP performs well, the typical direct latent
feature alignment lacks clarity in its representation and similarity scores. On
the other hand, lexical representation, a vector whose element represents the
similarity between the sample and a word from the vocabulary, is a natural
sparse representation and interpretable, providing exact matches for individual
words. However, lexical representations are difficult to learn due to no
ground-truth supervision and false-discovery issues, and thus requires complex
design to train effectively. In this paper, we introduce LexVLA, a more
interpretable VLA framework by learning a unified lexical representation for
both modalities without complex design. We use DINOv2 as our visual model for
its local-inclined features and Llama 2, a generative language model, to
leverage its in-context lexical prediction ability. To avoid the false
discovery, we propose an overuse penalty to refrain the lexical representation
from falsely frequently activating meaningless words. We demonstrate that these
two pre-trained uni-modal models can be well-aligned by fine-tuning on the
modest multi-modal dataset and avoid intricate training configurations. On
cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal
dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M)
and those trained from scratch on even bigger datasets (e.g., 1.1B data,
including CC-12M). We conduct extensive experiments to analyze LexVLA. Codes
are available at https://github.com/Clementine24/LexVLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual
  <span class="highlight-title">Language Model</span>ing <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, Luke Zettlemoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major consideration in multilingual language modeling is how to best
represent languages with diverse vocabularies and scripts. Although
contemporary text encoding methods cover most of the world's writing systems,
they exhibit bias towards the high-resource languages of the Global West. As a
result, texts of underrepresented languages tend to be segmented into long
sequences of linguistically meaningless units. To address the disparities, we
introduce a new paradigm that encodes the same information with segments of
consistent size across diverse languages. Our encoding convention (MYTE) is
based on morphemes, as their inventories are more balanced across languages
than characters, which are used in previous methods. We show that MYTE produces
shorter encodings for all 99 analyzed languages, with the most notable
improvements for non-European languages and non-Latin scripts. This, in turn,
improves multilingual LM performance and diminishes the perplexity gap
throughout diverse languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Narrative Analysis: Unraveling Perceptions of <span class="highlight-title">Generative</span> AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Written texts reflect an author's perspective, making the thorough analysis
of literature a key research method in fields such as the humanities and social
sciences. However, conventional text mining techniques like sentiment analysis
and topic modeling are limited in their ability to capture the hierarchical
narrative structures that reveal deeper argumentative patterns. To address this
gap, we propose a method that leverages large language models (LLMs) to extract
and organize these structures into a hierarchical framework. We validate this
approach by analyzing public opinions on generative AI collected by Japan's
Agency for Cultural Affairs, comparing the narratives of supporters and
critics. Our analysis provides clearer visualization of the factors influencing
divergent opinions on generative AI, offering deeper insights into the
structures of agreement and disagreement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Meaningful Learning: Enhancing Abstract Reasoning in Large Language
  Models via Generic Fact Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Xiong, Xiao Ding, <span class="highlight-author">Ting Liu</span>, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have developed impressive performance and strong
explainability across various reasoning scenarios, marking a significant stride
towards mimicking human-like intelligence. Despite this, when tasked with
several simple questions supported by a generic fact, LLMs often struggle to
abstract and apply the generic fact to provide consistent and precise answers,
revealing a deficiency in abstract reasoning abilities. This has sparked a
vigorous debate about whether LLMs are genuinely reasoning or merely
memorizing. In light of this, we design a preliminary study to quantify and
delve into the abstract reasoning abilities of existing LLMs. Our findings
reveal a substantial discrepancy between their general reasoning and abstract
reasoning performances. To relieve this problem, we tailor an abstract
reasoning dataset (AbsR) together with a meaningful learning paradigm to teach
LLMs how to leverage generic facts for reasoning purposes. The results show
that our approach not only boosts the general reasoning performance of LLMs but
also makes considerable strides towards their capacity for abstract reasoning,
moving beyond simple memorization or imitation to a more nuanced understanding
and application of generic facts. The code is available at
https://github.com/Waste-Wood/MeanLearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMLPA: <span class="highlight-title">Language Model</span> Linguistic Personality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Zheng, Xian Wang, Simo Hosio, Xiaoxian Xu, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in everyday life and
research. One of the most common use cases is conversational interactions,
enabled by the language generation capabilities of LLMs. Just as between two
humans, a conversation between an LLM-powered entity and a human depends on the
personality of the conversants. However, measuring the personality of a given
LLM is currently a challenge. This paper introduces the Language Model
Linguistic Personality Assessment (LMLPA), a system designed to evaluate the
linguistic personalities of LLMs. Our system helps to understand LLMs' language
generation capabilities by quantitatively assessing the distinct personality
traits reflected in their linguistic outputs. Unlike traditional human-centric
psychometrics, the LMLPA adapts a personality assessment questionnaire,
specifically the Big Five Inventory, to align with the operational capabilities
of LLMs, and also incorporates the findings from previous language-based
personality measurement literature. To mitigate sensitivity to the order of
options, our questionnaire is designed to be open-ended, resulting in textual
answers. Thus, the AI rater is needed to transform ambiguous personality
information from text responses into clear numerical indicators of personality
traits. Utilising Principal Component Analysis and reliability validations, our
findings demonstrate that LLMs possess distinct personality traits that can be
effectively quantified by the LMLPA. This research contributes to
Human-Computer Interaction and Human-Centered AI, providing a robust framework
for future studies to refine AI personality assessments and expand their
applications in multiple areas, including education and manufacturing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing research literature attitude towards Sustainable Development
  Goals: an <span class="highlight-title">LLM</span>-based topic modeling approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world is facing a multitude of challenges that hinder the development of
human civilization and the well-being of humanity on the planet. The
Sustainable Development Goals (SDGs) were formulated by the United Nations in
2015 to address these global challenges by 2030. Natural language processing
techniques can help uncover discussions on SDGs within research literature. We
propose a completely automated pipeline to 1) fetch content from the Scopus
database and prepare datasets dedicated to five groups of SDGs; 2) perform
topic modeling, a statistical technique used to identify topics in large
collections of textual data; and 3) enable topic exploration through
keywords-based search and topic frequency time series extraction. For topic
modeling, we leverage the stack of BERTopic scaled up to be applied on large
corpora of textual documents (we find hundreds of topics on hundreds of
thousands of documents), introducing i) a novel LLM-based embeddings
computation for representing scientific abstracts in the continuous space and
ii) a hyperparameter optimizer to efficiently find the best configuration for
any new big datasets. We additionally produce the visualization of results on
interactive dashboards reporting topics' temporal evolution. Results are made
inspectable and explorable, contributing to the interpretability of the topic
modeling process. Our proposed LLM-based topic modeling pipeline for big-text
datasets allows users to capture insights on the evolution of the attitude
toward SDGs within scientific abstracts in the 2006-2023 time span. All the
results are reproducible by using our system; the workflow can be generalized
to be applied at any point in time to any big corpus of textual documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EHRNoteQA: An <span class="highlight-title">LLM</span> Benchmark for Real-World Clinical Practice Using
  Discharge Summaries <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16040v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16040v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discharge summaries in Electronic Health Records (EHRs) are crucial for
clinical decision-making, but their length and complexity make information
extraction challenging, especially when dealing with accumulated summaries
across multiple patient admissions. Large Language Models (LLMs) show promise
in addressing this challenge by efficiently analyzing vast and complex data.
Existing benchmarks, however, fall short in properly evaluating LLMs'
capabilities in this context, as they typically focus on single-note
information or limited topics, failing to reflect the real-world inquiries
required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel
benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each
linked to distinct patients' discharge summaries. Every QA pair is initially
generated using GPT-4 and then manually reviewed and refined by three
clinicians to ensure clinical relevance. EHRNoteQA includes questions that
require information across multiple discharge summaries and covers eight
diverse topics, mirroring the complexity and diversity of real clinical
inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice
question answering, and propose a reliable evaluation method for each. We
evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the
model performance (e.g., the length and number of discharge summaries).
Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations
in clinical practice, we measure the correlation between the LLM performance on
EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results
show that LLM performance on EHRNoteQA have higher correlation with
clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to
other benchmarks, demonstrating its practical relevance in evaluating LLMs in
clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Datasets and Benchmarks)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Extraction from High-Level Corruption Schemes via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of financial crime that has been observed in recent years has
created an increasing concern around the topic and many people, organizations
and governments are more and more frequently trying to combat it. Despite the
increase of interest in this area, there is a lack of specialized datasets that
can be used to train and evaluate works that try to tackle those problems. This
article proposes a new micro-benchmark dataset for algorithms and models that
identify individuals and organizations, and their multiple writings, in news
articles, and presents an approach that assists in its creation. Experimental
efforts are also reported, using this dataset, to identify individuals and
organizations in financial-crime-related articles using various low-billion
parameter Large Language Models (LLMs). For these experiments, standard metrics
(Accuracy, Precision, Recall, F1 Score) are reported and various prompt
variants comprising the best practices of prompt engineering are tested. In
addition, to address the problem of ambiguous entity mentions, a simple, yet
effective LLM-based disambiguation method is proposed, ensuring that the
evaluation aligns with reality. Finally, the proposed approach is compared
against a widely used state-of-the-art open-source baseline, showing the
superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Shape of Word Embeddings: Quantifying Non-Isometry With Topological
  Data Analysis <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondřej Draganov, Steven Skiena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embeddings represent language vocabularies as clouds of $d$-dimensional
points. We investigate how information is conveyed by the general shape of
these clouds, instead of representing the semantic meaning of each token.
Specifically, we use the notion of persistent homology from topological data
analysis (TDA) to measure the distances between language pairs from the shape
of their unlabeled embeddings. These distances quantify the degree of
non-isometry of the embeddings. To distinguish whether these differences are
random training errors or capture real information about the languages, we use
the computed distance matrices to construct language phylogenetic trees over 81
Indo-European languages. Careful evaluation shows that our reconstructed trees
exhibit strong and statistically-significant similarities to the reference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EMNLP Findings 2024. The code used is shared on GitHub:
  https://github.com/OnDraganov/shape-of-word-embeddings. URL of the
  publication: "https://aclanthology.org/2024.findings-emnlp.705"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction Tuning for <span class="highlight-title">Large Language Model</span>s: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), which can also be referred to as supervised
fine-tuning (SFT)\footnote{In this paper, unless specified otherwise,
instruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a
crucial technique to enhance the capabilities and controllability of large
language models (LLMs). Instruction tuning refers to the process of further
training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs
in a supervised fashion, which bridges the gap between the next-word prediction
objective of LLMs and the users' objective of having LLMs adhere to human
instructions. In this work, we make a systematic review of the literature,
including the general methodology of IT, the construction of IT datasets, the
training of IT models, and applications to different modalities, domains and
application, along with analysis on aspects that influence the outcome of IT
(e.g., generation of instruction outputs, size of the instruction dataset,
etc). We also review the potential pitfalls of IT along with criticism against
it, along with efforts pointing out current deficiencies of existing strategies
and suggest some avenues for fruitful research.Project page:
github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V4; Last update: Nov 11, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Provable Length and Compositional Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04875v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04875v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Ahuja, Amin Mansouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution generalization capabilities of sequence-to-sequence
models can be studied from the lens of two crucial forms of generalization:
length generalization -- the ability to generalize to longer sequences than
ones seen during training, and compositional generalization: the ability to
generalize to token combinations not seen during training. In this work, we
provide first provable guarantees on length and compositional generalization
for common sequence-to-sequence models -- deep sets, transformers, state space
models, and recurrent neural nets -- trained to minimize the prediction error.
Taking a first principles perspective, we study the realizable case, i.e., the
labeling function is realizable on the architecture. We show that \emph{simple
limited capacity} versions of these different architectures achieve both length
and compositional generalization. In all our results across different
architectures, we find that the learned representations are linearly related to
the representations generated by the true labeling function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CUDRT: Benchmarking the Detection Models of Human vs. Large Language
  Models Generated Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tao, Yanfang Chen, Dinghao Xi, Zhiyu Li, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have greatly enhanced text generation
across industries, their human-like outputs make distinguishing between human
and AI authorship challenging. Although many LLM-generated text detectors
exist, current benchmarks mainly rely on static datasets, limiting their
effectiveness in assessing model-based detectors requiring prior training.
Furthermore, these benchmarks focus on specific scenarios like question
answering and text refinement and are primarily limited to English, overlooking
broader linguistic applications and LLM subtleties. To address these gaps, we
construct a comprehensive bilingual benchmark in Chinese and English to
rigorously evaluate mainstream LLM-generated text detection methods. We
categorize LLM text generation into five key operations-Create, Update, Delete,
Rewrite, and Translate (CUDRT)-covering the full range of LLM activities. For
each CUDRT category, we developed extensive datasets enabling thorough
assessment of detection performance, incorporating the latest mainstream LLMs
for each language. We also establish a robust evaluation framework to support
scalable, reproducible experiments, facilitating an in-depth analysis of how
LLM operations, different LLMs, datasets, and multilingual training sets impact
detector performance, particularly for model-based methods. Our extensive
experiments provide critical insights for optimizing LLM-generated text
detectors and suggest future directions to improve detection accuracy and
generalization across diverse scenarios.Source code and dataset are available
at GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALPINE: Unveiling the Planning Capability of Autoregressive Learning in
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning is a crucial element of both human intelligence and contemporary
large language models (LLMs). In this paper, we initiate a theoretical
investigation into the emergence of planning capabilities in Transformer-based
LLMs via their next-word prediction mechanisms. We model planning as a network
path-finding task, where the objective is to generate a valid path from a
specified source node to a designated target node. Our mathematical
characterization shows that Transformer architectures can execute path-finding
by embedding the adjacency and reachability matrices within their weights.
Furthermore, our theoretical analysis of gradient-based learning dynamics
reveals that LLMs can learn both the adjacency and a limited form of the
reachability matrices. These theoretical insights are then validated through
experiments, which demonstrate that Transformer architectures indeed learn the
adjacency and an incomplete reachability matrices, consistent with our
theoretical predictions. When applying our methodology to the real-world
planning benchmark Blocksworld, our observations remain consistent.
Additionally, our analyses uncover a fundamental limitation of current
Transformer architectures in path-finding: these architectures cannot identify
reachability relationships through transitivity, which leads to failures in
generating paths when concatenation is required. These findings provide new
insights into how the internal mechanisms of autoregressive learning facilitate
intelligent planning and deepen our understanding of how future LLMs might
achieve more advanced and general planning-and-reasoning capabilities across
diverse applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-to-Defer for Extractive Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have profoundly impacted the field of extractive
question-answering, leveraging large-scale textual corpora to enhance
contextual language understanding. Despite their success, these models struggle
in complex scenarios that demand nuanced interpretation or inferential
reasoning beyond immediate textual cues. Furthermore, their size poses
deployment challenges on resource-constrained devices. Addressing these
limitations, we introduce an adapted two-stage Learning-to-Defer mechanism that
enhances decision-making by enabling selective deference to human experts or
larger models without retraining language models in the context of
question-answering. This approach not only maintains computational efficiency
but also significantly improves model reliability and accuracy in ambiguous
contexts. We establish the theoretical soundness of our methodology by proving
Bayes and $(\mathcal{H}, \mathcal{R})$--consistency of our surrogate loss
function, guaranteeing the optimality of the final solution. Empirical
evaluations on the SQuADv2 dataset illustrate performance gains from
integrating human expertise and leveraging larger models. Our results further
demonstrate that deferring a minimal number of queries allows the smaller model
to achieve performance comparable to their larger counterparts while preserving
computing efficiency, thus broadening the applicability of pre-trained language
models in diverse operational environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 17 main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fast Multilingual <span class="highlight-title">LLM</span> Inference: Speculative Decoding and
  Specialized Drafters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized natural language processing
and broadened their applicability across diverse commercial applications.
However, the deployment of these models is constrained by high inference time
in multilingual settings. To mitigate this challenge, this paper explores a
training recipe of an assistant model in speculative decoding, which is
leveraged to draft and-then its future tokens are verified by the target LLM.
We show that language-specific draft models, optimized through a targeted
pretrain-and-finetune strategy, substantially brings a speedup in inference
time compared to the previous methods. We validate these models across various
languages in inference time, out-of-domain speedup, and GPT-4o evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoCar: A Relationship Network-based Evaluation Method for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Wang, Wenfang Wu, Chongyun Gao, Daling Wang, Shi Feng, Yifei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have received increasing attention. However, due
to the complexity of its capabilities, how to rationally evaluate the
capabilities of LLMs is still a task to be solved. We propose the RoCar method,
which utilizes the defined basic schemas to randomly construct a task graph and
generates natural language evaluation tasks based on the task graph to evaluate
the reasoning and memory abilities of LLMs respectively. Due to the very large
randomness of the task construction process, it is possible to ensure that none
of the LLMs to be tested has directly learned the evaluation tasks,
guaranteeing the fairness of the evaluation method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FiSTECH: Financial Style Transfer to Enhance Creativity without
  Hallucinations in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohini Roychowdhury, Marko Krema, Brian Moore, Xingjian Lai, Dike Effedua, Bharat Jethwani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent trends in Generative AI have emerged towards fine-tuning foundational
large language models (LLMs) to create domain-specific LLMs for automation and
chatbot-like applications. Specialized applications for analytics-heavy domains
such as Financial report generation require specific writing styles that
comprise compound and creative sentences with minimized hallucinations. In this
work, we explore the self-corrective auto-regressive qualities of LLMs to learn
creativity in writing styles with minimal prompting. We propose a novel
two-stage fine-tuning (FT) strategy wherein in the first stage public domain
financial reports are used to train for writing styles while allowing the LLM
to hallucinate. In the second stage the examples of hallucinations are manually
corrected and further used to fine-tune the LLM. The finally trained LLM learns
to generate specific financial report sections using minimal instructions and
tabular data inputs while ensuring low fine-tuning costs. Our proposed
two-stage fine-tuning boosts the accuracy of financial questions answering by
two-folds while reducing hallucinations by over 50%. Also, the fine-tuned model
has lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity
and knowledge density with lower uncertainty and cross entropy than base LLMs.
Thus, the proposed framework can be generalized to train creativity in LLMs by
first allowing them to hallucinate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 figures, 5 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-<span class="highlight-title">LLM</span>: A Benchmark <span class="highlight-title">Dataset</span> for Understanding <span class="highlight-title">Large Language Model</span>
  Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14038v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14038v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPRING Lab IITM's submission to Low Resource Indic Language Translation
  Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamees Sayed, Advait Joglekar, Srinivasan Umesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a robust translation model for four low-resource Indic languages:
Khasi, Mizo, Manipuri, and Assamese. Our approach includes a comprehensive
pipeline from data collection and preprocessing to training and evaluation,
leveraging data from WMT task datasets, BPCC, PMIndia, and OpenLanguageData. To
address the scarcity of bilingual data, we use back-translation techniques on
monolingual datasets for Mizo and Khasi, significantly expanding our training
corpus. We fine-tune the pre-trained NLLB 3.3B model for Assamese, Mizo, and
Manipuri, achieving improved performance over the baseline. For Khasi, which is
not supported by the NLLB model, we introduce special tokens and train the
model on our Khasi corpus. Our training involves masked language modelling,
followed by fine-tuning for English-to-Indic and Indic-to-English translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in WMT 2024. Low-Resource Indic Language Translation Shared
  Task</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DetectBench: Can <span class="highlight-title">Large Language Model</span> Detect and Piece Together Implicit
  Evidence? <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting evidence within the context is a key step in the process of
reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence
detection will strengthen context-based reasoning performance. This paper
proposes a benchmark called DetectBench for verifying the ability to detect and
piece together implicit evidence within a long context. DetectBench contains
3,928 multiple-choice questions, with an average of 994 tokens per question.
Each question contains an average of 4.55 pieces of implicit evidence, and
solving the problem typically requires 7.62 logical jumps to find the correct
answer. To enhance the performance of LLMs in evidence detection, this paper
proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that
the existing LLMs' abilities to detect evidence in long contexts are far
inferior to humans. However, the Detective Reasoning Prompt effectively
enhances the capability of powerful LLMs in evidence detection, while the
Finetuning method shows significant effects in enhancing the performance of
weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are
improved, their final reasoning performance is also enhanced accordingly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video <span class="highlight-title">Summarization</span>: Towards Entity-Aware Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hammad A. Ayyubi, Tianqi Liu, Arsha Nagrani, Xudong Lin, Mingda Zhang, Anurag Arnab, Feng Han, Yukun Zhu, Jialu Liu, Shih-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing popular video captioning benchmarks and models deal with generic
captions devoid of specific person, place or organization named entities. In
contrast, news videos present a challenging setting where the caption requires
such named entities for meaningful summarization. As such, we propose the task
of summarizing news video directly to entity-aware captions. We also release a
large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task.
Further, we propose a method that augments visual information from videos with
context retrieved from external world knowledge to generate entity-aware
captions. We demonstrate the effectiveness of our approach on three video
captioning models. We also show that our approach generalizes to existing news
image captions dataset. With all the extensive experiments and insights, we
believe we establish a solid basis for future research on this challenging
task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hidden Persuaders: <span class="highlight-title">LLM</span>s' Political Leaning and Their Influence on Voters <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How could LLMs influence our democracy? We investigate LLMs' political
leanings and the potential influence of LLMs on voters by conducting multiple
experiments in a U.S. presidential election context. Through a voting
simulation, we first demonstrate 18 open- and closed-weight LLMs' political
preference for a Democratic nominee over a Republican nominee. We show how this
leaning towards the Democratic nominee becomes more pronounced in
instruction-tuned models compared to their base versions by analyzing their
responses to candidate-policy related questions. We further explore the
potential impact of LLMs on voter choice by conducting an experiment with 935
U.S. registered voters. During the experiments, participants interacted with
LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results
show a shift in voter choices towards the Democratic nominee following LLM
interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs
were not asked to persuade users to support the Democratic nominee during the
discourse. This effect is larger than many previous studies on the
persuasiveness of political campaigns, which have shown minimal effects in
presidential elections. Many users also expressed a desire for further
political interaction with LLMs. Which aspects of LLM interactions drove these
shifts in voter choice requires further study. Lastly, we explore how a safety
method can make LLMs more politically neutral, while raising the question of
whether such neutrality is truly the path forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NutriBench: A <span class="highlight-title">Dataset</span> for Evaluating <span class="highlight-title">Large Language Model</span>s on Nutrition
  Estimation from Meal Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12843v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12843v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Laya Pullela, Yao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Changing Answer Order Can Decrease MMLU Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, Megan Ung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) have grown in prevalence, particular
benchmarks have become essential for the evaluation of these models and for
understanding model capabilities. Most commonly, we use test accuracy averaged
across multiple subtasks in order to rank models on leaderboards, to determine
which model is best for our purposes. In this paper, we investigate the
robustness of the accuracy measurement on a widely used multiple choice
question answering dataset, MMLU. When shuffling the answer label contents, we
find that all explored models decrease in accuracy on MMLU, but not every model
is equally sensitive. These findings suggest a possible adjustment to the
standard practice of leaderboard testing, where we additionally consider the
percentage of examples each model answers correctly by random chance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Visual Sketchpad: Sketching as a Visual Chain of <span class="highlight-title">Thought</span> for Multimodal
  <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09403v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09403v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Hu, <span class="highlight-author">Weijia Shi</span>, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans draw to facilitate reasoning: we draw auxiliary lines when solving
geometry problems; we mark and circle when reasoning on maps; we use sketches
to amplify our ideas and relieve our limited-capacity working memory. However,
such actions are missing in current multimodal language models (LMs). Current
chain-of-thought and tool-use paradigms only use text as intermediate reasoning
steps. In this work, we introduce Sketchpad, a framework that gives multimodal
LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts
planning and reasoning according to the visual artifacts it has drawn.
Different from prior work, which uses text-to-image models to enable LMs to
draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is
closer to human sketching and better facilitates reasoning. Sketchpad can also
use specialist vision models during the sketching process (e.g., draw bounding
boxes with object detection models, draw masks with segmentation models), to
further enhance visual perception and reasoning. We experiment with a wide
range of math tasks (including geometry, functions, graphs, and chess) and
complex visual reasoning tasks. Sketchpad substantially improves performance on
all tasks over strong base models with no sketching, yielding an average gain
of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a
new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial
reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in
https://visualsketchpad.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project and codes url:
  https://visualsketchpad.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChuLo: Chunk-Level Key Information Representation for Long Document
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Soyeon Caren Han, Yue Dai, Feiqi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have achieved remarkable success in various Natural
Language Processing (NLP) tasks, yet their ability to handle long documents is
constrained by computational limitations. Traditional approaches, such as
truncating inputs, sparse self-attention, and chunking, attempt to mitigate
these issues, but they often lead to information loss and hinder the model's
ability to capture long-range dependencies. In this paper, we introduce ChuLo,
a novel chunk representation method for long document classification that
addresses these limitations. Our ChuLo groups input tokens using unsupervised
keyphrase extraction, emphasizing semantically important keyphrase based chunk
to retain core document content while reducing input length. This approach
minimizes information loss and improves the efficiency of Transformer-based
models. Preserving all tokens in long document understanding, especially token
classification tasks, is especially important to ensure that fine-grained
annotations, which depend on the entire sequence context, are not lost. We
evaluate our method on multiple long document classification tasks and long
document token classification tasks, demonstrating its effectiveness through
comprehensive qualitative and quantitative analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to a conference and is currently under
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposition of surprisal: Unified computational model of ERP
  components in language processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Li, Richard Futrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The functional interpretation of language-related ERP components has been a
central debate in psycholinguistics for decades. We advance an
information-theoretic model of human language processing in the brain in which
incoming linguistic input is processed at first shallowly and later with more
depth, with these two kinds of information processing corresponding to distinct
electroencephalographic signatures. Formally, we show that the information
content (surprisal) of a word in context can be decomposed into two quantities:
(A) shallow surprisal, which signals shallow processing difficulty for a word,
and corresponds with the N400 signal; and (B) deep surprisal, which reflects
the discrepancy between shallow and deep representations, and corresponds to
the P600 signal and other late positivities. Both of these quantities can be
estimated straightforwardly using modern NLP models. We validate our theory by
successfully simulating ERP patterns elicited by a variety of linguistic
manipulations in previously-reported experimental data from six experiments,
with successful novel qualitative and quantitative predictions. Our theory is
compatible with traditional cognitive theories assuming a `good-enough' shallow
representation stage, but with a precise information-theoretic formulation. The
model provides an information-theoretic model of ERP components grounded on
cognitive processes, and brings us closer to a fully-specified
neuro-computational model of language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrAttack: <span class="highlight-title">Prompt</span> Decomposition and Reconstruction Makes Powerful <span class="highlight-title">LLM</span>
  Jailbreakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety alignment of Large Language Models (LLMs) is vulnerable to both
manual and automated jailbreak attacks, which adversarially trigger LLMs to
output harmful content. However, current methods for jailbreaking LLMs, which
nest entire harmful prompts, are not effective at concealing malicious intent
and can be easily identified and rejected by well-aligned LLMs. This paper
discovers that decomposing a malicious prompt into separated sub-prompts can
effectively obscure its underlying malicious intent by presenting it in a
fragmented, less detectable form, thereby addressing these limitations. We
introduce an automatic prompt \textbf{D}ecomposition and
\textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack).
DrAttack includes three key components: (a) `Decomposition' of the original
prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly
by in-context learning with semantically similar but harmless reassembling
demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'
synonyms that maintain the original intent while jailbreaking LLMs. An
extensive empirical study across multiple open-source and closed-source LLMs
demonstrates that, with a significantly reduced number of queries, DrAttack
obtains a substantial gain of success rate over prior SOTA prompt-only
attackers. Notably, the success rate of 78.0\% on GPT-4 with merely 15 queries
surpassed previous art by 33.1\%. The project is available at
https://github.com/xirui-li/DrAttack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWE-bench: Can <span class="highlight-title">Language Model</span>s Resolve Real-World GitHub Issues? <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have outpaced our ability to evaluate them effectively, but
for their future development it is essential to study the frontier of their
capabilities. We find real-world software engineering to be a rich,
sustainable, and challenging testbed for evaluating the next generation of
language models. To this end, we introduce SWE-bench, an evaluation framework
consisting of $2,294$ software engineering problems drawn from real GitHub
issues and corresponding pull requests across $12$ popular Python repositories.
Given a codebase along with a description of an issue to be resolved, a
language model is tasked with editing the codebase to address the issue.
Resolving issues in SWE-bench frequently requires understanding and
coordinating changes across multiple functions, classes, and even files
simultaneously, calling for models to interact with execution environments,
process extremely long contexts and perform complex reasoning that goes far
beyond traditional code generation tasks. Our evaluations show that both
state-of-the-art proprietary models and our fine-tuned model SWE-Llama can
resolve only the simplest issues. The best-performing model, Claude 2, is able
to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps
towards LMs that are more practical, intelligent, and autonomous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data, code, and leaderboard are available at https://www.swebench.com
  ICLR 2024, https://openreview.net/forum?id=VTF8yNQM66</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Temporal</span> Dynamics of Emotion and Cognition in Human Translation:
  Integrating the Task Segment Framework and the HOF Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Carl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper develops a novel generative model of human translation processes
grounded in empirical translation process data. Assuming three processes that
unfold concurrently in the translating mind, it integrates the Task Segment
Framework (Munoz & Apfelthaler 2022) and the HOF taxonomy (Carl et al 2024)
into a coherent architecture: uninterrupted translation production is caused by
routinized/automated processes, cognitive/reflective interventions lead to
longer keystroke pauses, while emotional/affective states of the mind are
identified by distinctive gazing patterns. Utilizing data from the CRITT
Translation Process Research Database (TPR-DB), the paper illustrates how the
temporal structure of keystroke and gazing data can be related to the three
assumed hidden mental processes that are believed to cause the observable data.
The paper relates this embedded generative model with Robinsons (2023)
ideosomatic theory of translation, opening exciting, new theoretical horizons
for Cognitive Translation Studies, grounded in empirical data and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple is Effective: The Roles of Graphs and <span class="highlight-title">Large Language Model</span>s in
  Knowledge-Graph-Based Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mufei Li, Siqi Miao, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/Graph-COM/SubgraphRAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrinsically-Focused Evaluation of Omissions in Medical <span class="highlight-title">Summarization</span> <span class="chip">ML4H 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Schumacher, Daniel Rosenthal, Dhruv Naik, Varun Nair, Luladay Price, Geoffrey Tso, Anitha Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown promise in safety-critical
applications such as healthcare, yet the ability to quantify performance has
lagged. An example of this challenge is in evaluating a summary of the
patient's medical record. A resulting summary can enable the provider to get a
high-level overview of the patient's health status quickly. Yet, a summary that
omits important facts about the patient's record can produce a misleading
picture. This can lead to negative consequences on medical decision-making. We
propose MED-OMIT as a metric to explore this challenge. We focus on using
provider-patient history conversations to generate a subjective (a summary of
the patient's history) as a case study. We begin by discretizing facts from the
dialogue and identifying which are omitted from the subjective. To determine
which facts are clinically relevant, we measure the importance of each fact to
a simulated differential diagnosis. We compare MED-OMIT's performance to that
of clinical experts and find broad agreement We use MED-OMIT to evaluate LLM
performance on subjective generation and find some LLMs (gpt-4 and
llama-3.1-405b) work well with little effort, while others (e.g. Llama 2)
perform worse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ML4H 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Atlas-Chat: Adapting <span class="highlight-title">Large Language Model</span>s for Low-Resource Moroccan
  Arabic Dialect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guokan Shang, Hadi Abdine, Yousef Khoubrane, Amr Mohamed, Yassine Abbahaddou, Sofiane Ennadir, Imane Momayiz, Xuguang Ren, Eric Moulines, Preslav Nakov, Michalis Vazirgiannis, <span class="highlight-author">Eric Xing</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Atlas-Chat, the first-ever collection of LLMs specifically
developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as
Darija, we construct our instruction dataset by consolidating existing Darija
language resources, creating novel datasets both manually and synthetically,
and translating English instructions with stringent quality control.
Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior
ability in following Darija instructions and performing standard NLP tasks.
Notably, our models outperform both state-of-the-art and Arabic-specialized
LLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains a 13% performance
boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation
suite for Darija covering both discriminative and generative tasks.
Furthermore, we perform an experimental analysis of various fine-tuning
strategies and base model choices to determine optimal configurations. All our
resources are publicly accessible, and we believe our work offers comprehensive
design methodologies of instruction-tuning for low-resource languages, which
are often neglected in favor of data-rich languages by contemporary LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge
  into <span class="highlight-title">LLM</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Wu, Kevin Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s as Method Actors: A Model for <span class="highlight-title">Prompt</span> Engineering and Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Doyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce "Method Actors" as a mental model for guiding LLM prompt
engineering and prompt architecture. Under this mental model, LLMs should be
thought of as actors; prompts as scripts and cues; and LLM responses as
performances. We apply this mental model to the task of improving LLM
performance at playing Connections, a New York Times word puzzle game that
prior research identified as a challenging benchmark for evaluating LLM
reasoning. Our experiments with GPT-4o show that a "Method Actors" approach can
significantly improve LLM performance over both a vanilla and "Chain of
Thoughts" approach. A vanilla approach solves 27% of Connections puzzles in our
dataset and a "Chain of Thoughts" approach solves 41% of puzzles, whereas our
strongest "Method Actor" approach solves 86% of puzzles. We also test OpenAI's
newest model designed specifically for complex reasoning tasks, o1-preview.
When asked to solve a puzzle all at once, o1-preview solves 79% of Connections
puzzles in our dataset, and when allowed to build puzzle solutions one guess at
a time over multiple API calls, o1-preview solves 100% of the puzzles.
Incorporating a "Method Actor" prompt architecture increases the percentage of
puzzles that o1-preview solves perfectly from 76% to 87%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Sound Symbolism in Audio-visual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Cheng Tseng, Yi-Jen Shih, David Harwath, Raymond Mooney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual pre-trained models have gained substantial attention recently
and demonstrated superior performance on various audio-visual tasks. This study
investigates whether pre-trained audio-visual models demonstrate non-arbitrary
associations between sounds and visual representations$\unicode{x2013}$known as
sound symbolism$\unicode{x2013}$which is also observed in humans. We developed
a specialized dataset with synthesized images and audio samples and assessed
these models using a non-parametric approach in a zero-shot setting. Our
findings reveal a significant correlation between the models' outputs and
established patterns of sound symbolism, particularly in models trained on
speech data. These results suggest that such models can capture sound-meaning
connections akin to human language processing, providing insights into both
cognitive architectures and machine learning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding <span class="highlight-title">Large Language Model</span>s In Embodied Environment With Imperfect
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Liu, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s represent belief state geometry in their residual stream 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam S. Shai, Sarah E. Marzen, Lucas Teixeira, Alexander Gietelink Oldenziel, Paul M. Riechers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What computational structure are we building into large language models when
we train them on next-token prediction? Here, we present evidence that this
structure is given by the meta-dynamics of belief updating over hidden states
of the data-generating process. Leveraging the theory of optimal prediction, we
anticipate and then find that belief states are linearly represented in the
residual stream of transformers, even in cases where the predicted belief state
geometry has highly nontrivial fractal structure. We investigate cases where
the belief state geometry is represented in the final residual stream or
distributed across the residual streams of multiple layers, providing a
framework to explain these observations. Furthermore we demonstrate that the
inferred belief states contain information about the entire future, beyond the
local next-token prediction that the transformers are explicitly trained on.
Our work provides a general framework connecting the structure of training data
to the geometric structure of activations inside transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWE-<span class="highlight-title">agent</span>: <span class="highlight-title">Agent</span>-Computer Interfaces Enable Automated Software
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) agents are increasingly being used to automate
complicated tasks in digital environments. Just as humans benefit from powerful
software applications, such as integrated development environments, for complex
tasks like software engineering, we posit that LM agents represent a new
category of end users with their own needs and abilities, and would benefit
from specially-built interfaces to the software they use. We investigate how
interface design affects the performance of language model agents. As a result
of this exploration, we introduce SWE-agent: a system that facilitates LM
agents to autonomously use computers to solve software engineering tasks.
SWE-agent's custom agent-computer interface (ACI) significantly enhances an
agent's ability to create and edit code files, navigate entire repositories,
and execute tests and other programs. We evaluate SWE-agent on SWE-bench and
HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate
of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art
achieved with non-interactive LMs. Finally, we provide insight on how the
design of the ACI can impact agents' behavior and performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data, and demo available at https://swe-agent.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking <span class="highlight-title">LLM</span> Memorization through the Lens of Adversarial Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15146v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15146v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained on web-scale datasets raise substantial
concerns regarding permissible data usage. One major question is whether these
models "memorize" all their training data or they integrate many data sources
in some way more akin to how a human would learn and synthesize information.
The answer hinges, to a large degree, on how we define memorization. In this
work, we propose the Adversarial Compression Ratio (ACR) as a metric for
assessing memorization in LLMs. A given string from the training data is
considered memorized if it can be elicited by a prompt (much) shorter than the
string itself -- in other words, if these strings can be "compressed" with the
model by computing adversarial prompts of fewer tokens. The ACR overcomes the
limitations of existing notions of memorization by (i) offering an adversarial
view of measuring memorization, especially for monitoring unlearning and
compliance; and (ii) allowing for the flexibility to measure memorization for
arbitrary strings at a reasonably low compute. Our definition serves as a
practical tool for determining when model owners may be violating terms around
data usage, providing a potential legal tool and a critical lens through which
to address such scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://locuslab.github.io/acr-memorization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hire a Linguist!: Learning Endangered Languages with In-Context
  Linguistic Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can large language models (LLMs) process and translate endangered
languages? Many languages lack a large corpus to train a decent LLM; therefore
existing LLMs rarely perform well in unseen, endangered languages. On the
contrary, we observe that 2000 endangered languages, though without a large
corpus, have a grammar book or a dictionary. We propose LINGOLLM, a
training-free approach to enable an LLM to process unseen languages that hardly
occur in its pre-training. Our key insight is to demonstrate linguistic
knowledge of an unseen language in an LLM's prompt, including a dictionary, a
grammar book, and morphologically analyzed input text. We implement LINGOLLM on
top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks
across 8 endangered or low-resource languages. Our results show that LINGOLLM
elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language
directions. Our findings demonstrate the tremendous value of linguistic
knowledge in the age of LLMs for endangered languages. Our data, code, and
model generations can be found at https://github.com/LLiLab/llm4endangeredlang.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Shapley index for music streaming platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Bergantiños, Juan D. Moreno-Ternero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an index to measure the popularity of artists in music streaming
platforms. This index, which can be used to allocate the amount raised via paid
subscriptions among participating artists, is based on the Shapley value, a
centerpiece in cooperative game theory. We characterize this Shapley index
combining several axioms formalizing principles with normative appeal. This
permits to place the index in the literature, as an alternative to the
well-known (and widely used in the industry) pro-rata and user-centric indices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invar-RAG: Invariant <span class="highlight-title">LLM</span>-aligned Retrieval for Better Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Liu, Liang Zhang, Qian Li, Jianghua Wu, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown impressive capability in
providing reliable answer predictions and addressing hallucination problems. A
typical RAG implementation uses powerful retrieval models to extract external
information and large language models (LLMs) to generate answers. In contrast,
recent LLM-based retrieval has gained attention for its substantial
improvements in information retrieval (IR) due to the LLMs' semantic
understanding capability. However, directly applying LLM to RAG systems
presents challenges. This may cause feature locality problems as massive
parametric knowledge can hinder effective usage of global information across
the corpus; for example, an LLM-based retriever often inputs document summaries
instead of full documents. Moreover, various pre-trained tasks in LLMs
introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning
architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever
is constructed by integrating LoRA-based representation learning to tackle
feature locality issues. To enhance retrieval performance, we develop two
patterns (invariant and variant patterns) and an invariance loss to reduce LLM
variance. In the generation stage, a refined fine-tuning method is employed to
improve LLM accuracy in generating answers based on retrieved information.
Experimental results show that Invar-RAG significantly outperforms existing
baselines across three open-domain question answering (ODQA) datasets. Code is
available in the Supplementary Material for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Assisted Relevance Assessments: When Should We Ask <span class="highlight-title">LLM</span>s for Help? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test collections are information retrieval tools that allow researchers to
quickly and easily evaluate ranking algorithms. While test collections have
become an integral part of IR research, the process of data creation involves
significant efforts in manual annotations, which often makes it very expensive
and time-consuming. Thus, the test collections could become small when the
budget is limited, which may lead to unstable evaluations. As an alternative,
recent studies have proposed the use of large language models (LLMs) to
completely replace human assessors. However, while LLMs seem to somewhat
correlate with human judgments, they are not perfect and often show bias.
Moreover, even if a well-performing LLM or prompt is found on one dataset,
there is no guarantee that it will perform similarly in practice, due to
difference in tasks and data. Thus a complete replacement with LLMs is argued
to be too risky and not fully trustable.
  Thus, in this paper, we propose \textbf{L}LM-\textbf{A}ssisted
\textbf{R}elevance \textbf{A}ssessments (\textbf{LARA}), an effective method to
balance manual annotations with LLM annotations, which helps to make a rich and
reliable test collection. We use the LLM's predicted relevance probabilities in
order to select the most profitable documents to manually annotate under a
budget constraint. While solely relying on LLM's predicted probabilities to
manually annotate performs fairly well, with theoretical reasoning, LARA guides
the human annotation process even more effectively via online calibration
learning. Then, using the calibration model learned from the limited manual
annotations, LARA debiases the LLM predictions to annotate the remaining
non-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc
datasets show that LARA outperforms the alternative solutions under almost any
budget constraint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Conditional Expert Selection Network for Multi-domain
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyao Dong, Xingyu Lou, Feng Liu, Ruian Wang, Wenyi Yu, Ping Wang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MOE) has recently become the de facto standard in
Multi-domain recommendation (MDR) due to its powerful expressive ability.
However, such MOE-based method typically employs all experts for each instance,
leading to scalability issue and low-discriminability between domains and
experts. Furthermore, the design of commonly used domain-specific networks
exacerbates the scalability issues. To tackle the problems, We propose a novel
method named CESAA consists of Conditional Expert Selection (CES) Module and
Adaptive Expert Aggregation (AEA) Module to tackle these challenges.
Specifically, CES first combines a sparse gating strategy with domain-shared
experts. Then AEA utilizes mutual information loss to strengthen the
correlations between experts and specific domains, and significantly improve
the distinction between experts. As a result, only domain-shared experts and
selected domain-specific experts are activated for each instance, striking a
balance between computational efficiency and model performance. Experimental
results on both public ranking and industrial retrieval datasets verify the
effectiveness of our method in MDR tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span> in Medical Informatics: Direct Classification and
  Enhanced Text Representations for Automatic ICD Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyd Boukhers, AmeerAli Khan, Qusai Ramadan, Cong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the complexity of accurately classifying International
Classification of Diseases (ICD) codes from medical discharge summaries is
challenging due to the intricate nature of medical documentation. This paper
explores the use of Large Language Models (LLM), specifically the LLAMA
architecture, to enhance ICD code classification through two methodologies:
direct application as a classifier and as a generator of enriched text
representations within a Multi-Filter Residual Convolutional Neural Network
(MultiResCNN) framework. We evaluate these methods by comparing them against
state-of-the-art approaches, revealing LLAMA's potential to significantly
improve classification outcomes by providing deep contextual insights into
medical texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at the 2024 IEEE International Conference on Bioinformatics
  and Biomedicine (BIBM 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AssistRAG: Boosting the Potential of <span class="highlight-title">Large Language Model</span>s with an
  Intelligent Information Assistant <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhou, Zheng Liu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as "hallucination". Initial retrieval-augmented
generation (RAG) methods like the "Retrieve-Read" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting the Targeted Transferability of Adversarial Examples via
  Salient Region & Weighted Feature Drop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanjun Xu, Linghui Li, Kaiguo Yuan, Bingyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks can be vulnerable to adversarially crafted examples,
presenting significant risks to practical applications. A prevalent approach
for adversarial attacks relies on the transferability of adversarial examples,
which are generated from a substitute model and leveraged to attack unknown
black-box models. Despite various proposals aimed at improving transferability,
the success of these attacks in targeted black-box scenarios is often hindered
by the tendency for adversarial examples to overfit to the surrogate models. In
this paper, we introduce a novel framework based on Salient region & Weighted
Feature Drop (SWFD) designed to enhance the targeted transferability of
adversarial examples. Drawing from the observation that examples with higher
transferability exhibit smoother distributions in the deep-layer outputs, we
propose the weighted feature drop mechanism to modulate activation values
according to weights scaled by norm distribution, effectively addressing the
overfitting issue when generating adversarial examples. Additionally, by
leveraging salient region within the image to construct auxiliary images, our
method enables the adversarial example's features to be transferred to the
target category in a model-agnostic manner, thereby enhancing the
transferability. Comprehensive experiments confirm that our approach
outperforms state-of-the-art methods across diverse configurations. On average,
the proposed SWFD raises the attack success rate for normally trained models
and robust models by 16.31% and 7.06% respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Discovery Dialogue Generation Using Human Intent Analysis and
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungHeon Doh, Keunwoo Choi, Daeyong Kwon, Taesu Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A conversational music retrieval system can help users discover music that
matches their preferences through dialogue. To achieve this, a conversational
music retrieval system should seamlessly engage in multi-turn conversation by
1) understanding user queries and 2) responding with natural language and
retrieved music. A straightforward solution would be a data-driven approach
utilizing such conversation logs. However, few datasets are available for the
research and are limited in terms of volume and quality. In this paper, we
present a data generation framework for rich music discovery dialogue using a
large language model (LLM) and user intents, system actions, and musical
attributes. This is done by i) dialogue intent analysis using grounded theory,
ii) generating attribute sequences via cascading database filtering, and iii)
generating utterances using large language models. By applying this framework
to the Million Song dataset, we create LP-MusicDialog, a Large Language Model
based Pseudo Music Dialogue dataset, containing over 288k music conversations
using more than 319k music items. Our evaluation shows that the synthetic
dataset is competitive with an existing, small human dialogue dataset in terms
of dialogue consistency, item relevance, and naturalness. Furthermore, using
the dataset, we train a conversational music retrieval model and show promising
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 25th International Society for Music
  Information Retrieval Conference (ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CROLoss: Towards a Customizable Loss for Retrieval Models in Recommender
  Systems <span class="chip">CIKM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxiang Tang, Wentao Bai, Guilin Li, Xialong Liu, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale recommender systems, retrieving top N relevant candidates
accurately with resource constrain is crucial. To evaluate the performance of
such retrieval models, Recall@N, the frequency of positive samples being
retrieved in the top N ranking, is widely used. However, most of the
conventional loss functions for retrieval models such as softmax cross-entropy
and pairwise comparison methods do not directly optimize Recall@N. Moreover,
those conventional loss functions cannot be customized for the specific
retrieval size N required by each application and thus may lead to sub-optimal
performance. In this paper, we proposed the Customizable Recall@N Optimization
Loss (CROLoss), a loss function that can directly optimize the Recall@N metrics
and is customizable for different choices of N. This proposed CROLoss
formulation defines a more generalized loss function space, covering most of
the conventional loss functions as special cases. Furthermore, we develop the
Lambda method, a gradient-based method that invites more flexibility and can
further boost the system performance. We evaluate the proposed CROLoss on two
public benchmark datasets. The results show that CROLoss achieves SOTA results
over conventional loss functions for both datasets with various choices of
retrieval size N. CROLoss has been deployed onto our online E-commerce
advertising platform, where a fourteen-day online A/B test demonstrated that
CROLoss contributes to a significant business revenue growth of 4.75%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Accepted by by CIKM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Extraction from High-Level Corruption Schemes via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of financial crime that has been observed in recent years has
created an increasing concern around the topic and many people, organizations
and governments are more and more frequently trying to combat it. Despite the
increase of interest in this area, there is a lack of specialized datasets that
can be used to train and evaluate works that try to tackle those problems. This
article proposes a new micro-benchmark dataset for algorithms and models that
identify individuals and organizations, and their multiple writings, in news
articles, and presents an approach that assists in its creation. Experimental
efforts are also reported, using this dataset, to identify individuals and
organizations in financial-crime-related articles using various low-billion
parameter Large Language Models (LLMs). For these experiments, standard metrics
(Accuracy, Precision, Recall, F1 Score) are reported and various prompt
variants comprising the best practices of prompt engineering are tested. In
addition, to address the problem of ambiguous entity mentions, a simple, yet
effective LLM-based disambiguation method is proposed, ensuring that the
evaluation aligns with reality. Finally, the proposed approach is compared
against a widely used state-of-the-art open-source baseline, showing the
superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-<span class="highlight-title">LLM</span>: A Benchmark <span class="highlight-title">Dataset</span> for Understanding <span class="highlight-title">Large Language Model</span>
  Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14038v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14038v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-DCon: Leveraging <span class="highlight-title">Large Language Model</span>s (<span class="highlight-title">LLM</span>s) to Empower
  Training-Free <span class="highlight-title">Dataset</span> Condensation for Content-Based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, Ke Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern techniques in Content-based Recommendation (CBR) leverage item content
information to provide personalized services to users, but suffer from
resource-intensive training on large datasets. To address this issue, we
explore the dataset condensation for textual CBR in this paper. The goal of
dataset condensation is to synthesize a small yet informative dataset, upon
which models can achieve performance comparable to those trained on large
datasets. While existing condensation approaches are tailored to classification
tasks for continuous data like images or embeddings, direct application of them
to CBR has limitations. To bridge this gap, we investigate efficient dataset
condensation for content-based recommendation. Inspired by the remarkable
abilities of large language models (LLMs) in text comprehension and generation,
we leverage LLMs to empower the generation of textual content during
condensation. To handle the interaction data involving both users and items, we
devise a dual-level condensation method: content-level and user-level. At
content-level, we utilize LLMs to condense all contents of an item into a new
informative title. At user-level, we design a clustering-based synthesis
module, where we first utilize LLMs to extract user interests. Then, the user
interests and user embeddings are incorporated to condense users and generate
interactions for condensed users. Notably, the condensation paradigm of this
method is forward and free from iterative optimization on the synthesized
dataset. Extensive empirical findings from our study, conducted on three
authentic datasets, substantiate the efficacy of the proposed method.
Particularly, we are able to approximate up to 97% of the original performance
while reducing the dataset size by 95% (i.e., on dataset MIND).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An updated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge
  into <span class="highlight-title">LLM</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Wu, Kevin Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know Your Neighborhood: General and Zero-Shot Capable Binary Function
  Search Powered by Call Graphlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Collyer, Tim Watson, Iain Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary code similarity detection is an important problem with applications in
areas such as malware analysis, vulnerability research and license violation
detection. This paper proposes a novel graph neural network architecture
combined with a novel graph data representation called call graphlets. A call
graphlet encodes the neighborhood around each function in a binary executable,
capturing the local and global context through a series of statistical
features. A specialized graph neural network model operates on this graph
representation, learning to map it to a feature vector that encodes semantic
binary code similarities using deep-metric learning. The proposed approach is
evaluated across five distinct datasets covering different architectures,
compiler tool chains, and optimization levels. Experimental results show that
the combination of call graphlets and the novel graph neural network
architecture achieves comparable or state-of-the-art performance compared to
baseline techniques across cross-architecture, mono-architecture and zero shot
tasks. In addition, our proposed approach also performs well when evaluated
against an out-of-domain function inlining task. The work provides a general
and effective graph neural network-based solution for conducting binary code
similarity detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Under-Review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepONet as a Multi-Operator Extrapolation Model: Distributed
  <span class="highlight-title">Pretrain</span>ing with Physics-Informed Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zecheng Zhang, Christian Moya, Lu Lu, Guang Lin, Hayden Schaeffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel fine-tuning method to achieve multi-operator learning
through training a distributed neural operator with diverse function data and
then zero-shot fine-tuning the neural network using physics-informed losses for
downstream tasks. Operator learning effectively approximates solution operators
for PDEs and various PDE-related problems, yet it often struggles to generalize
to new tasks. To address this, we investigate fine-tuning a pretrained model,
while carefully selecting an initialization that enables rapid adaptation to
new tasks with minimal data. Our approach combines distributed learning to
integrate data from various operators in pre-training, while physics-informed
methods enable zero-shot fine-tuning, minimizing the reliance on downstream
data. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,
applying both to train complex nonlinear target operators that are difficult to
learn only using random initialization. Through comprehensive numerical
examples, we demonstrate the advantages of our approach, showcasing significant
improvements in accuracy. Our findings provide a robust framework for advancing
multi-operator learning and highlight the potential of transfer learning
techniques in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-10T00:00:00Z">2024-11-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CriticAL: Critic Automation with <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Y. Li, Vivek Vajipey, Noah D. Goodman, Emily B. Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the world through models is a fundamental goal of scientific
research. While large language model (LLM) based approaches show promise in
automating scientific discovery, they often overlook the importance of
criticizing scientific models. Criticizing models deepens scientific
understanding and drives the development of more accurate models. Automating
model criticism is difficult because it traditionally requires a human expert
to define how to compare a model with data and evaluate if the discrepancies
are significant--both rely heavily on understanding the modeling assumptions
and domain. Although LLM-based critic approaches are appealing, they introduce
new challenges: LLMs might hallucinate the critiques themselves. Motivated by
this, we introduce CriticAL (Critic Automation with Language Models). CriticAL
uses LLMs to generate summary statistics that capture discrepancies between
model predictions and data, and applies hypothesis tests to evaluate their
significance. We can view CriticAL as a verifier that validates models and
their critiques by embedding them in a hypothesis testing framework. In
experiments, we evaluate CriticAL across key quantitative and qualitative
dimensions. In settings where we synthesize discrepancies between models and
datasets, CriticAL reliably generates correct critiques without hallucinating
incorrect ones. We show that both human and LLM judges consistently prefer
CriticAL's critiques over alternative approaches in terms of transparency and
actionability. Finally, we show that CriticAL's critiques enable an LLM
scientist to improve upon human-designed models on real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The KIPARLA Forest treebank of spoken Italian: an <span class="highlight-title">overview</span> of initial
  design choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Pannitto, Caterina Mauri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents an overview of initial design choices discussed towards
the creation of a treebank for the Italian KIParla corpus
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning for Preserving Patient Privacy: A Framework for
  Synthesizing Realistic Patient Portal Messages <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Gatto, Parker Seegmiller, Timothy E. Burdick, Sarah Masud Preum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the COVID-19 pandemic, clinicians have seen a large and sustained
influx in patient portal messages, significantly contributing to clinician
burnout. To the best of our knowledge, there are no large-scale public patient
portal messages corpora researchers can use to build tools to optimize
clinician portal workflows. Informed by our ongoing work with a regional
hospital, this study introduces an LLM-powered framework for configurable and
realistic patient portal message generation. Our approach leverages few-shot
grounded text generation, requiring only a small number of de-identified
patient portal messages to help LLMs better match the true style and tone of
real data. Clinical experts in our team deem this framework as HIPAA-friendly,
unlike existing privacy-preserving approaches to synthetic text generation
which cannot guarantee all sensitive attributes will be protected. Through
extensive quantitative and human evaluation, we show that our framework
produces data of higher quality than comparable generation methods as well as
all related datasets. We believe this work provides a path forward for (i) the
release of large-scale synthetic patient message datasets that are
stylistically similar to ground-truth samples and (ii) HIPAA-friendly data
generation which requires minimal human de-identification efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube
  Comments on Movie-Drama using <span class="highlight-title">Transformer</span>s: Insights from Interpretability
  Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usafa Akther Rifa, Pronay Debnath, Busra Kamal Rafa, Shamaun Safa Hridi, Md. Aminur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, YouTube has become the leading platform for Bangla movies
and dramas, where viewers express their opinions in comments that convey their
sentiments about the content. However, not all comments are relevant for
sentiment analysis, necessitating a filtering mechanism. We propose a system
that first assesses the relevance of comments and then analyzes the sentiment
of those deemed relevant. We introduce a dataset of 14,000 manually collected
and preprocessed comments, annotated for relevance (relevant or irrelevant) and
sentiment (positive or negative). Eight transformer models, including
BanglaBERT, were used for classification tasks, with BanglaBERT achieving the
highest accuracy (83.99% for relevance detection and 93.3% for sentiment
analysis). The study also integrates LIME to interpret model decisions,
enhancing transparency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Consensus through Ensemble Validation: A Framework for <span class="highlight-title">LLM</span>
  Reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ninad Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown significant advances in text
generation but often lack the reliability needed for autonomous deployment in
high-stakes domains like healthcare, law, and finance. Existing approaches rely
on external knowledge or human oversight, limiting scalability. We introduce a
novel framework that repurposes ensemble methods for content validation through
model consensus. In tests across 78 complex cases requiring factual accuracy
and causal consistency, our framework improved precision from 73.1% to 93.9%
with two models (95% CI: 83.5%-97.9%) and to 95.6% with three models (95% CI:
85.2%-98.8%). Statistical analysis indicates strong inter-model agreement
($\kappa$ > 0.76) while preserving sufficient independence to catch errors
through disagreement. We outline a clear pathway to further enhance precision
with additional validators and refinements. Although the current approach is
constrained by multiple-choice format requirements and processing latency, it
offers immediate value for enabling reliable autonomous AI systems in critical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epistemic Integrity in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Mayank Goel, Reihaneh Rabbany, Jean-François Godbout, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are increasingly relied upon as sources of information,
but their propensity for generating false or misleading statements with high
confidence poses risks for users and society. In this paper, we confront the
critical problem of epistemic miscalibration $\unicode{x2013}$ where a model's
linguistic assertiveness fails to reflect its true internal certainty. We
introduce a new human-labeled dataset and a novel method for measuring the
linguistic assertiveness of Large Language Models (LLMs) which cuts error rates
by over 50% relative to previous benchmarks. Validated across multiple
datasets, our method reveals a stark misalignment between how confidently
models linguistically present information and their actual accuracy. Further
human evaluations confirm the severity of this miscalibration. This evidence
underscores the urgent risk of the overstated certainty LLMs hold which may
mislead users on a massive scale. Our framework provides a crucial step forward
in diagnosing this miscalibration, offering a path towards correcting it and
more trustworthy AI across domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CULL-MT: Compression Using Language and Layer pruning for Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedram Rostami, Mohammad Javad Dousti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation models often outperform traditional
bilingual models by leveraging translation knowledge transfer. Recent
advancements have led to these models supporting hundreds of languages and
achieving state-of-the-art results across various translation directions.
However, as these models grow larger, their inference operations become
increasingly costly. In many use cases, there is no need to support such a wide
range of language pairs, as translation is typically needed in only a few
selected directions. In this paper, we present CULL-MT, a compression method
for machine translation models based on structural layer pruning and selected
language directions. Our approach identifies and prunes unimportant layers
using a greedy strategy, then mitigates the impact by applying knowledge
distillation from the original model along with parameter-efficient
fine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models.
In a multi-way translation scenario (Persian, French, and German to English),
we find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned
with only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive,
with a 2.0 spBLEU drop after pruning 5 layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VocalTweets: Investigating Social Media Offensive Language Among
  Nigerian Musicians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunday Oluyele, Juwon Akingbade, Victor Akinode
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Musicians frequently use social media to express their opinions, but they
often convey different messages in their music compared to their posts online.
Some utilize these platforms to abuse their colleagues, while others use it to
show support for political candidates or engage in activism, as seen during the
#EndSars protest. There are extensive research done on offensive language
detection on social media, the usage of offensive language by musicians has
received limited attention. In this study, we introduce VocalTweets, a
code-switched and multilingual dataset comprising tweets from 12 prominent
Nigerian musicians, labeled with a binary classification method as Normal or
Offensive. We trained a model using HuggingFace's base-Twitter-RoBERTa,
achieving an F1 score of 74.5. Additionally, we conducted cross-corpus
experiments with the OLID dataset to evaluate the generalizability of our
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClinicalBench: Can <span class="highlight-title">LLM</span>s Beat Traditional ML Models in Clinical
  Prediction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canyu Chen, Jian Yu, Shan Chen, Che Liu, Zhongwei Wan, Danielle Bitterman, Fei Wang, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold great promise to revolutionize current
clinical systems for their superior capacities on medical text processing tasks
and medical licensing exams. Meanwhile, traditional ML models such as SVM and
XGBoost have still been mainly adopted in clinical prediction tasks. An
emerging question is Can LLMs beat traditional ML models in clinical
prediction? Thus, we build a new benchmark ClinicalBench to comprehensively
study the clinical predictive modeling capacities of both general-purpose and
medical LLMs, and compare them with traditional ML models. ClinicalBench
embraces three common clinical prediction tasks, two databases, 14
general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through
extensive empirical investigation, we discover that both general-purpose and
medical LLMs, even with different model scales, diverse prompting or
fine-tuning strategies, still cannot beat traditional ML models in clinical
prediction yet, shedding light on their potential deficiency in clinical
reasoning and decision-making. We call for caution when practitioners adopt
LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap
between LLMs' development for healthcare and real-world clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. 10 pages for main paper,
  66 pages including appendix. Project website: https://clinicalbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-Efficient Fine-Tuning for <span class="highlight-title">GPT</span>-like Deep Models to Reduce
  Hallucination and to Improve Reproducibility in Scientific Text Generation
  Using Stochastic Optimisation Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Sulimov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly adopted for complex scientific
text generation tasks, yet they often suffer from limitations in accuracy,
consistency, and hallucination control. This thesis introduces a
Parameter-Efficient Fine-Tuning (PEFT) approach tailored for GPT-like models,
aiming to mitigate hallucinations and enhance reproducibility, particularly in
the computational domain of mass spectrometry. We implemented Low-Rank
Adaptation (LoRA) adapters to refine GPT-2, termed MS-GPT, using a specialized
corpus of mass spectrometry literature. Through novel evaluation methods
applied to LLMs, including BLEU, ROUGE, and Perplexity scores, the fine-tuned
MS-GPT model demonstrated superior text coherence and reproducibility compared
to the baseline GPT-2, confirmed through statistical analysis with the Wilcoxon
rank-sum test. Further, we propose a reproducibility metric based on cosine
similarity of model outputs under controlled prompts, showcasing MS-GPT's
enhanced stability. This research highlights PEFT's potential to optimize LLMs
for scientific contexts, reducing computational costs while improving model
reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>73 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLM-Based Discrete <span class="highlight-title">Diffusion</span> <span class="highlight-title">Language Model</span>s with Entropy-Adaptive Gibbs
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, discrete diffusion language models have demonstrated promising
results in NLP. However, there has been limited research on integrating
Pretrained Language Models (PLMs) into discrete diffusion models, resulting in
underwhelming performance in downstream NLP generation tasks. This integration
is particularly challenging because of the discrepancy between step-wise
denoising strategy of diffusion models and single-step mask prediction approach
of MLM-based PLMs. In this paper, we introduce Diffusion-EAGS, a novel approach
that effectively integrates PLMs with the diffusion models. Furthermore, as it
is challenging for PLMs to determine where to apply denoising during the
diffusion process, we integrate an entropy tracking module to assist them.
Finally, we propose entropy-based noise scheduling in the forward process to
improve the effectiveness of entropy-adaptive sampling throughout the
generation phase. Experimental results show that Diffusion-EAGS outperforms
existing diffusion baselines in downstream generation tasks, achieving high
text quality and diversity with precise token-level control. We also show that
our model is capable of adapting to bilingual and low-resource settings, which
are common in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTC-Assisted <span class="highlight-title">LLM</span>-Based Contextual ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanrou Yang, Ziyang Ma, Zhifu Gao, Shiliang Zhang, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual ASR or hotword customization holds substantial practical value.
Despite the impressive performance of current end-to-end (E2E) automatic speech
recognition (ASR) systems, they often face challenges in accurately recognizing
rare words. Typical E2E contextual ASR models commonly feature complex
architectures and decoding mechanisms, limited in performance and susceptible
to interference from distractor words. With large language model (LLM)-based
ASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based
Contextual ASR model with an efficient filtering algorithm. By using coarse CTC
decoding results to filter potential relevant hotwords and incorporating them
into LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and
2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on
recognizing rare long-tail words, demonstrating significant improvements
compared to the baseline LLM-based ASR model, and substantially surpassing
other related work. More remarkably, with the help of the large language model
and proposed filtering algorithm, our contextual ASR model still performs well
with 2000 biasing words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SequentialBreak: <span class="highlight-title">Large Language Model</span>s Can be Fooled by Embedding
  Jailbreak <span class="highlight-title">Prompt</span>s into Sequential <span class="highlight-title">Prompt</span> Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijoy Ahmed Saiem, MD Sadik Hossain Shanto, Rakib Ahsan, Md Rafi ur Rashid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the integration of the Large Language Models (LLMs) into various
applications increases, so does their susceptibility to misuse, raising
significant security concerns. Numerous jailbreak attacks have been proposed to
assess the security defense of LLMs. Current jailbreak attacks mainly rely on
scenario camouflage, prompt obfuscation, prompt optimization, and prompt
iterative optimization to conceal malicious prompts. In particular, sequential
prompt chains in a single query can lead LLMs to focus on certain prompts while
ignoring others, facilitating context manipulation. This paper introduces
SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We
discuss several scenarios, not limited to examples like Question Bank, Dialog
Completion, and Game Environment, where the harmful prompt is embedded within
benign ones that can fool LLMs into generating harmful responses. The distinct
narrative structures of these scenarios show that SequentialBreak is flexible
enough to adapt to various prompt formats beyond those discussed. Extensive
experiments demonstrate that SequentialBreak uses only a single query to
achieve a substantial gain of attack success rate over existing baselines
against both open-source and closed-source models. Through our research, we
highlight the urgent need for more robust and resilient safeguards to enhance
LLM security and prevent potential misuse. All the result files and website
associated with this research are available in this GitHub repository:
https://anonymous.4open.science/r/JailBreakAttack-4F3B/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive
  Toxicity Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Yang, Filip Sondej, Harry Mayne, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety fine-tuning algorithms are commonly used to fine-tune language models
to reduce harmful outputs, but the exact internal mechanisms of how those
models achieve this remain unclear. In studying direct preference optimisation
(DPO) for toxicity reduction, current explanations claim that DPO works by
dampening the most toxic MLP neurons to learn an offset to avert toxic regions
in the residual stream. However, by ablating the most toxic neurons and
applying activation patching, we find this explanation incomplete. By
projecting neuron activation changes onto a toxicity probe, we find that only
31.8\% of toxicity reduction comes from dampened toxic neurons. Instead, DPO
reduces toxicity by accumulating effects across multiple neuron groups, both
reducing writing in the toxic direction and promoting anti-toxicity in the
residual stream. Moreover, DPO gives noisy adjustments to neuron activations,
with many neurons actually increasing toxicity. This indicates that DPO is a
balancing process between opposing neuron effects to achieve toxicity
reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sultan Alrashed, Dmitrii Khizbullin, David R. Pugh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) grow and develop, so do their data demands.
This is especially true for multilingual LLMs, where the scarcity of
high-quality and readily available data online has led to a multitude of
synthetic dataset generation approaches. A key technique in this space is
machine translation (MT), where high-quality English text is adapted to a
target, comparatively low-resource language. This report introduces
FineWeb-Edu-Ar, a machine-translated version of the exceedingly popular
(deduplicated) FineWeb-Edu dataset from HuggingFace. To the best of our
knowledge, FineWeb-Edu-Ar is the largest publicly available machine-translated
Arabic dataset out there, with its size of 202B tokens of an Arabic-trained
tokenizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CausalStock: Deep End-to-end Causal Discovery for News-driven Stock
  Movement Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqi Li, Yuebo Sun, Yuxin Lin, Xin Gao, Shuo Shang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two issues in news-driven multi-stock movement prediction tasks
that are not well solved in the existing works. On the one hand, "relation
discovery" is a pivotal part when leveraging the price information of other
stocks to achieve accurate stock movement prediction. Given that stock
relations are often unidirectional, such as the "supplier-consumer"
relationship, causal relations are more appropriate to capture the impact
between stocks. On the other hand, there is substantial noise existing in the
news data leading to extracting effective information with difficulty. With
these two issues in mind, we propose a novel framework called CausalStock for
news-driven multi-stock movement prediction, which discovers the temporal
causal relations between stocks. We design a lag-dependent temporal causal
discovery mechanism to model the temporal causal graph distribution. Then a
Functional Causal Model is employed to encapsulate the discovered causal
relations and predict the stock movements. Additionally, we propose a Denoised
News Encoder by taking advantage of the excellent text evaluation ability of
large language models (LLMs) to extract useful information from massive news
data. The experiment results show that CausalStock outperforms the strong
baselines for both news-driven multi-stock movement prediction and multi-stock
movement prediction tasks on six real-world datasets collected from the US,
China, Japan, and UK markets. Moreover, getting benefit from the causal
relations, CausalStock could offer a clear prediction mechanism with good
explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Training Meets Consistency: Improving <span class="highlight-title">LLM</span>s' Reasoning With
  Consistency-Driven Rationale Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyeok Lee, Keisuke Sakaguchi, JinYeong Bak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-training approach for large language models (LLMs) improves reasoning
abilities by training the models on their self-generated rationales. Previous
approaches have labeled rationales that produce correct answers for a given
question as appropriate for training. However, a single measure risks
misjudging rationale quality, leading the models to learn flawed reasoning
patterns. To address this issue, we propose CREST (Consistency-driven Rationale
Evaluation for Self-Training), a self-training framework that further evaluates
each rationale through follow-up questions and leverages this evaluation to
guide its training. Specifically, we introduce two methods: (1) filtering out
rationales that frequently result in incorrect answers on follow-up questions
and (2) preference learning based on mixed preferences from rationale
evaluation results of both original and follow-up questions. Experiments on
three question-answering datasets using open LLMs show that CREST not only
improves the logical robustness and correctness of rationales but also improves
reasoning abilities compared to previous self-training approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span> Vocabulary Compression for Low-Compute Environments <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreeram Vennam, Anish Joishy, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to compress the final linear layer of language models,
reducing memory usage by up to 3.4x without significant performance loss. By
grouping tokens based on Byte Pair Encoding (BPE) merges, we prevent
materialization of the memory-intensive logits tensor. Evaluations on the
TinyStories dataset show that our method performs on par with GPT-Neo and GPT2
while significantly improving throughput by up to 3x, making it suitable for
low-compute environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Learning and Compression Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>s Matter: Comparing ML/GAI Approaches for Generating Inductive
  Qualitative Coding Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Chen, Alexandros Lotsos, Lexie Zhao, Grace Wang, Uri Wilensky, Bruce Sherin, Michael Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive qualitative methods have been a mainstay of education research for
decades, yet it takes much time and effort to conduct rigorously. Recent
advances in artificial intelligence, particularly with generative AI (GAI),
have led to initial success in generating inductive coding results. Like human
coders, GAI tools rely on instructions to work, and how to instruct it may
matter. To understand how ML/GAI approaches could contribute to qualitative
coding processes, this study applied two known and two theory-informed novel
approaches to an online community dataset and evaluated the resulting coding
results. Our findings show significant discrepancies between ML/GAI approaches
and demonstrate the advantage of our approaches, which introduce human coding
processes into GAI prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AERA 2025 Annual Meeting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Knowing When You Don't Know": A Multilingual Relevance Assessment
  <span class="highlight-title">Dataset</span> for Robust Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the World Model Implicit in a <span class="highlight-title">Generative</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyon Vafa, Justin Y. Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work suggests that large language models may implicitly learn world
models. How should we assess this possibility? We formalize this question for
the case where the underlying reality is governed by a deterministic finite
automaton. This includes problems as diverse as simple logical reasoning,
geographic navigation, game-playing, and chemistry. We propose new evaluation
metrics for world model recovery inspired by the classic Myhill-Nerode theorem
from language theory. We illustrate their utility in three domains: game
playing, logic puzzles, and navigation. In all domains, the generative models
we consider do well on existing diagnostics for assessing world models, but our
evaluation metrics reveal their world models to be far less coherent than they
appear. Such incoherence creates fragility: using a generative model to solve
related but subtly different tasks can lead to failures. Building generative
models that meaningfully capture the underlying logic of the domains they model
would be immensely valuable; our results suggest new ways to assess how close a
given model is to that goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Large Language Model</span>s for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have garnered remarkable advancements across
diverse code-related tasks, known as Code LLMs, particularly in code generation
that generates source code with LLM from natural language descriptions. This
burgeoning field has captured significant interest from both academic
researchers and industry professionals due to its practical significance in
software development, e.g., GitHub Copilot. Despite the active exploration of
LLMs for a variety of code tasks, either from the perspective of natural
language processing (NLP) or software engineering (SE) or both, there is a
noticeable absence of a comprehensive and up-to-date literature review
dedicated to LLM for code generation. In this survey, we aim to bridge this gap
by providing a systematic literature review that serves as a valuable reference
for researchers investigating the cutting-edge progress in LLMs for code
generation. We introduce a taxonomy to categorize and discuss the recent
developments in LLMs for code generation, covering aspects such as data
curation, latest advances, performance evaluation, ethical implications,
environmental impact, and real-world applications. In addition, we present a
historical overview of the evolution of LLMs for code generation and offer an
empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks
across various levels of difficulty and types of programming tasks to highlight
the progressive enhancements in LLM capabilities for code generation. We
identify critical challenges and promising opportunities regarding the gap
between academia and practical development. Furthermore, we have established a
dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)
to continuously document and disseminate the most recent advances in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Financial Statement Analysis with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Kim, Maximilian Muhn, Valeri Nikolaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate whether large language models (LLMs) can successfully perform
financial statement analysis in a way similar to a professional human analyst.
We provide standardized and anonymous financial statements to GPT4 and instruct
the model to analyze them to determine the direction of firms' future earnings.
Even without narrative or industry-specific information, the LLM outperforms
financial analysts in its ability to predict earnings changes directionally.
The LLM exhibits a relative advantage over human analysts in situations when
the analysts tend to struggle. Furthermore, we find that the prediction
accuracy of the LLM is on par with a narrowly trained state-of-the-art ML
model. LLM prediction does not stem from its training memory. Instead, we find
that the LLM generates useful narrative insights about a company's future
performance. Lastly, our trading strategies based on GPT's predictions yield a
higher Sharpe ratio and alphas than strategies based on other models. Our
results suggest that LLMs may take a central role in analysis and
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Previously posted on SSRN (May 21, 2024). See
  http://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Effective Masked <span class="highlight-title">Diffusion</span> <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While diffusion models excel at generating high-quality images, prior work
reports a significant performance gap between diffusion and autoregressive (AR)
methods in language modeling. In this work, we show that simple masked discrete
diffusion is more performant than previously thought. We apply an effective
training recipe that improves the performance of masked diffusion models and
derive a simplified, Rao-Blackwellized objective that results in additional
improvements. Our objective has a simple form -- it is a mixture of classical
masked language modeling losses -- and can be used to train encoder-only
language models that admit efficient samplers, including ones that can generate
arbitrary lengths of text semi-autoregressively like a traditional language
model. On language modeling benchmarks, a range of masked diffusion models
trained with modern engineering practices achieves a new state-of-the-art among
diffusion models, and approaches AR perplexity. We provide the code, along with
a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. We provide the code at
  https://github.com/kuleshov-group/mdlm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health Text Simplification: An Annotated Corpus for Digestive Cancer
  Education and Novel Strategies for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15043v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15043v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: The reading level of health educational materials significantly
influences the understandability and accessibility of the information,
particularly for minoritized populations. Many patient educational resources
surpass the reading level and complexity of widely accepted standards. There is
a critical need for high-performing text simplification models in health
information to enhance dissemination and literacy. This need is particularly
acute in cancer education, where effective prevention and screening education
can substantially reduce morbidity and mortality.
  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research, comprising educational content from the American Cancer Society,
Centers for Disease Control and Prevention, and National Cancer Institute.
Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large
Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model's effectiveness with unlabeled data.
  Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal of Biomedical Informatics, Volume 158, October
  2024, 104727</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focal Inferential Infusion Coupled with Tractable Density Discrimination
  for Implicit Hate Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11896v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11896v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Masud, Ashutosh Bajpai, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pretrained large language models (PLMs) have achieved
state-of-the-art on many natural language processing (NLP) tasks, they lack an
understanding of subtle expressions of implicit hate speech. Various attempts
have been made to enhance the detection of implicit hate by augmenting external
context or enforcing label separation via distance-based metrics. Combining
these two approaches, we introduce FiADD, a novel Focused Inferential Adaptive
Density Discrimination framework. FiADD enhances the PLM finetuning pipeline by
bringing the surface form/meaning of an implicit hate speech closer to its
implied form while increasing the inter-cluster distance among various labels.
We test FiADD on three implicit hate datasets and observe significant
improvement in the two-way and three-way hate classification tasks. We further
experiment on the generalizability of FiADD on three other tasks, detecting
sarcasm, irony, and stance, in which surface and implied forms differ, and
observe similar performance improvements. Consequently, we analyze the
generated latent space to understand its evolution under FiADD, which
corroborates the advantage of employing FiADD for implicit hate speech
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 Figures, 9 Tables. Accepted at NLE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Unstructured Data to In-Context Learning: Exploring What Tasks Can
  Be Learned and When 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Christian Wibisono, Yixin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like transformers demonstrate impressive
in-context learning (ICL) capabilities, allowing them to make predictions for
new tasks based on prompt exemplars without parameter updates. While existing
ICL theories often assume structured training data resembling ICL tasks (e.g.,
x-y pairs for linear regression), LLMs are typically trained unsupervised on
unstructured text, such as web content, which lacks clear parallels to tasks
like word analogy. To address this gap, we examine what enables ICL in models
trained on unstructured data, focusing on critical sequence model requirements
and training data structure. We find that many ICL capabilities can emerge
simply from co-occurrence of semantically related word pairs in unstructured
data; word analogy completion, for example, can provably arise purely through
co-occurrence modeling, using classical language models like continuous bag of
words (CBOW), without needing positional information or attention mechanisms.
However, positional information becomes crucial for logic reasoning tasks
requiring generalization to unseen tokens. Finally, we identify two cases where
ICL fails: one in logic reasoning tasks that require generalizing to new,
unseen patterns, and another in analogy completion where relevant word pairs
appear only in fixed training positions. These findings suggest that LLMs' ICL
abilities depend heavily on the structural elements within their training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Direct Preference Optimization: <span class="highlight-title">Dataset</span>s,
  Theories, Variants, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, Zhou Zhao, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of large language models (LLMs), aligning policy
models with human preferences has become increasingly critical. Direct
Preference Optimization (DPO) has emerged as a promising approach for
alignment, acting as an RL-free alternative to Reinforcement Learning from
Human Feedback (RLHF). Despite DPO's various advancements and inherent
limitations, an in-depth review of these aspects is currently lacking in the
literature. In this work, we present a comprehensive review of the challenges
and opportunities in DPO, covering theoretical analyses, variants, relevant
preference datasets, and applications. Specifically, we categorize recent
studies on DPO based on key research questions to provide a thorough
understanding of DPO's current landscape. Additionally, we propose several
future research directions to offer insights on model alignment for the
research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoManual: Constructing Instruction Manuals by <span class="highlight-title">LLM</span> <span class="highlight-title">Agent</span>s via
  Interactive Environmental Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16247v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16247v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, Xiaofei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) based agents have shown promise in autonomously
completing tasks across various domains, e.g., robotics, games, and web
navigation. However, these agents typically require elaborate design and expert
prompts to solve tasks in specific domains, which limits their adaptability. We
introduce AutoManual, a framework enabling LLM agents to autonomously build
their understanding through interaction and adapt to new environments.
AutoManual categorizes environmental knowledge into diverse rules and optimizes
them in an online fashion by two agents: 1) The Planner codes actionable plans
based on current rules for interacting with the environment. 2) The Builder
updates the rules through a well-structured rule system that facilitates online
rule management and essential detail retention. To mitigate hallucinations in
managing rules, we introduce a *case-conditioned prompting* strategy for the
Builder. Finally, the Formulator agent compiles these rules into a
comprehensive manual. The self-generated manual can not only improve the
adaptability but also guide the planning of smaller LLMs while being
human-readable. Given only one simple demonstration, AutoManual significantly
improves task success rates, achieving 97.4\% with GPT-4-turbo and 86.2\% with
GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at
https://github.com/minghchen/automanual.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How do <span class="highlight-title">Large Language Model</span>s Handle Multilingualism? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities across
diverse languages. This study explores how LLMs handle multilingualism. Based
on observed language ratio shifts among layers and the relationships between
network structures and certain capabilities, we hypothesize the LLM's
multilingual workflow ($\texttt{MWork}$): LLMs initially understand the query,
converting multilingual inputs into English for task-solving. In the
intermediate layers, they employ English for thinking and incorporate
multilingual knowledge with self-attention and feed-forward structures,
respectively. In the final layers, LLMs generate responses aligned with the
original language of the query. To verify $\texttt{MWork}$, we introduce
Parallel Language-specific Neuron Detection ($\texttt{PLND}$) to identify
activated neurons for inputs in different languages without any labeled data.
Using $\texttt{PLND}$, we validate $\texttt{MWork}$ through extensive
experiments involving the deactivation of language-specific neurons across
various layers and structures. Moreover, $\texttt{MWork}$ allows fine-tuning of
language-specific neurons with a small dataset, enhancing multilingual
abilities in a specific language without compromising others. This approach
results in an average improvement of $3.6\%$ for high-resource languages and
$2.3\%$ for low-resource languages across all tasks with just $400$ documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRISM: A Methodology for Auditing Biases in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leif Azzopardi, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditing Large Language Models (LLMs) to discover their biases and
preferences is an emerging challenge in creating Responsible Artificial
Intelligence (AI). While various methods have been proposed to elicit the
preferences of such models, countermeasures have been taken by LLM trainers,
such that LLMs hide, obfuscate or point blank refuse to disclosure their
positions on certain subjects. This paper presents PRISM, a flexible,
inquiry-based methodology for auditing LLMs - that seeks to illicit such
positions indirectly through task-based inquiry prompting rather than direct
inquiry of said preferences. To demonstrate the utility of the methodology, we
applied PRISM on the Political Compass Test, where we assessed the political
leanings of twenty-one LLMs from seven providers. We show LLMs, by default,
espouse positions that are economically left and socially liberal (consistent
with prior work). We also show the space of positions that these models are
willing to espouse - where some models are more constrained and less compliant
than others - while others are more neutral and objective. In sum, PRISM can
more reliably probe and audit LLMs to understand their preferences, biases and
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis and Mitigation of the Reversal Curse <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research observed a noteworthy phenomenon in large language models
(LLMs), referred to as the ``reversal curse.'' The reversal curse is that when
dealing with two entities, denoted as $a$ and $b$, connected by their relation
$R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of
``$aRb$,'' but encounter challenges when processing ``$bR^{-1}a$,'' whether in
generation or comprehension. For instance, GPT-4 can accurately respond to the
query ``Tom Cruise's mother is?'' with ``Mary Lee Pfeiffer,'' but it struggles
to provide a satisfactory answer when asked ``Mary Lee Pfeiffer's son is?'' In
this paper, we undertake the first-ever study of how the reversal curse happens
in LLMs. Our investigations reveal that the reversal curse can stem from the
specific training objectives, which become particularly evident in the
widespread use of next-token prediction within most causal language models. We
hope this initial investigation can draw more attention to the reversal curse,
as well as other underlying limitations in current LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main. This paper was originally titled "Are We
  Falling into a Middle-Intelligence Trap? An Analysis and Mitigation of the
  Reversal Curse." The title was revised during the submission to EMNLP, and we
  are now updating the title for this preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient <span class="highlight-title">LLM</span>
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangya Wan, Yuqi Wu, Jie Chen, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Consistency (SC) is a widely used method to mitigate hallucinations in
Large Language Models (LLMs) by sampling the LLM multiple times and outputting
the most frequent solution. Despite its benefits, SC results in significant
computational costs proportional to the number of samples generated. Previous
early-stopping approaches, such as Early Stopping Self Consistency and Adaptive
Consistency, have aimed to reduce these costs by considering output
consistency, but they do not analyze the quality of the reasoning paths (RPs)
themselves. To address this issue, we propose Reasoning-Aware Self-Consistency
(RASC), an innovative early-stopping framework that dynamically adjusts the
number of sample generations by considering both the output answer and the RPs
from Chain of Thought (CoT) prompting. RASC assigns confidence scores
sequentially to the generated samples, stops when certain criteria are met, and
then employs weighted majority voting to optimize sample usage and enhance
answer reliability. We comprehensively test RASC with multiple LLMs across
varied QA datasets. RASC outperformed existing methods and significantly
reduces sample usage by an average of 80% while maintaining or improving
accuracy up to 5% compared to the original SC
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DELIFT: Data Efficient <span class="highlight-title">Language model</span> Instruction Fine Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevksy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is essential for enhancing their
performance on specific tasks but is often resource-intensive due to redundant
or uninformative data. To address this inefficiency, we introduce DELIFT (Data
Efficient Language model Instruction Fine-Tuning), a novel algorithm that
systematically optimizes data selection across the three key stages of
fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,
reasoning, question-answering), and (3) continual fine-tuning (e.g.,
incorporating new data versions). Unlike existing methods that focus on
single-stage optimization or rely on computationally intensive gradient
calculations, DELIFT operates efficiently across all stages. Central to our
approach is a pairwise utility metric that quantifies how beneficial a data
sample is for improving the model's responses to other samples, effectively
measuring the informational value relative to the model's current capabilities.
By leveraging different submodular functions applied to this metric, DELIFT
selects diverse and optimal subsets that are useful across all stages of
fine-tuning. Experiments across various tasks and model scales demonstrate that
DELIFT can reduce the fine-tuning data size by up to 70% without compromising
performance, offering significant computational savings and outperforming
existing methods in both efficiency and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: <span class="highlight-title">Language Model</span> Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach for classification tasks using Large Language Models
(LLMs) in an explainable method. Unlike ML models, which rely heavily on data
cleaning and feature engineering, this method streamlines the process using
LLMs. This paper proposes a method called "Language Model Learning (LML)"
powered by a new method called "Data-Augmented Prediction (DAP)." The
classification is performed by LLMs using a method similar to that used by
humans who manually explore and understand the data to decide classifications.
In the process of LML, a dataset is summarized and evaluated to determine the
features leading to each label the most. In the DAP process, the system uses
the data summary and a row of the testing dataset to automatically generate a
query to retrieve relevant rows from the dataset for context-aware
classification. LML and DAP unlock new possibilities in areas that require
explainable and context-aware decisions by ensuring satisfactory accuracy even
with complex data. The system scored an accuracy above 90% in some test cases,
confirming the effectiveness and potential of the system to outperform ML
models in various scenarios. The source code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Made the abstract and the content clearer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChemDFM: A Large Language Foundation Model for Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14818v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14818v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Kai Yu, Xin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has played an increasingly important role in
chemical research. However, most models currently used in chemistry are
specialist models that require training and tuning for specific tasks. A more
generic and efficient solution would be an AI model that could address many
tasks and support free-form dialogue in the broad field of chemistry. In its
utmost form, such a generalist AI chemist could be referred to as Chemical
General Intelligence. Large language models (LLMs) have recently logged
tremendous success in the general domain of natural language processing,
showing emerging task generalization and free-form dialogue capabilities.
However, domain knowledge of chemistry is largely missing when training
general-domain LLMs. The lack of such knowledge greatly hinders the performance
of generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,
a pioneering LLM for chemistry trained on 34B tokens from chemical literature
and textbooks, and fine-tuned using 2.7M instructions. As a result, it can
understand and reason with chemical knowledge in free-form dialogue.
Quantitative evaluations show that ChemDFM significantly surpasses most
representative open-source LLMs. It outperforms GPT-4 on a great portion of
chemical tasks, despite the substantial size difference. We have open-sourced
the inference codes, evaluation datasets, and model weights of ChemDFM on
Huggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 12 figures, 12 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Mixcode Popular Songs with Artificial Intelligence: Concepts,
  Plans, and Speculations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Kayla Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music is a potent form of expression that can communicate, accentuate or even
create the emotions of an individual or a collective. Both historically and in
contemporary experiences, musical expression was and is commonly
instrumentalized for social, political and/or economic purposes. Generative
artificial intelligence provides a wealth of both opportunities and challenges
with regard to music and its role in society. This paper discusses a proposed
project integrating artificial intelligence and popular music, with the
ultimate goal of creating a powerful tool for implementing music for social
transformation, education, healthcare, and emotional well-being. Given that it
is being presented at the outset of a collaboration between a computer
scientist/data analyst and an ethnomusicologist/social anthropologist. it is
mainly conceptual and somewhat speculative in nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link to the paper:https://aimc2024.pubpub.org/pub/rdulfbve/release/1
  Published in The International Conference on AI and Musical Creativity at the
  University of Oxford (2024) https://aimc2024.pubpub.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning for Tag Recommendation: Tackling Data Sparsity and Cold
  Start Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanshuai Luo, Rui Wang, Yaxin Liang, Ankai Liang, Wenyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of digital information, personalized recommendation
systems have become an indispensable part of Internet services, especially in
the fields of e-commerce, social media, and online entertainment. However,
traditional collaborative filtering and content-based recommendation methods
have limitations in dealing with data sparsity and cold start problems,
especially in the face of largescale heterogeneous data, which makes it
difficult to meet user expectations. This paper proposes a new label
recommendation algorithm based on metric learning, which aims to overcome the
challenges of traditional recommendation systems by learning effective distance
or similarity metrics to capture the subtle differences between user
preferences and item features. Experimental results show that the algorithm
outperforms baseline methods including local response metric learning (LRML),
collaborative metric learning (CML), and adaptive tensor factorization (ATF)
based on adversarial learning on multiple evaluation metrics. In particular, it
performs particularly well in the accuracy of the first few recommended items,
while maintaining high robustness and maintaining high recommendation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Knowing When You Don't Know": A Multilingual Relevance Assessment
  <span class="highlight-title">Dataset</span> for Robust Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn
  Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinggang Sun, Ziming Guo, Haining Yu, Chuanyi Liu, Xiang Li, Bingxuan Wang, Xiangzhan Yu, Tiancheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) for specific domain tasks has
achieved great success in Text-to-SQL tasks. However, these fine-tuned models
often face challenges with multi-turn Text-to-SQL tasks caused by ambiguous or
unanswerable questions. It is desired to enhance LLMs to handle multiple types
of questions in multi-turn Text-to-SQL tasks. To address this, we propose a
novel data augmentation method, called QDA-SQL, which generates multiple types
of multi-turn Q\&A pairs using LLMs. In QDA-SQL, we introduce a method
incorporating validation and correction mechanisms to handle complex multi-turn
Text-to-SQL tasks. Experimental results demonstrate that QDA-SQL enables
fine-tuned models to exhibit higher performance on SQL statement accuracy and
enhances their ability to handle complex, unanswerable questions in multi-turn
Text-to-SQL tasks. The generation script and test set are released at
https://github.com/mcxiaoxiao/QDA-SQL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Dixit, Tim Oates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations. Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM). Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a "reasoning score" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more
structured problem-solving process potentially providing educational benefits
for students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th MATH-AI Workshop at NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: <span class="highlight-title">Language Model</span> Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach for classification tasks using Large Language Models
(LLMs) in an explainable method. Unlike ML models, which rely heavily on data
cleaning and feature engineering, this method streamlines the process using
LLMs. This paper proposes a method called "Language Model Learning (LML)"
powered by a new method called "Data-Augmented Prediction (DAP)." The
classification is performed by LLMs using a method similar to that used by
humans who manually explore and understand the data to decide classifications.
In the process of LML, a dataset is summarized and evaluated to determine the
features leading to each label the most. In the DAP process, the system uses
the data summary and a row of the testing dataset to automatically generate a
query to retrieve relevant rows from the dataset for context-aware
classification. LML and DAP unlock new possibilities in areas that require
explainable and context-aware decisions by ensuring satisfactory accuracy even
with complex data. The system scored an accuracy above 90% in some test cases,
confirming the effectiveness and potential of the system to outperform ML
models in various scenarios. The source code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Made the abstract and the content clearer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-09T00:00:00Z">2024-11-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">48</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating
  Financial <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojun Wu, Junxi Liu, Huanyi Su, Zhouchi Lin, Yiyan Qi, Chengjin Xu, Jiajun Su, Jiajie Zhong, Fuwei Wang, Saizhuo Wang, Fengrui Hua, Jia Li, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models become increasingly prevalent in the financial
sector, there is a pressing need for a standardized method to comprehensively
assess their performance. However, existing finance benchmarks often suffer
from limited language and task coverage, as well as challenges such as
low-quality datasets and inadequate adaptability for LLM evaluation. To address
these limitations, we propose "Golden Touchstone", the first comprehensive
bilingual benchmark for financial LLMs, which incorporates representative
datasets from both Chinese and English across eight core financial NLP tasks.
Developed from extensive open source data collection and industry-specific
demands, this benchmark includes a variety of financial tasks aimed at
thoroughly assessing models' language understanding and generation
capabilities. Through comparative analysis of major models on the benchmark,
such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and
limitations in processing complex financial information. Additionally, we
open-sourced Touchstone-GPT, a financial LLM trained through continual
pre-training and financial instruction tuning, which demonstrates strong
performance on the bilingual benchmark but still has limitations in specific
tasks.This research not only provides the financial large language models with
a practical evaluation tool but also guides the development and optimization of
future research. The source code for Golden Touchstone and model weight of
Touchstone-GPT have been made publicly available at
\url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the
ongoing evolution of FinLLMs and fostering further research in this critical
area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Detection of <span class="highlight-title">LLM</span>-Generated Text: A Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongye Su, Yuqing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of large language models to generate complex texts allows them to
be widely integrated into many aspects of life, and their output can quickly
fill all network resources. As the impact of LLMs grows, it becomes
increasingly important to develop powerful detectors for the generated text.
This detector is essential to prevent the potential misuse of these
technologies and to protect areas such as social media from the negative
effects of false content generated by LLMS. The main goal of LLM-generated text
detection is to determine whether text is generated by an LLM, which is a basic
binary classification task. In our work, we mainly use three different
classification methods based on open source datasets: traditional machine
learning techniques such as logistic regression, k-means clustering, Gaussian
Naive Bayes, support vector machines, and methods based on converters such as
BERT, and finally algorithms that use LLMs to detect LLM-generated text. We
focus on model generalization, potential adversarial attacks, and accuracy of
model evaluation. Finally, the possible research direction in the future is
proposed, and the current experimental results are summarized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An $\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemente Pasti, Talu Karagöz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting finite state automata (FSAs) from black-box models offers a
powerful approach to gaining interpretable insights into complex model
behaviors. To support this pursuit, we present a weighted variant of Angluin's
(1987) $\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the
original algorithm, devising a way to exactly learn deterministic weighted FSAs
whose weights support division. Furthermore, we formulate the learning process
in a manner that highlights the connection with FSA minimization, showing how
$\mathbf{L^*}$ directly learns a minimal automaton for the target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Human Explanations for Robust Hate Speech Detection <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer L. Chen, Faisal Ladhak, Daniel Li, Noémie Elhadad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the black-box nature and complexity of large transformer language
models (LM), concerns about generalizability and robustness present ethical
implications for domains such as hate speech (HS) detection. Using the content
rich Social Bias Frames dataset, containing human-annotated stereotypes,
intent, and targeted groups, we develop a three stage analysis to evaluate if
LMs faithfully assess hate speech. First, we observe the need for modeling
contextually grounded stereotype intents to capture implicit semantic meaning.
Next, we design a new task, Stereotype Intent Entailment (SIE), which
encourages a model to contextually understand stereotype presence. Finally,
through ablation tests and user studies, we find a SIE objective improves
content understanding, but challenges remain in modeling implicit intent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2021 ACL Unimplicit Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IOPO: Empowering <span class="highlight-title">LLM</span>s with Complex Instruction Following via
  Input-Output Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of large language models (LLMs), the ability of models to
accurately follow instructions is paramount as more agents and applications
leverage LLMs for construction, where the complexity of instructions are
rapidly increasing. However, on the one hand, there is only a certain amount of
complex instruction evaluation data; on the other hand, there are no dedicated
algorithms to improve the ability to follow complex instructions. To this end,
this paper introduces TRACE, a benchmark for improving and evaluating the
complex instructionfollowing ability, which consists of 120K training data and
1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference
Optimization) alignment method which takes both input and output preference
pairs into consideration, where LLMs not only rapidly align with response
preferences but also meticulously explore the instruction preferences.
Extensive experiments on both in-domain and outof-domain datasets confirm the
effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and
6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Knowledge Boundaries in <span class="highlight-title">Large Language Model</span>s for Retrieval
  Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhang, Xinyu Wang, Yong Jiang, Zhuo Chen, Feiteng Mu, Mengting Hu, Pengjun Xie, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly recognized for their practical
applications. However, these models often encounter challenges in dynamically
changing knowledge, as well as in managing unknown static knowledge.
Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a
significant impact on LLMs. Actually, we find that the impact of RAG on the
question answering capabilities of LLMs can be categorized into three groups:
beneficial, neutral, and harmful. By minimizing retrieval requests that yield
neutral or harmful results, we can effectively reduce both time and
computational costs, while also improving the overall performance of LLMs. This
insight motivates us to differentiate between types of questions using certain
metrics as indicators, to decrease the retrieval ratio without compromising
performance. In our work, we propose a method that is able to identify
different types of questions from this view by training a Knowledge Boundary
Model (KBM). Experiments conducted on 11 English and Chinese datasets
illustrate that the KBM effectively delineates the knowledge boundary,
significantly decreasing the proportion of retrievals required for optimal
end-to-end performance. Specifically, we evaluate the effectiveness of KBM in
three complex scenarios: dynamic knowledge, long-tail static knowledge, and
multi-hop problems, as well as its functionality as an external LLM plug-in.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Chi-kiu Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We assess the difficulty of gender resolution in literary-style dialogue
settings and the influence of gender stereotypes. Instances of the test suite
contain spoken dialogue interleaved with external meta-context about the
characters and the manner of speaking. We find that character and manner
stereotypes outside of the dialogue significantly impact the gender agreement
of referents within the dialogue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding
  And A Retrieval-Aware Tuning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yew Ken Chia, Liying Cheng, Hou Pong Chan, Chaoqun Liu, Maojia Song, Sharifah Mahani Aljunied, Soujanya Poria, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to understand and answer questions over documents can be useful
in many business and practical applications. However, documents often contain
lengthy and diverse multimodal contents such as texts, figures, and tables,
which are very time-consuming for humans to read thoroughly. Hence, there is an
urgent need to develop effective and automated methods to aid humans in this
task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an
automated framework to evaluate the performance of large multimodal models. We
further propose a retrieval-aware tuning approach for efficient and effective
multimodal document reading. Compared to existing works, our benchmark consists
of more recent and lengthy documents with hundreds of pages, while also
requiring open-ended solutions and not just extractive answers. To our
knowledge, our training framework is the first to directly address the
retrieval setting for multimodal long documents. To enable tuning open-source
models, we construct a training corpus in a fully automatic manner for the
question-answering task over such documents. Experiments show that our tuning
approach achieves a relative improvement of 4.6% for the correctness of model
responses, compared to the baseline open-source models. Our data, code, and
models are available at https://multimodal-documents.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Algorithms and RAG Enhancing Semi-Supervised Text
  Classification with Large <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhong, Jiahao Zeng, Yongxin Yu, Bohong Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative semi-supervised learning approach for
text classification, addressing the challenge of abundant data but limited
labeled examples. Our methodology integrates few-shot learning with
retrieval-augmented generation (RAG) and conventional statistical clustering,
enabling effective learning from a minimal number of labeled instances while
generating high-quality labeled data. To the best of our knowledge, we are the
first to incorporate RAG alongside clustering in text data generation. Our
experiments on the Reuters and Web of Science datasets demonstrate
state-of-the-art performance, with few-shot augmented data alone producing
results nearly equivalent to those achieved with fully labeled datasets.
Notably, accuracies of 95.41\% and 82.43\% were achieved for complex text
document classification tasks, where the number of categories can exceed 100.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEEKR: Selective Attention-Guided Knowledge Retention for <span class="highlight-title">Continual</span>
  Learning of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan He, Haiyun Guo, Kuan Zhu, Zihan Zhao, Ming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) is crucial for language models to dynamically adapt
to the evolving real-world demands. To mitigate the catastrophic forgetting
problem in CL, data replay has been proven a simple and effective strategy, and
the subsequent data-replay-based distillation can further enhance the
performance. However, existing methods fail to fully exploit the knowledge
embedded in models from previous tasks, resulting in the need for a relatively
large number of replay samples to achieve good results. In this work, we first
explore and emphasize the importance of attention weights in knowledge
retention, and then propose a SElective attEntion-guided Knowledge Retention
method (SEEKR) for data-efficient replay-based continual learning of large
language models (LLMs). Specifically, SEEKR performs attention distillation on
the selected attention heads for finer-grained knowledge retention, where the
proposed forgettability-based and task-sensitivity-based measures are used to
identify the most valuable attention heads. Experimental results on two
continual learning benchmarks for LLMs demonstrate the superiority of SEEKR
over the existing methods on both performance and efficiency. Explicitly, SEEKR
achieves comparable or even better performance with only 1/10 of the replayed
data used by other methods, and reduces the proportion of replayed data to 1%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expansion Quantization Network: An Efficient Micro-emotion Annotation
  and Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhou, Senlin Luo, Haofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text emotion detection constitutes a crucial foundation for advancing
artificial intelligence from basic comprehension to the exploration of
emotional reasoning. Most existing emotion detection datasets rely on manual
annotations, which are associated with high costs, substantial subjectivity,
and severe label imbalances. This is particularly evident in the inadequate
annotation of micro-emotions and the absence of emotional intensity
representation, which fail to capture the rich emotions embedded in sentences
and adversely affect the quality of downstream task completion. By proposing an
all-labels and training-set label regression method, we map label values to
energy intensity levels, thereby fully leveraging the learning capabilities of
machine models and the interdependencies among labels to uncover multiple
emotions within samples. This led to the establishment of the Emotion
Quantization Network (EQN) framework for micro-emotion detection and
annotation. Using five commonly employed sentiment datasets, we conducted
comparative experiments with various models, validating the broad applicability
of our framework within NLP machine learning models. Based on the EQN
framework, emotion detection and annotation are conducted on the GoEmotions
dataset. A comprehensive comparison with the results from Google literature
demonstrates that the EQN framework possesses a high capability for automatic
detection and annotation of micro-emotions. The EQN framework is the first to
achieve automatic micro-emotion annotation with energy-level scores, providing
strong support for further emotion detection analysis and the quantitative
research of emotion computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From References to Insights: Collaborative Knowledge Minigraph <span class="highlight-title">Agent</span>s
  for Automating Scholarly Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zhang, Yan Liu, Sheng-hua Zhong, Gong Chen, Yu Yang, Jiannong Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Literature reviews play a crucial role in scientific research for
understanding the current state of research, identifying gaps, and guiding
future studies on specific topics. However, the process of conducting a
comprehensive literature review is yet time-consuming. This paper proposes a
novel framework, collaborative knowledge minigraph agents (CKMAs), to automate
scholarly literature reviews. A novel prompt-based algorithm, the knowledge
minigraph construction agent (KMCA), is designed to identify relationships
between information pieces from academic literature and automatically
constructs knowledge minigraphs. By leveraging the capabilities of large
language models on constructed knowledge minigraphs, the multiple path
summarization agent (MPSA) efficiently organizes information pieces and
relationships from different viewpoints to generate literature review
paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results
demonstrate that the proposed techniques generate informative, complete,
consistent, and insightful summaries for different research problems, promoting
the use of LLMs in more professional fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building an Efficient Multilingual Non-Profit IR System for the Islamic
  Domain Leveraging Multiprocessing Design in Rust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vera Pavlova, Mohammed Makhlouf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of large language models (LLMs) has dramatically improved
many applications of Natural Language Processing (NLP), including Information
Retrieval (IR). However, domains that are not driven by commercial interest
often lag behind in benefiting from AI-powered solutions. One such area is
religious and heritage corpora. Alongside similar domains, Islamic literature
holds significant cultural value and is regularly utilized by scholars and the
general public. Navigating this extensive amount of text is challenging, and
there is currently no unified resource that allows for easy searching of this
data using advanced AI tools. This work focuses on the development of a
multilingual non-profit IR system for the Islamic domain. This process brings a
few major challenges, such as preparing multilingual domain-specific corpora
when data is limited in certain languages, deploying a model on
resource-constrained devices, and enabling fast search on a limited budget. By
employing methods like continued pre-training for domain adaptation and
language reduction to decrease model size, a lightweight multilingual retrieval
model was prepared, demonstrating superior performance compared to larger
models pre-trained on general domain data. Furthermore, evaluating the proposed
architecture that utilizes Rust Language capabilities shows the possibility of
implementing efficient semantic search in a low-resource setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StopHC: A Harmful Content Detection and Mitigation Architecture for
  Social Media Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciprian-Octavian Truică, Ana-Teodora Constantinescu, Elena-Simona Apostol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mental health of social media users has started more and more to be put
at risk by harmful, hateful, and offensive content. In this paper, we propose
\textsc{StopHC}, a harmful content detection and mitigation architecture for
social media platforms. Our aim with \textsc{StopHC} is to create more secure
online environments. Our solution contains two modules, one that employs deep
neural network architecture for harmful content detection, and one that uses a
network immunization algorithm to block toxic nodes and stop the spread of
harmful content. The efficacy of our solution is demonstrated by experiments
conducted on two real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Reference Errors in Scientific Literature with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianmai M. Zhang, Neil F. Abernethy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference errors, such as citation and quotation errors, are common in
scientific papers. Such errors can result in the propagation of inaccurate
information, but are difficult and time-consuming to detect, posing a
significant challenge to scientific publishing. To support automatic detection
of reference errors, this work evaluated the ability of large language models
in OpenAI's GPT family to detect quotation errors. Specifically, we prepared an
expert-annotated, general-domain dataset of statement-reference pairs from
journal articles. Large language models were evaluated in different settings
with varying amounts of reference information provided by retrieval
augmentation. Our results showed that large language models are able to detect
erroneous citations with limited context and without fine-tuning. This study
contributes to the growing literature that seeks to utilize artificial
intelligence to assist in the writing, reviewing, and publishing of scientific
papers. Potential avenues for further improvements in this task are also
discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZhoBLiMP: a Systematic Assessment of <span class="highlight-title">Language Model</span>s with Linguistic
  Minimal Pairs in Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Liu, Yeting Shen, Hongao Zhu, Lilong Xu, Zhiheng Qian, Siyuan Song, Kejia Zhang, Jialong Tang, Pei Zhang, Baosong Yang, Rui Wang, Hai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether and how language models (LMs) acquire the syntax of natural languages
has been widely evaluated under the minimal pair paradigm. However, a lack of
wide-coverage benchmarks in languages other than English has constrained
systematic investigations into the issue. Addressing it, we first introduce
ZhoBLiMP, the most comprehensive benchmark of linguistic minimal pairs for
Chinese to date, with 118 paradigms, covering 15 linguistic phenomena. We then
train 20 LMs of different sizes (14M to 1.4B) on Chinese corpora of various
volumes (100M to 3B tokens) and evaluate them along with 14 off-the-shelf LLMs
on ZhoBLiMP. The overall results indicate that Chinese grammar can be mostly
learned by models with around 500M parameters, trained on 1B tokens with one
epoch, showing limited benefits for further scaling. Most (N=95) linguistic
paradigms are of easy or medium difficulty for LMs, while there are still 13
paradigms that remain challenging even for models with up to 32B parameters. In
regard to how LMs acquire Chinese grammar, we observe a U-shaped learning
pattern in several phenomena, similar to those observed in child language
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing <span class="highlight-title">Large Language Model</span>s through Quantization: A Comparative
  Analysis of PTQ and QAT Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jahid Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive analysis of quantization techniques for
optimizing Large Language Models (LLMs), specifically focusing on Post-Training
Quantization (PTQ) and Quantization-Aware Training (QAT). Through empirical
evaluation across models ranging from 10M to 1B parameters, we demonstrate that
quantization can achieve up to 68% reduction in model size while maintaining
performance within 6% of full-precision baselines when utilizing our proposed
scaling factor {\gamma}. Our experiments show that INT8 quantization delivers a
40% reduction in computational cost and power consumption, while INT4
quantization further improves these metrics by 60%. We introduce a novel
theoretical framework for mixed-precision quantization, deriving optimal bit
allocation strategies based on layer sensitivity and weight variance. Hardware
efficiency evaluations on edge devices reveal that our quantization approach
enables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60%
power reduction compared to full-precision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zyda-2: a 5 Trillion Token High-Quality <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present Zyda-2: a five trillion token dataset
for language model pretraining. Zyda-2 was used to train our Zamba2 series of
models which are state-of-the-art for their weight class. We build Zyda-2 by
collating high-quality open-source tokens such as FineWeb and DCLM, then
distilling them to the highest-quality subset via cross-deduplication and
model-based quality filtering. Zyda-2 is released under a permissive open
license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>initial upload 11/08/24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sufficient Context: A New Lens on Retrieval Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, Cyrus Rashtchian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a way to classify instances that have enough information to answer
the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the
context is sufficient, but often output incorrect answers instead of abstaining
when the context is not. On the other hand, open-source LLMs (Llama, Mistral,
Gemma) hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2-10% for Gemini, GPT,
and Gemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in <span class="highlight-title">LLM</span>
  Output 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elise Karinshak, Amanda Hu, Kewen Kong, Vishwanatha Rao, Jingren Wang, Jindong Wang, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immense effort has been dedicated to minimizing the presence of harmful or
biased generative content and better aligning AI output to human intention;
however, research investigating the cultural values of LLMs is still in very
early stages. Cultural values underpin how societies operate, providing
profound insights into the norms, priorities, and decision making of their
members. In recognition of this need for further research, we draw upon
cultural psychology theory and the empirically-validated GLOBE framework to
propose the LLM-GLOBE benchmark for evaluating the cultural value systems of
LLMs, and we then leverage the benchmark to compare the values of Chinese and
US LLMs. Our methodology includes a novel "LLMs-as-a-Jury" pipeline which
automates the evaluation of open-ended content to enable large-scale analysis
at a conceptual level. Results clarify similarities and differences that exist
between Eastern and Western cultural value systems and suggest that
open-generation tasks represent a more promising direction for evaluation of
cultural values. We interpret the implications of this research for subsequent
model development, evaluation, and deployment efforts as they relate to LLMs,
AI cultural alignment more broadly, and the influence of AI cultural value
systems on human-AI collaboration outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved intent classification based on context information using a
  windows-based approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeanfranco D. Farfan-Escobedo, Julio C. Dos Reis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational systems have a Natural Language Understanding (NLU) module. In
this module, there is a task known as an intent classification that aims at
identifying what a user is attempting to achieve from an utterance. Previous
works use only the current utterance to predict the intent of a given query and
they do not consider the role of the context (one or a few previous utterances)
in the dialog flow for this task. In this work, we propose several approaches
to investigate the role of contextual information for the intent classification
task. Each approach is used to carry out a concatenation between the dialogue
history and the current utterance. Our intent classification method is based on
a convolutional neural network that obtains effective vector representations
from BERT to perform accurate intent classification using an approach
window-based. Our experiments were carried out on a real-world Brazilian
Portuguese corpus with dialog flows provided by Wavy global company. Our
results achieved substantial improvements over the baseline, isolated
utterances (without context), in three approaches using the user's utterance
and system's response from previous messages as dialogue context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In preparation for Journal Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target-driven Attack for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhang, Mingyu Jin, Dong Shu, Taowen Wang, Dongfang Liu, Xiaobo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current large language models (LLM) provide a strong foundation for
large-scale user-oriented natural language tasks. Many users can easily inject
adversarial text or instructions through the user interface, thus causing LLM
model security challenges like the language model not giving the correct
answer. Although there is currently a large amount of research on black-box
attacks, most of these black-box attacks use random and heuristic strategies.
It is unclear how these strategies relate to the success rate of attacks and
thus effectively improve model robustness. To solve this problem, we propose
our target-driven black-box attack method to maximize the KL divergence between
the conditional probabilities of the clean text and the attack text to redefine
the attack's goal. We transform the distance maximization problem into two
convex optimization problems based on the attack goal to solve the attack text
and estimate the covariance. Furthermore, the projected gradient descent
algorithm solves the vector corresponding to the attack text. Our target-driven
black-box attack approach includes two attack strategies: token manipulation
and misinformation attack. Experimental results on multiple Large Language
Models and datasets demonstrate the effectiveness of our attack method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:2404.07234</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Findings of the Third Shared Task on Multilingual Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Novák, Barbora Dohnalová, Miloslav Konopík, Anna Nedoluzhko, Martin Popel, Ondřej Pražák, Jakub Sido, Milan Straka, Zdeněk Žabokrtský, Daniel Zeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents an overview of the third edition of the shared task on
multilingual coreference resolution, held as part of the CRAC 2024 workshop.
Similarly to the previous two editions, the participants were challenged to
develop systems capable of identifying mentions and clustering them based on
identity coreference.
  This year's edition took another step towards real-world application by not
providing participants with gold slots for zero anaphora, increasing the task's
complexity and realism. In addition, the shared task was expanded to include a
more diverse set of languages, with a particular focus on historical languages.
The training and evaluation data were drawn from version 1.2 of the
multilingual collection of harmonized coreference resources CorefUD,
encompassing 21 datasets across 15 languages. 6 systems competed in this shared
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on
Multilingual Coreference Resolution. In this third iteration of the shared
task, a novel objective is to also predict empty nodes needed for zero
coreference mentions (while the empty nodes were given on input in previous
years). This way, coreference resolution can be performed on raw text. We
evaluate two model variants: a~two-stage approach (where the empty nodes are
predicted first using a pretrained encoder model and then processed together
with sentence words by another pretrained model) and a single-stage approach
(where a single pretrained encoder model generates empty nodes, coreference
mentions, and coreference links jointly). In both settings, CorPipe surpasses
other participants by a large margin of 3.9 and 2.8 percent points,
respectively. The source code and the trained model are available at
https://github.com/ufal/crac2024-corpipe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchies over Vector Space: Orienting Word and Graph Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhi Guo, Steven Skiena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word and graph embeddings are widely used in deep learning applications. We
present a data structure that captures inherent hierarchical properties from an
unordered flat embedding space, particularly a sense of direction between pairs
of entities. Inspired by the notion of \textit{distributional generality}, our
algorithm constructs an arborescence (a directed rooted tree) by inserting
nodes in descending order of entity power (e.g., word frequency), pointing each
entity to the closest more powerful node as its parent.
  We evaluate the performance of the resulting tree structures on three tasks:
hypernym relation discovery, least-common-ancestor (LCA) discovery among words,
and Wikipedia page link recovery. We achieve average 8.98\% and 2.70\% for
hypernym and LCA discovery across five languages and 62.76\% accuracy on
directed Wiki-page link recovery, with both substantially above baselines.
Finally, we investigate the effect of insertion order, the power/similarity
trade-off and various power sources to optimize parent selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPAWNing Structural Priming Predictions from a Cognitively Motivated
  Parser <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grusha Prasad, Tal Linzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural priming is a widely used psycholinguistic paradigm to study human
sentence representations. In this work we introduce SPAWN, a cognitively
motivated parser that can generate quantitative priming predictions from
contemporary theories in syntax which assume a lexicalized grammar. By
generating and testing priming predictions from competing theoretical accounts,
we can infer which assumptions from syntactic theory are useful for
characterizing the representations humans build when processing sentences. As a
case study, we use SPAWN to generate priming predictions from two theories
(Whiz-Deletion and Participial-Phase) which make different assumptions about
the structure of English relative clauses. By modulating the reanalysis
mechanism that the parser uses and strength of the parser's prior knowledge, we
generated nine sets of predictions from each of the two theories. Then, we
tested these predictions using a novel web-based comprehension-to-production
priming paradigm. We found that while the some of the predictions from the
Participial-Phase theory aligned with human behavior, none of the predictions
from the the Whiz-Deletion theory did, thus suggesting that the
Participial-Phase theory might better characterize human relative clause
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of CoNLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled
  Dangling Cases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yin, Liyao Xiang, Dong Ding, Yuheng He, Yihan Wu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the entity alignment (EA) problem with unlabeled dangling
cases, meaning that partial entities have no counterparts in the other
knowledge graph (KG), and this type of entity remains unlabeled. To address
this challenge, we propose the framework \textit{Lambda} for dangling detection
and then entity alignment. Lambda features a GNN-based encoder called KEESA
with spectral contrastive learning for EA and a positive-unlabeled learning
algorithm for dangling detection called iPULE. iPULE offers theoretical
guarantees of unbiasedness, uniform deviation bounds, and convergence.
Experimental results demonstrate that each component contributes to overall
performances that are superior to baselines, even when baselines additionally
exploit 30\% of dangling entities labeled for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 as a poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Deep Unlearning in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Wu, Chhavi Yadav, Russ Salakhutdinov, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a key requirement of many data protection regulations
such as GDPR. Prior work on unlearning has mostly considered superficial
unlearning tasks where a single or a few related pieces of information are
required to be removed. However, the task of unlearning a fact is much more
challenging in recent large language models (LLMs), because the facts in LLMs
can be deduced from each other. In this work, we investigate whether current
unlearning methods for LLMs succeed beyond superficial unlearning of facts.
Specifically, we formally propose a framework and a definition for deep
unlearning facts that are interrelated. We design the metric, recall, to
quantify the extent of deep unlearning. To systematically evaluate deep
unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a
synthetic knowledge base of family relationships and biographies, together with
a realistic logical rule set that connects them. We use this dataset to test
four unlearning methods in four LLMs at different sizes. Our findings reveal
that in the task of deep unlearning only a single fact, they either fail to
properly unlearn with high recall, or end up unlearning many other irrelevant
facts. Our dataset and code are publicly available at:
https://github.com/wrh14/deep_unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenCoder: The Open Cookbook for Top-Tier Code <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) for code have become indispensable in various
domains, including code generation, reasoning tasks and agent systems. While
open-access code LLMs are increasingly approaching the performance levels of
proprietary models, high-quality code LLMs suitable for rigorous scientific
investigation, particularly those with reproducible data processing pipelines
and transparent training protocols, remain limited. The scarcity is due to
various challenges, including resource constraints, ethical considerations, and
the competitive advantages of keeping models advanced. To address the gap, we
introduce OpenCoder, a top-tier code LLM that not only achieves performance
comparable to leading models but also serves as an "open cookbook" for the
research community. Unlike most prior efforts, we release not only model
weights and inference code, but also the reproducible training data, complete
data processing pipeline, rigorous experimental ablation results, and detailed
training protocols for open scientific research. Through this comprehensive
release, we identify the key ingredients for building a top-tier code LLM: (1)
code optimized heuristic rules for data cleaning and methods for data
deduplication, (2) recall of text corpus related to code and (3) high-quality
synthetic data in both annealing and supervised fine-tuning stages. By offering
this level of openness, we aim to broaden access to all aspects of a top-tier
code LLM, with OpenCoder serving as both a powerful model and an open
foundation to accelerate research, and enable reproducible advancements in code
AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strategy<span class="highlight-title">LLM</span>: <span class="highlight-title">Large Language Model</span>s as Strategy Generators, Executors,
  Optimizers, and Evaluators for Problem Solving <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08803v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08803v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing prompting methods suffer from the issues of generalizability
and consistency, as they often rely on instance-specific solutions that may not
be applicable to other instances and lack task-level consistency across the
selected few-shot examples. To address these limitations, we propose a
comprehensive framework, StrategyLLM, allowing LLMs to perform inductive
reasoning, deriving general strategies from specific task instances, and
deductive reasoning, applying these general strategies to particular task
examples, for constructing generalizable and consistent few-shot prompts. It
employs four LLM-based agents: strategy generator, executor, optimizer, and
evaluator, working together to generate, evaluate, and select promising
strategies for a given task. Experimental results demonstrate that StrategyLLM
outperforms the competitive baseline CoT-SC that requires human-annotated
solutions on 13 datasets across 4 challenging tasks without human involvement,
including math reasoning (34.2\% $\rightarrow$ 38.8\%), commonsense reasoning
(70.3\% $\rightarrow$ 72.5\%), algorithmic reasoning (73.7\% $\rightarrow$
85.0\%), and symbolic reasoning (30.0\% $\rightarrow$ 79.2\%). Further analysis
reveals that StrategyLLM is applicable to various LLMs and demonstrates
advantages across numerous scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Implicit User Feedback, Emotions and Demographic
  Information in Task-Oriented and Document-Grounded Dialogues <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Petrak, Thy Thy Tran, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit user feedback, user emotions and demographic information have shown
to be promising sources for improving the accuracy and user engagement of
responses generated by dialogue systems. However, the influence of such
information on task completion and factual consistency, which are important
criteria for task-oriented and document-grounded dialogues, is not yet known.
To address this, we introduce FEDI, the first English task-oriented and
document-grounded dialogue dataset annotated with this information. Our
experiments with Flan-T5, GPT-2 and Llama 2 show a particularly positive impact
on task completion and factual consistency. Participants in our human
evaluation reported that the responses generated by the feedback-trained models
were more informative (Flan-T5 and GPT-2), relevant and factual consistent
(Llama 2).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and published in EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Low-Rank Adaptation Using IVON <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Cong, Nico Daheim, Yuesong Shen, Daniel Cremers, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 38th Workshop on Fine-Tuning in Machine Learning
  (NeurIPS 2024). Code available at
  https://github.com/team-approx-bayes/ivon-lora. In version 2 we fixed a typo
  in the equation of prior in section 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-Efficient Alignment for <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study methods for efficiently aligning large language models (LLMs) with
human preferences given budgeted online feedback. We first formulate the LLM
alignment problem in the frame of contextual dueling bandits. This formulation,
subsuming recent paradigms such as online RLHF and online DPO, inherently
quests for sample-efficient algorithms that incorporate online active
exploration. Leveraging insights from bandit theory, we introduce a unified
algorithm based on Thompson sampling and highlight its applications in two
distinct LLM alignment scenarios. The practical agent that efficiently
implements this algorithm, named SEA (Sample-Efficient Alignment), is
empirically validated through extensive experiments across three model scales
(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The
results demonstrate that SEA achieves highly sample-efficient alignment with
oracle's preferences, outperforming recent active exploration methods for LLMs.
Additionally, we release the implementation of SEA together with an efficient
codebase designed for online alignment of LLMs, aiming to accelerate future
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing <span class="highlight-title">Language Model</span>s on Their Knowledge Source <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zineddine Tighidet, Andrea Mogini, Jiali Mei, Benjamin Piwowarski, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often encounter conflicts between their learned,
internal (parametric knowledge, PK) and external knowledge provided during
inference (contextual knowledge, CK). Understanding how LLMs models prioritize
one knowledge source over the other remains a challenge. In this paper, we
propose a novel probing framework to explore the mechanisms governing the
selection between PK and CK in LLMs. Using controlled prompts designed to
contradict the model's PK, we demonstrate that specific model activations are
indicative of the knowledge source employed. We evaluate this framework on
various LLMs of different sizes and demonstrate that mid-layer activations,
particularly those related to relations in the input, are crucial in predicting
knowledge source selection, paving the way for more reliable models capable of
handling knowledge conflicts effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BlackBoxNLP@EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Give me a hint: Can <span class="highlight-title">LLM</span>s take a hint to solve math problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vansh Agrawal, Pratham Singla, Amitoj Singh Miglani, Shivank Garg, Ayush Mangal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While state-of-the-art LLMs have shown poor logical and basic mathematical
reasoning, recent works try to improve their problem-solving abilities using
prompting techniques. We propose giving "hints" to improve the language model's
performance on advanced mathematical problems, taking inspiration from how
humans approach math pedagogically. We also test robustness to adversarial
hints and demonstrate their sensitivity to them. We demonstrate the
effectiveness of our approach by evaluating various diverse LLMs, presenting
them with a broad set of problems of different difficulties and topics from the
MATH dataset and comparing against techniques such as one-shot, few-shot, and
chain of thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSI-<span class="highlight-title">Agent</span>: Incorporating Multi-Scale Insight into Embodied <span class="highlight-title">Agent</span>s for
  Superior Planning and Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term memory is significant for agents, in which insights play a crucial
role. However, the emergence of irrelevant insight and the lack of general
insight can greatly undermine the effectiveness of insight. To solve this
problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an
embodied agent designed to improve LLMs' planning and decision-making ability
by summarizing and utilizing insight effectively across different scales. MSI
achieves this through the experience selector, insight generator, and insight
selector. Leveraging a three-part pipeline, MSI can generate task-specific and
high-level insight, store it in a database, and then use relevant insight from
it to aid in decision-making. Our experiments show that MSI outperforms another
insight strategy when planning by GPT3.5. Moreover, We delve into the
strategies for selecting seed experience and insight, aiming to provide LLM
with more useful and relevant insight for better decision-making. Our
observations also indicate that MSI exhibits better robustness when facing
domain-shifting scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HERMES: <span class="highlight-title">temporal</span>-coHERent long-forM understanding with Episodes and
  Semantics <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17443v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17443v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research often treats long-form videos as extended short videos,
leading to several limitations: inadequate capture of long-range dependencies,
inefficient processing of redundant information, and failure to extract
high-level semantic concepts. To address these issues, we propose a novel
approach that more accurately reflects human cognition. This paper introduces
HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics,
a model that simulates episodic memory accumulation to capture action sequences
and reinforces them with semantic knowledge dispersed throughout the video. Our
work makes two key contributions: First, we develop an Episodic COmpressor
(ECO) that efficiently aggregates crucial representations from micro to
semi-macro levels, overcoming the challenge of long-range dependencies. Second,
we propose a Semantics ReTRiever (SeTR) that enhances these aggregated
representations with semantic information by focusing on the broader context,
dramatically reducing feature dimensionality while preserving relevant
macro-level information. This addresses the issues of redundancy and lack of
high-level concept extraction. Extensive experiments demonstrate that HERMES
achieves state-of-the-art performance across multiple long-video understanding
benchmarks in both zero-shot and fully-supervised settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an improved and expanded version of our EVAL-FoMo Workshop at
  ECCV'24 (v1 of this paper). Project page:
  https://joslefaure.github.io/assets/html/hermes.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai, Elizabeth Sherly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Efficient <span class="highlight-title">Transformer</span>s Really Save Computation? <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As transformer-based language models are trained on increasingly large
datasets and with vast numbers of parameters, finding more efficient
alternatives to the standard Transformer has become very valuable. While many
efficient Transformers and Transformer alternatives have been proposed, none
provide theoretical guarantees that they are a suitable replacement for the
standard Transformer. This makes it challenging to identify when to use a
specific model and what directions to prioritize for further investigation. In
this paper, we aim to understand the capabilities and limitations of efficient
Transformers, specifically the Sparse Transformer and the Linear Transformer.
We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)
prompts and follow previous works to model them as Dynamic Programming (DP)
problems. Our results show that while these models are expressive enough to
solve general DP tasks, contrary to expectations, they require a model size
that scales with the problem size. Nonetheless, we identify a class of DP
problems for which these models can be more efficient than the standard
Transformer. We confirm our theoretical results through experiments on
representative DP tasks, adding to the understanding of efficient Transformers'
practical strengths and weaknesses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, ICML 2024 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-guided <span class="highlight-title">Generative</span> <span class="highlight-title">Prompt</span> Injection Attack on <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07234v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07234v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhang, Mingyu Jin, Qinkai Yu, Chengzhi Liu, Haochen Xue, Xiaobo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current large language models (LLMs) provide a strong foundation for
large-scale user-oriented natural language tasks. A large number of users can
easily inject adversarial text or instructions through the user interface, thus
causing LLMs model security challenges. Although there is currently a large
amount of research on prompt injection attacks, most of these black-box attacks
use heuristic strategies. It is unclear how these heuristic strategies relate
to the success rate of attacks and thus effectively improve model robustness.
To solve this problem, we redefine the goal of the attack: to maximize the KL
divergence between the conditional probabilities of the clean text and the
adversarial text. Furthermore, we prove that maximizing the KL divergence is
equivalent to maximizing the Mahalanobis distance between the embedded
representation $x$ and $x'$ of the clean text and the adversarial text when the
conditional probability is a Gaussian distribution and gives a quantitative
relationship on $x$ and $x'$. Then we designed a simple and effective
goal-guided generative prompt injection strategy (G2PIA) to find an injection
text that satisfies specific constraints to achieve the optimal attack effect
approximately. It is particularly noteworthy that our attack method is a
query-free black-box attack method with low computational cost. Experimental
results on seven LLM models and four datasets show the effectiveness of our
attack method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GigaCheck: Detecting <span class="highlight-title">LLM</span>-generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Aleksandr Gordeev, Vladimir Dokholyan, Maksim Kuprashevich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing quality and spread of LLM-based assistants, the amount of
LLM-generated content is growing rapidly. In many cases and tasks, such texts
are already indistinguishable from those written by humans, and the quality of
generation tends to only increase. At the same time, detection methods are
developing more slowly, making it challenging to prevent misuse of generative
AI technologies.
  In this work, we investigate the task of generated text detection by
proposing the GigaCheck. Our research explores two approaches: (i)
distinguishing human-written texts from LLM-generated ones, and (ii) detecting
LLM-generated intervals in Human-Machine collaborative texts. For the first
task, our approach utilizes a general-purpose LLM, leveraging its extensive
language abilities to fine-tune efficiently for the downstream task of
LLM-generated text detection, achieving high performance even with limited
data. For the second task, we propose a novel approach that combines computer
vision and natural language processing techniques. Specifically, we use a
fine-tuned general-purpose LLM in conjunction with a DETR-like detection model,
adapted from computer vision, to localize AI-generated intervals within text.
  We evaluate the GigaCheck on five classification datasets with English texts
and three datasets designed for Human-Machine collaborative text analysis. Our
results demonstrate that GigaCheck outperforms previous methods, even in
out-of-distribution settings, establishing a strong baseline across all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepEdit: Knowledge Editing as Decoding with Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10471v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10471v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to edit the knowledge in multi-step reasoning has become the major
challenge in the knowledge editing (KE) of large language models (LLMs). The
difficulty arises because the hallucinations of LLMs during multi-step
reasoning often lead to incorrect use of new knowledge and incorrect answers.
To address this issue, we design decoding constraints to "regulate" LLMs'
reasoning, enhancing logical coherence when incorporating new knowledge. We
propose a new KE framework: DEEPEDIT (Depth-first Search-based Constrained
Decoding for Knowledge Editing), which enhances LLMs's ability to generate
coherent reasoning chains with new knowledge through depth-first search. Our
search selects the most important knowledge that satisfies our constraints as
the reasoning step to efficiently increase the reasoning depth. In addition to
DEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which
provide more precise and challenging assessments of KE approaches.
Qualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning
chains involving new knowledge. Quantitatively, it yields significant
improvements on multiple KE benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Upsample or Upweight? Balanced Training on Heavily Imbalanced <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjian Li, Haoran Xu, Weiting Tan, Kenton Murray, Daniel Khashabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data availability across domains often follows a long-tail distribution: a
few domains have abundant data, while most face dat . a scarcity. This
imbalance poses challenges in training language models uniformly across all
domains. In our study, we focus on multilingual settings, where data sizes vary
significantly between high- and low-resource languages. Common strategies to
address this include upsampling low-resource languages (Temperature Sampling)
or upweighting their loss (Scalarization). Although often considered
equivalent, this assumption has not been proven, which motivates our study.
Through both theoretical and empirical analysis, we identify the conditions
under which these approaches are equivalent and when they diverge.
Specifically, we demonstrate that these two methods are equivalent under full
gradient descent, but this equivalence breaks down with stochastic gradient
descent. Empirically, we observe that Temperature Sampling converges more
quickly but is prone to overfitting. We argue that this faster convergence is
likely due to the lower variance in gradient estimations, as shown
theoretically. Based on these insights, we propose Cooldown, a strategy that
reduces sampling temperature during training, accelerating convergence without
overfitting to low-resource languages. Our method is competitive with existing
data re-weighting and offers computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge <span class="highlight-title">Distill</span>ation of Black-Box <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Chen, Ruijun Chen, Yuqi Yi, Xiaojun Quan, Chenliang Li, Ming Yan, Ji Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the exceptional performance of proprietary large language models (LLMs)
like GPT-4, recent research has increasingly focused on boosting the
capabilities of smaller models through knowledge distillation (KD) from these
powerful yet black-box teachers. While leveraging the high-quality outputs of
these teachers is advantageous, the inaccessibility of their internal states
often limits effective knowledge transfer. To overcome this limitation, we
introduce Proxy-KD, a novel method that uses a proxy model to facilitate the
efficient transfer of knowledge from black-box LLMs to smaller models. Our
experiments show that Proxy-KD not only enhances the performance of KD from
black-box teacher models but also surpasses traditional white-box KD
techniques.~This approach presents a compelling new avenue for distilling
knowledge from advanced LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-MMD: Multi-Domain Multimodal <span class="highlight-title">Dataset</span> for Time Series Analysis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Kamarthi, Aditya B. Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang, B. Aditya Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data are ubiquitous across a wide range of real-world domains.
While real-world time series analysis (TSA) requires human experts to integrate
numerical series data with multimodal domain-specific knowledge, most existing
TSA models rely solely on numerical data, overlooking the significance of
information beyond numerical series. This oversight is due to the untapped
potential of textual series data and the absence of a comprehensive,
high-quality multimodal dataset. To overcome this obstacle, we introduce
Time-MMD, the first multi-domain, multimodal time series dataset covering 9
primary data domains. Time-MMD ensures fine-grained modality alignment,
eliminates data contamination, and provides high usability. Additionally, we
develop MM-TSFlib, the first multimodal time-series forecasting (TSF) library,
seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth
analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib
demonstrate significant performance enhancements by extending unimodal TSF to
multimodality, evidenced by over 15% mean squared error reduction in general,
and up to 40% in domains with rich textual data. More importantly, our datasets
and library revolutionize broader applications, impacts, research topics to
advance TSA. The dataset and library are available at
https://github.com/AdityaLab/Time-MMD and
https://github.com/AdityaLab/MM-TSFlib.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STORYSUMM: Evaluating Faithfulness in Story <span class="highlight-title">Summarization</span> <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Subbiah, Faisal Ladhak, Akankshya Mishra, Griffin Adams, Lydia B. Chilton, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation has been the gold standard for checking faithfulness in
abstractive summarization. However, with a challenging source domain like
narrative, multiple annotators can agree a summary is faithful, while missing
details that are obvious errors only once pointed out. We therefore introduce a
new dataset, STORYSUMM, comprising LLM summaries of short stories with
localized faithfulness labels and error explanations. This benchmark is for
evaluation methods, testing whether a given method can detect challenging
inconsistencies. Using this dataset, we first show that any one human
annotation protocol is likely to miss inconsistencies, and we advocate for
pursuing a range of methods when establishing ground truth for a summarization
dataset. We finally test recent automatic metrics and find that none of them
achieve more than 70% balanced accuracy on this task, demonstrating that it is
a challenging benchmark for future work in faithfulness evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Main 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Agent</span>Ohana: Design Unified Data and Training Pipeline for Effective
  <span class="highlight-title">Agent</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15506v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15506v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang, Zuxin Liu, Liangwei Yang, Yihao Feng, Shirley Kokane, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks. Begin the
exploration at \url{https://github.com/SalesforceAIResearch/xLAM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add GitHub repo link at
  \url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link
  at \url{https://huggingface.co/Salesforce/xLAM-v0.1-r}</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuidelineGuard: An <span class="highlight-title">Agent</span>ic Framework for Medical Note Evaluation with
  Guideline Adherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MD Ragib Shahriyear
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although rapid advancements in Large Language Models (LLMs) are facilitating
the integration of artificial intelligence-based applications and services in
healthcare, limited research has focused on the systematic evaluation of
medical notes for guideline adherence. This paper introduces GuidelineGuard, an
agentic framework powered by LLMs that autonomously analyzes medical notes,
such as hospital discharge and office visit notes, to ensure compliance with
established healthcare guidelines. By identifying deviations from recommended
practices and providing evidence-based suggestions, GuidelineGuard helps
clinicians adhere to the latest standards from organizations like the WHO and
CDC. This framework offers a novel approach to improving documentation quality
and reducing clinical errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotative Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces annotative indexing, a novel framework that unifies and
generalizes traditional inverted indexes, column stores, object stores, and
graph databases. As a result, annotative indexing can provide the underlying
indexing framework for databases that support knowledge graphs, entity
retrieval, semi-structured data, and ranked retrieval. While we primarily focus
on human language data in the form of text, annotative indexing is sufficiently
general to support a range of other datatypes, and we provide examples of
SQL-like queries over a JSON store that includes numbers and dates. Taking
advantage of the flexibility of annotative indexing, we also demonstrate a
fully dynamic annotative index incorporating support for ACID properties of
transactions with hundreds of multiple concurrent readers and writers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking
  with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier, Juntao Li, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) like Llama has
significantly advanced information retrieval (IR) systems. However, using LLMs
for long documents, as in RankLLaMA, remains challenging due to computational
complexity, especially concerning input token length. Furthermore, the internal
mechanisms of LLMs during ranking are still not fully understood. In this
paper, we first explore the internal workings of LLMs during relevance
judgement and identify that specific attention heads play a crucial role in
aligning relevant tokens. This observation inspires us to revisit the block
pre-ranking strategy used in KeyB, which remains state-of-the-art (SOTA) on the
TREC 2019 DL document ranking dataset. Building on these insights, we develop
KeyB2, an advanced long document IR approach that integrates block pre-ranking
with the performance of LLMs. KeyB2 efficiently identifies and processes the
most relevant blocks, reducing computational costs and improving ranking
effectiveness. Additionally, we introduce a new bi-encoder block matching
strategy for KeyB2. Comprehensive experiments on long-document datasets,
including TREC 2019 DL, Robust04, and MLDR-zh, show that KeyB2 outperforms
baselines like RankLLaMA and KeyB by reducing reranking time and GPU memory
usage while enhancing retrieval performance, achieving new SOTA results on TREC
2019 DL with higher NDCG@10 and MAP scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Retrieval-Augmented Generation for University Knowledge
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, Afsaneh Fatemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table, Submitted to 15th IKT conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpret the Internal States of Recommendation Model with Sparse
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Wang, Xiaoyu Zhang, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable recommendation systems are important to enhance transparency,
accuracy, and fairness. Beyond result-level explanations, model-level
interpretations can provide valuable insights that allow developers to optimize
system designs and implement targeted improvements. However, most current
approaches depend on specialized model designs, which often lack generalization
capabilities. Given the various kinds of recommendation models, existing
methods have limited ability to effectively interpret them. To address this
issue, we propose RecSAE, an automatic, generalizable probing method for
interpreting the internal states of Recommendation models with Sparse
AutoEncoder. RecSAE serves as a plug-in module that does not affect original
models during interpretations, while also enabling predictable modifications to
their behaviors based on interpretation results. Firstly, we train an
autoencoder with sparsity constraints to reconstruct internal activations of
recommendation models, making the RecSAE latents more interpretable and
monosemantic than the original neuron activations. Secondly, we automated the
construction of concept dictionaries based on the relationship between latent
activations and input item sequences. Thirdly, RecSAE validates these
interpretations by predicting latent activations on new item sequences using
the concept dictionary and deriving interpretation confidence scores from
precision and recall. We demonstrate RecSAE's effectiveness on two datasets,
identifying hundreds of highly interpretable concepts from pure ID-based
models. Latent ablation studies further confirm that manipulating latent
concepts produces corresponding changes in model output behavior, underscoring
RecSAE's utility for both understanding and targeted tuning recommendation
models. Code and data are publicly available at
https://github.com/Alice1998/RecSAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Snippet-based Conversational Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Sun, Naoki Otani, Hannah Kim, Dan Zhang, Nikita Bhutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRS) engage users in interactive
dialogues to gather preferences and provide personalized recommendations.
Traditionally, CRS rely on pre-defined attributes or expensive, domain-specific
annotated datasets to guide conversations, which limits flexibility and
adaptability across domains. In this work, we introduce SnipRec, a novel CRS
that enhances dialogues and recommendations by extracting diverse expressions
and preferences from user-generated content (UGC) like customer reviews. Using
large language models, SnipRec maps user responses and UGC to concise snippets,
which are used to generate clarification questions and retrieve relevant items.
Our approach eliminates the need for domain-specific training, making it
adaptable to new domains and effective without prior knowledge of user
preferences. Extensive experiments on the Yelp dataset demonstrate the
effectiveness of snippet-based representations against document and
sentence-based representations. Additionally, SnipRec is able to improve
Hits@10 by 0.25 over the course of five conversational turns, underscoring the
efficiency of SnipRec in capturing user preferences through multi-turn
conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled
  Dangling Cases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yin, Liyao Xiang, Dong Ding, Yuheng He, Yihan Wu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the entity alignment (EA) problem with unlabeled dangling
cases, meaning that partial entities have no counterparts in the other
knowledge graph (KG), and this type of entity remains unlabeled. To address
this challenge, we propose the framework \textit{Lambda} for dangling detection
and then entity alignment. Lambda features a GNN-based encoder called KEESA
with spectral contrastive learning for EA and a positive-unlabeled learning
algorithm for dangling detection called iPULE. iPULE offers theoretical
guarantees of unbiasedness, uniform deviation bounds, and convergence.
Experimental results demonstrate that each component contributes to overall
performances that are superior to baselines, even when baselines additionally
exploit 30\% of dangling entities labeled for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 as a poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Learnable Clustering for Intent Learning in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05975v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05975v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Xinwang Liu, Shengju Yu, Kejun Zhang, Wenliang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent learning, which aims to learn users' intents for user understanding
and item recommendation, has become a hot research spot in recent years.
However, existing methods suffer from complex and cumbersome alternating
optimization, limiting performance and scalability. To this end, we propose a
novel intent learning method termed \underline{ELCRec}, by unifying behavior
representation learning into an \underline{E}nd-to-end \underline{L}earnable
\underline{C}lustering framework, for effective and efficient
\underline{Rec}ommendation. Concretely, we encode user behavior sequences and
initialize the cluster centers (latent intents) as learnable neurons. Then, we
design a novel learnable clustering module to separate different cluster
centers, thus decoupling users' complex intents. Meanwhile, it guides the
network to learn intents from behaviors by forcing behavior embeddings close to
cluster centers. This allows simultaneous optimization of recommendation and
clustering via mini-batch data. Moreover, we propose intent-assisted
contrastive learning by using cluster centers as self-supervision signals,
further enhancing mutual promotion. Both experimental results and theoretical
analyses demonstrate the superiority of ELCRec from six perspectives. Compared
to the runner-up, ELCRec improves NDCG@5 by 8.9\% and reduces computational
costs by 22.5\% on the Beauty dataset. Furthermore, due to the scalability and
universal applicability, we deploy this method on the industrial recommendation
system with 130 million page views and achieve promising results. The codes are
available on GitHub (https://github.com/yueliu1999/ELCRec). A collection
(papers, codes, datasets) of deep group recommendation/intent learning methods
is available on GitHub
(https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-08T00:00:00Z">2024-11-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">106</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recycled Attention: Efficient inference for long-context <span class="highlight-title">language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Xu, Tanya Goyal, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating long sequences of tokens given a long-context input imposes a
heavy computational burden for large language models (LLMs). One of the
computational bottleneck comes from computing attention over a long sequence of
input at each generation step. In this paper, we propose Recycled Attention, an
inference-time method which alternates between full context attention and
attention over a subset of input tokens. When performing partial attention, we
recycle the attention pattern of a previous token that has performed full
attention and attend only to the top K most attended tokens, reducing the cost
of data movement and attention computation. Compared to previously proposed
inference-time acceleration method which attends only to local context or
tokens with high accumulative attention scores, our approach flexibly chooses
tokens that are relevant to the current decoding step. We evaluate our methods
on RULER, a suite of tasks designed to comprehensively evaluate long-context
abilities, and long-context language modeling tasks. Applying our method to
off-the-shelf LLMs achieves comparable speedup to baselines which only consider
local context while improving the performance by 2x. We further explore two
ideas to improve performance-efficiency trade-offs: (1) dynamically decide when
to perform recycled or full attention step based on the query similarities and
(2) continued pre-training the model with Recycled Attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASL STEM Wiki: <span class="highlight-title">Dataset</span> and Benchmark for Interpreting STEM Articles <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayo Yin, Chinmay Singh, Fyodor O. Minakov, Vanessa Milan, Hal Daumé III, Cyril Zhang, Alex X. Lu, Danielle Bragg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deaf and hard-of-hearing (DHH) students face significant barriers in
accessing science, technology, engineering, and mathematics (STEM) education,
notably due to the scarcity of STEM resources in signed languages. To help
address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia
articles on STEM topics in English, interpreted into over 300 hours of American
Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset
focused on STEM, facilitating the development of AI resources for STEM
education in ASL. We identify several use cases of ASL STEM Wiki with
human-centered applications. For example, because this dataset highlights the
frequent use of fingerspelling for technical concepts, which inhibits DHH
students' ability to learn, we develop models to identify fingerspelled words
-- which can later be used to query for appropriate ASL signs to suggest to
interpreters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using <span class="highlight-title">Language Model</span>s to Disambiguate Lexical Choices in Translation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Barua, Sanjay Subramanian, Kayo Yin, Alane Suhr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In translation, a concept represented by a single word in a source language
can have multiple variations in a target language. The task of lexical
selection requires using context to identify which variation is most
appropriate for a source text. We work with native speakers of nine languages
to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual
concept variation when translating from English. We evaluate recent LLMs and
neural machine translation systems on DTAiLS, with the best-performing model,
GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use
language models to generate English rules describing target-language concept
variations. Providing weaker models with high-quality lexical rules improves
accuracy substantially, in some cases reaching or outperforming GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s as Method Actors: A Model for <span class="highlight-title">Prompt</span> Engineering and Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Doyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce "Method Actors" as a mental model for guiding LLM prompt
engineering and prompt architecture. Under this mental model, LLMs should be
thought of as actors; prompts as scripts and cues; and LLM responses as
performances. We apply this mental model to the task of improving LLM
performance at playing Connections, a New York Times word puzzle game that
prior research identified as a challenging benchmark for evaluating LLM
reasoning. Our experiments with GPT-4o show that a "Method Actors" approach can
significantly improve LLM performance over both a vanilla and "Chain of
Thoughts" approach. A vanilla approach solves 27% of Connections puzzles in our
dataset and a "Chain of Thoughts" approach solves 41% of puzzles, whereas our
strongest "Method Actor" approach solves 86% of puzzles. We also test OpenAI's
newest model designed specifically for complex reasoning tasks, o1-preview.
When asked to solve a puzzle all at once, o1-preview solves 79% of Connections
puzzles in our dataset, and when allowed to build puzzle solutions one guess at
a time over multiple API calls, o1-preview solves 100% of the puzzles.
Incorporating a "Method Actor" prompt architecture increases the percentage of
puzzles that o1-preview solves perfectly from 76% to 87%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative Assessment of Intersectional Empathetic Bias and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Formanek, Ondrej Sotolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing amount of literature critiques the current operationalizations of
empathy based on loose definitions of the construct. Such definitions
negatively affect dataset quality, model robustness, and evaluation
reliability. We propose an empathy evaluation framework that operationalizes
empathy close to its psychological origins. The framework measures the variance
in responses of LLMs to prompts using existing metrics for empathy and
emotional valence. The variance is introduced through the controlled generation
of the prompts by varying social biases affecting context understanding, thus
impacting empathetic understanding. The control over generation ensures high
theoretical validity of the constructs in the prompt dataset. Also, it makes
high-quality translation, especially into languages that currently have
little-to-no way of evaluating empathy or bias, such as the Slavonic family,
more manageable. Using chosen LLMs and various prompt types, we demonstrate the
empathy evaluation with the framework, including multiple-choice answers and
free generation. The variance in our initial evaluation sample is small and we
were unable to measure convincing differences between the empathetic
understanding in contexts given by different social groups. However, the
results are promising because the models showed significant alterations their
reasoning chains needed to capture the relatively subtle changes in the
prompts. This provides the basis for future research into the construction of
the evaluation sample and statistical methods for measuring the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact or Fiction? Can <span class="highlight-title">LLM</span>s be Reliable Annotators for Political Truths? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronica Chatrath, Marcelo Lotif, Shaina Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political misinformation poses significant challenges to democratic
processes, shaping public opinion and trust in media. Manual fact-checking
methods face issues of scalability and annotator bias, while machine learning
models require large, costly labelled datasets. This study investigates the use
of state-of-the-art large language models (LLMs) as reliable annotators for
detecting political factuality in news articles. Using open-source LLMs, we
create a politically diverse dataset, labelled for bias through LLM-generated
annotations. These annotations are validated by human experts and further
evaluated by LLM-based judges to assess the accuracy and reliability of the
annotations. Our approach offers a scalable and robust alternative to
traditional fact-checking, enhancing transparency and public trust in media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Socially Responsible Language Modelling Research (SoLaR)
  Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinDVer: Explainable Claim Verification over Long and Hybrid-Content
  Financial Documents <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zhao, Yitao Long, Yuru Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Yiming Zhang, Xiangru Tang, Chen Zhao, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FinDVer, a comprehensive benchmark specifically designed to
evaluate the explainable claim verification capabilities of LLMs in the context
of understanding and analyzing long, hybrid-content financial documents.
FinDVer contains 2,400 expert-annotated examples, divided into three subsets:
information extraction, numerical reasoning, and knowledge-intensive reasoning,
each addressing common scenarios encountered in real-world financial contexts.
We assess a broad spectrum of LLMs under long-context and RAG settings. Our
results show that even the current best-performing system, GPT-4o, still lags
behind human experts. We further provide in-depth analysis on long-context and
RAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering
insights to drive future advancements. We believe that FinDVer can serve as a
valuable benchmark for evaluating LLMs in claim verification over complex,
expert-domain documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024 <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Malon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Separating disinformation from fact on the web has long challenged both the
search and the reasoning powers of humans. We show that the reasoning power of
large language models (LLMs) and the retrieval power of modern search engines
can be combined to automate this process and explainably verify claims. We
integrate LLMs and search under a multi-hop evidence pursuit strategy. This
strategy generates an initial question based on an input claim using a sequence
to sequence model, searches and formulates an answer to the question, and
iteratively generates follow-up questions to pursue the evidence that is
missing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)
shared task. Compared to a strategy of generating all the questions at once,
our method obtains .045 higher label accuracy and .155 higher AVeriTeC score
(evaluating the adequacy of the evidence). Through ablations, we show the
importance of various design choices, such as the question generation method,
medium-sized context, reasoning with one document at a time, adding metadata,
paraphrasing, reducing the problem to two classes, and reconsidering the final
verdict. Our submitted system achieves .510 AVeriTeC score on the dev set and
.477 AVeriTeC score on the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Seventh FEVER Workshop at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Navigation with Vision <span class="highlight-title">Language Model</span>s: Transforming Spatial
  Reasoning into Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Goetting, Himanshu Gaurav Singh, Antonio Loquercio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VLMnav, an embodied framework to transform a Vision-Language Model
(VLM) into an end-to-end navigation policy. In contrast to prior work, we do
not rely on a separation between perception, planning, and control; instead, we
use a VLM to directly select actions in one step. Surprisingly, we find that a
VLM can be used as an end-to-end policy zero-shot, i.e., without any
fine-tuning or exposure to navigation data. This makes our approach open-ended
and generalizable to any downstream navigation task. We run an extensive study
to evaluate the performance of our approach in comparison to baseline prompting
methods. In addition, we perform a design analysis to understand the most
impactful design decisions. Visual examples and code for our project can be
found at https://jirl-upenn.github.io/VLMnav/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FisherMask: Enhancing Neural Network Labeling Efficiency in Image
  Classification Using Fisher Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreen Gul, Mohamed Elmahallawy, Sanjay Madria, Ardhendu Tripathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models are popular across various domains due to their
remarkable performance and efficiency. However, their effectiveness relies
heavily on large amounts of labeled data, which are often time-consuming and
labor-intensive to generate manually. To overcome this challenge, it is
essential to develop strategies that reduce reliance on extensive labeled data
while preserving model performance. In this paper, we propose FisherMask, a
Fisher information-based active learning (AL) approach that identifies key
network parameters by masking them based on their Fisher information values.
FisherMask enhances batch AL by using Fisher information to select the most
critical parameters, allowing the identification of the most impactful samples
during AL training. Moreover, Fisher information possesses favorable
statistical properties, offering valuable insights into model behavior and
providing a better understanding of the performance characteristics within the
AL pipeline. Our extensive experiments demonstrate that FisherMask
significantly outperforms state-of-the-art methods on diverse datasets,
including CIFAR-10 and FashionMNIST, especially under imbalanced settings.
These improvements lead to substantial gains in labeling efficiency. Hence
serving as an effective tool to measure the sensitivity of model parameters to
data samples. Our code is available on
\url{https://github.com/sgchr273/FisherMask}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aioli: A Unified Optimization Framework for <span class="highlight-title">Language Model</span> Data Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image2Text2Image: A Novel Framework for Label-Free Evaluation of
  Image-to-Text Generation with Text-to-Image <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the quality of automatically generated image descriptions is a
complex task that requires metrics capturing various dimensions, such as
grammaticality, coverage, accuracy, and truthfulness. Although human evaluation
provides valuable insights, its cost and time-consuming nature pose
limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr
attempt to fill this gap, but they often exhibit weak correlations with human
judgment. To address this challenge, we propose a novel evaluation framework
called Image2Text2Image, which leverages diffusion models, such as Stable
Diffusion or DALL-E, for text-to-image generation. In the Image2Text2Image
framework, an input image is first processed by a selected image captioning
model, chosen for evaluation, to generate a textual description. Using this
generated description, a diffusion model then creates a new image. By comparing
features extracted from the original and generated images, we measure their
similarity using a designated similarity metric. A high similarity score
suggests that the model has produced a faithful textual description, while a
low score highlights discrepancies, revealing potential weaknesses in the
model's performance. Notably, our framework does not rely on human-annotated
reference captions, making it a valuable tool for assessing image captioning
models. Extensive experiments and human evaluations validate the efficacy of
our proposed Image2Text2Image evaluation framework. The code and dataset will
be published to support further research in the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2408.01723</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asterisk*: Keep it Simple 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Semenov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes Asterisk, a compact GPT-based model for generating text
embeddings. The model uses a minimalist architecture with two layers, two
attention heads, and 256 embedding dimensions. By applying knowledge
distillation from larger pretrained models, we explore the trade-offs between
model size and performance while minimizing computational and memory
requirements. The model is primarily evaluated and optimized for classification
tasks, with experimental results showing its moderate performance in zero-shot
classification across various downstream applications. With additional
configuration, the model performance can approach or even surpass that of
larger architectures on specific classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unmasking the Limits of <span class="highlight-title">Large Language Model</span>s: A Systematic Evaluation
  of Masked Text Processing Ability through MskQA and MskCal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuka Matsuzaki, Haru-Tada Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper sheds light on the limitations of Large Language Models (LLMs) by
rigorously evaluating their ability to process masked text. We introduce two
novel tasks: MskQA, measuring reasoning on masked question-answering datasets
like RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic
problems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some
resilience to masked text, their performance is highly contingent on masking
rates and semantic cues. Specifically, "solid masking," where semantic clues
are entirely absent, leads to a significant performance drop compared to
"partial lifting," where some semantic information is retained, indicating
LLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently
outperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to
handle numerical reasoning with masked text. This underscores the crucial role
of semantic cues in the reasoning process of LLMs. Our study illuminates the
interplay between background knowledge and reasoning ability in masked text
processing, paving the way for a deeper understanding of LLM capabilities and
limitations, and highlighting the need for more robust evaluation methods to
accurately assess their true comprehension abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating <span class="highlight-title">Large Language Model</span> Capability in Vietnamese Fact-Checking
  Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Truong To, Hung Tuan Le, Dat Van-Thanh Nguyen, Manh Trong Nguyen, Tri Thien Nguyen, Tin Van Huynh, Kiet Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), with gradually improving reading comprehension
and reasoning capabilities, are being applied to a range of complex language
tasks, including the automatic generation of language data for various
purposes. However, research on applying LLMs for automatic data generation in
low-resource languages like Vietnamese is still underdeveloped and lacks
comprehensive evaluation. In this paper, we explore the use of LLMs for
automatic data generation for the Vietnamese fact-checking task, which faces
significant data limitations. Specifically, we focus on fact-checking data
where claims are synthesized from multiple evidence sentences to assess the
information synthesis capabilities of LLMs. We develop an automatic data
construction process using simple prompt techniques on LLMs and explore several
methods to improve the quality of the generated data. To evaluate the quality
of the data generated by LLMs, we conduct both manual quality assessments and
performance evaluations using language models. Experimental results and manual
evaluations illustrate that while the quality of the generated data has
significantly improved through fine-tuning techniques, LLMs still cannot match
the data quality produced by humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Open-Source <span class="highlight-title">Large Language Model</span>s on Argumentation Mining
  Subtasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Yeghaneh Abkenar, Weixing Wang, Hendrik Graupner, Manfred Stede
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the capability of four open-sourcelarge language models (LLMs) in
argumentation mining (AM). We conduct experiments on three different corpora;
persuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based
on two argumentation mining sub-tasks: (i) argumentative discourse units
classifications (ADUC), and (ii) argumentative relation classification (ARC).
This work aims to assess the argumentation capability of open-source LLMs,
including Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot
and few-shot scenarios. Our analysis contributes to further assessing
computational argumentation with open-source LLMs in future research efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Fake News on Social Media Towards Public Users of Different
  Age Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kahlil bin Abdul Hakim, Sathishkumar Veerappampalayam Easwaramoorthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines how fake news affects social media users across a range
of age groups and how machine learning (ML) and artificial intelligence (AI)
can help reduce the spread of false information. The paper evaluates various
machine learning models for their efficacy in identifying and categorizing fake
news and examines current trends in the spread of fake news, including deepfake
technology. The study assesses four models using a Kaggle dataset: Random
Forest, Support Vector Machine (SVM), Neural Networks, and Logistic Regression.
The results show that SVM and neural networks perform better than other models,
with accuracies of 93.29% and 93.69%, respectively. The study also emphasises
how people in the elder age group diminished capacity for critical analysis of
news content makes them more susceptible to disinformation. Natural language
processing (NLP) and deep learning approaches have the potential to improve the
accuracy of false news detection. Biases in AI and ML models and difficulties
in identifying information generated by AI continue to be major problems in
spite of the developments. The study recommends that datasets be expanded to
encompass a wider range of languages and that detection algorithms be
continuously improved to keep up with the latest advancements in disinformation
tactics. In order to combat fake news and promote an informed and resilient
society, this study emphasizes the value of cooperative efforts between AI
researchers, social media platforms, and governments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Adapting <span class="highlight-title">Large Language Model</span>s to Represent Folktales in
  Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JA Meaney, Beatrice Alex, William Lamb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Folktales are a rich resource of knowledge about the society and culture of a
civilisation. Digital folklore research aims to use automated techniques to
better understand these folktales, and it relies on abstract representations of
the textual data. Although a number of large language models (LLMs) claim to be
able to represent low-resource langauges such as Irish and Gaelic, we present
two classification tasks to explore how useful these representations are, and
three adaptations to improve the performance of these models. We find that
adapting the models to work with longer sequences, and continuing pre-training
on the domain of folktales improves classification performance, although these
findings are tempered by the impressive performance of a baseline SVM with
non-contextual features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Answerability of Queries in Retrieval-Augmented Code
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonmin Kim, Jaeyeon Kim, Hancheol Park, Wooksu Shin, Tae-Ho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to unprecedented language understanding and generation capabilities of
large language model (LLM), Retrieval-augmented Code Generation (RaCG) has
recently been widely utilized among software developers. While this has
increased productivity, there are still frequent instances of incorrect codes
being provided. In particular, there are cases where plausible yet incorrect
codes are generated for queries from users that cannot be answered with the
given queries and API descriptions. This study proposes a task for evaluating
answerability, which assesses whether valid answers can be generated based on
users' queries and retrieved APIs in RaCG. Additionally, we build a benchmark
dataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to
evaluate the performance of models performing this task. Experimental results
show that this task remains at a very challenging level, with baseline models
exhibiting a low performance of 46.7%. Furthermore, this study discusses
methods that could significantly improve performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Good is Your Wikipedia? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Tatariya, Artur Kulmizev, Wessel Poelman, Esther Ploeger, Marcel Bollmann, Johannes Bjerva, Jiaming Luo, Heather Lent, Miryam de Lhoneux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia's perceived high quality and broad language coverage have
established it as a fundamental resource in multilingual NLP. In the context of
low-resource languages, however, these quality assumptions are increasingly
being scrutinised. This paper critically examines the data quality of Wikipedia
in a non-English setting by subjecting it to various quality filtering
techniques, revealing widespread issues such as a high percentage of one-line
articles and duplicate articles. We evaluate the downstream impact of quality
filtering on Wikipedia and find that data quality pruning is an effective means
for resource-efficient training without hurting performance, especially for
low-resource languages. Moreover, we advocate for a shift in perspective from
seeking a general definition of data quality towards a more language- and
task-specific one. Ultimately, we aim for this study to serve as a guide to
using Wikipedia for pretraining in a multilingual setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LBPE: Long-token-first Tokenization to Improve <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lian, Yizhe Xiong, Zijia Lin, Jianwei Niu, Shasha Mo, Hui Chen, Peng Liu, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)
facilitates robust handling of subword units and avoids issues of
out-of-vocabulary words. Despite its success, a critical challenge persists:
long tokens, rich in semantic information, have fewer occurrences in tokenized
datasets compared to short tokens, which can result in imbalanced learning
issue across different tokens. To address that, we propose LBPE, which
prioritizes long tokens during the encoding process. LBPE generates tokens
according to their reverse ranks of token length rather than their ranks in the
vocabulary, granting longer tokens higher priority during the encoding process.
Consequently, LBPE smooths the frequency differences between short and long
tokens, and thus mitigates the learning imbalance. Extensive experiments across
diverse language modeling tasks demonstrate that LBPE consistently outperforms
the original BPE, well demonstrating its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.17808</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KyrgyzNLP: Challenges, Progress, and Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Alekseev, Timur Turatali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have excelled in numerous benchmarks, advancing
AI applications in both linguistic and non-linguistic tasks. However, this has
primarily benefited well-resourced languages, leaving less-resourced ones
(LRLs) at a disadvantage. In this paper, we highlight the current state of the
NLP field in the specific LRL: kyrgyz tili.
  Human evaluation, including annotated datasets created by native speakers,
remains an irreplaceable component of reliable NLP performance, especially for
LRLs where automatic evaluations can fall short. In recent assessments of the
resources for Turkic languages, Kyrgyz is labeled with the status 'Scraping
By', a severely under-resourced language spoken by millions. This is concerning
given the growing importance of the language, not only in Kyrgyzstan but also
among diaspora communities where it holds no official status.
  We review prior efforts in the field, noting that many of the publicly
available resources have only recently been developed, with few exceptions
beyond dictionaries (the processed data used for the analysis is presented at
https://kyrgyznlp.github.io/). While recent papers have made some headway, much
more remains to be done. Despite interest and support from both business and
government sectors in the Kyrgyz Republic, the situation for Kyrgyz language
resources remains challenging. We stress the importance of community-driven
efforts to build these resources, ensuring the future advancement
sustainability. We then share our view of the most pressing challenges in
Kyrgyz NLP. Finally, we propose a roadmap for future development in terms of
research topics and language resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keynote talk at the 12th International Conference on Analysis of
  Images, Social Networks and Texts (AIST-2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EUREKHA: Enhancing User Representation for Key Hackers Identification in
  Underground Forums 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdoul Nasser Hassane Amadou, Anas Motii, Saida Elouardi, EL Houcine Bergou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underground forums serve as hubs for cybercriminal activities, offering a
space for anonymity and evasion of conventional online oversight. In these
hidden communities, malicious actors collaborate to exchange illicit knowledge,
tools, and tactics, driving a range of cyber threats from hacking techniques to
the sale of stolen data, malware, and zero-day exploits. Identifying the key
instigators (i.e., key hackers), behind these operations is essential but
remains a complex challenge. This paper presents a novel method called EUREKHA
(Enhancing User Representation for Key Hacker Identification in Underground
Forums), designed to identify these key hackers by modeling each user as a
textual sequence. This sequence is processed through a large language model
(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.
These extracted features are then fed into a Graph Neural Network (GNN) to
model user structural relationships, significantly improving identification
accuracy. Furthermore, we employ BERTopic (Bidirectional Encoder
Representations from Transformers Topic Modeling) to extract personalized
topics from user-generated content, enabling multiple textual representations
per user and optimizing the selection of the most representative sequence. Our
study demonstrates that fine-tuned LLMs outperform state-of-the-art methods in
identifying key hackers. Additionally, when combined with GNNs, our model
achieves significant improvements, resulting in approximately 6% and 10%
increases in accuracy and F1-score, respectively, over existing methods.
EUREKHA was tested on the Hack-Forums dataset, and we provide open-source
access to our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Trustcom 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supporting Automated Fact-checking across Topics: Similarity-driven
  Gradual Topic Learning for Claim Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amani S. Abumansour, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting check-worthy claims for fact-checking is considered a crucial part
of expediting the fact-checking process by filtering out and ranking the
check-worthy claims for being validated among the impressive amount of claims
could be found online. The check-worthy claim detection task, however, becomes
more challenging when the model needs to deal with new topics that differ from
those seen earlier. In this study, we propose a domain-adaptation framework for
check-worthy claims detection across topics for the Arabic language to adopt a
new topic, mimicking a real-life scenario of the daily emergence of events
worldwide. We propose the Gradual Topic Learning (GTL) model, which builds an
ability to learning gradually and emphasizes the check-worthy claims for the
target topic during several stages of the learning process. In addition, we
introduce the Similarity-driven Gradual Topic Learning (SGTL) model that
synthesizes gradual learning with a similarity-based strategy for the target
topic. Our experiments demonstrate the effectiveness of our proposed model,
showing an overall tendency for improving performance over the state-of-the-art
baseline across 11 out of the 14 topics under study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Workflow<span class="highlight-title">LLM</span>: Enhancing Workflow Orchestration Capability of Large
  <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengda Fan, Xin Cong, Yuepeng Fu, Zhong Zhang, Shuyan Zhang, Yuanwei Liu, Yesai Wu, Yankai Lin, <span class="highlight-author">Zhiyuan Liu</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have driven a
revolutionary paradigm shift in process automation from Robotic Process
Automation to Agentic Process Automation by automating the workflow
orchestration procedure based on LLMs. However, existing LLMs (even the
advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in
workflow orchestration. To address this limitation, we present WorkflowLLM, a
data-centric framework elaborately designed to enhance the capability of LLMs
in workflow orchestration. It first constructs a large-scale fine-tuning
dataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83
applications across 28 categories. Specifically, the construction process can
be divided into three phases: (1) Data Collection: we collect real-world
workflow data from Apple Shortcuts and RoutineHub, transcribing them into
Python-style code. We further equip them with generated hierarchical thought
via ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task
queries to enrich the diversity and complexity of workflows. (3) Workflow
Generation: we leverage an annotator model trained on collected data to
generate workflows for synthesized queries. Finally, we merge the synthetic
samples that pass quality confirmation with the collected samples to obtain the
WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain
WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong
capacity to orchestrate complex workflows, while also achieving notable
generalization performance on previously unseen APIs. Additionally,
WorkflowBench exhibits robust zero-shot generalization capabilities on an
out-of-distribution task planning dataset, T-Eval. Our data and code are
available at https://github.com/OpenBMB/WorkflowLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VISTA: Visual Integrated System for Tailored Automation in Math Problem
  Generation Using <span class="highlight-title">LLM</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwoo Lee, Kwangsuk Park, Jihyeon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate and consistent visual aids is a critical challenge in
mathematics education, where visual representations like geometric shapes and
functions play a pivotal role in enhancing student comprehension. This paper
introduces a novel multi-agent framework that leverages Large Language Models
(LLMs) to automate the creation of complex mathematical visualizations
alongside coherent problem text. Our approach not only simplifies the
generation of precise visual aids but also aligns these aids with the problem's
core mathematical concepts, improving both problem creation and assessment. By
integrating multiple agents, each responsible for distinct tasks such as
numeric calculation, geometry validation, and visualization, our system
delivers mathematically accurate and contextually relevant problems with visual
aids. Evaluation across Geometry and Function problem types shows that our
method significantly outperforms basic LLMs in terms of text coherence,
consistency, relevance and similarity, while maintaining the essential
geometrical and functional integrity of the original problems. Although some
challenges remain in ensuring consistent visual outputs, our framework
demonstrates the immense potential of LLMs in transforming the way educators
generate and utilize visual aids in math education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop on Large Foundation Models for
  Educational Assessment (FM-Assess)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the rules of peptide self-assembly through data mining with
  <span class="highlight-title">large language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenze Yang, Sarah K. Yorke, Tuomas P. J. Knowles, Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peptides are ubiquitous and important biologically derived molecules, that
have been found to self-assemble to form a wide array of structures. Extensive
research has explored the impacts of both internal chemical composition and
external environmental stimuli on the self-assembly behaviour of these systems.
However, there is yet to be a systematic study that gathers this rich
literature data and collectively examines these experimental factors to provide
a global picture of the fundamental rules that govern protein self-assembly
behavior. In this work, we curate a peptide assembly database through a
combination of manual processing by human experts and literature mining
facilitated by a large language model. As a result, we collect more than 1,000
experimental data entries with information about peptide sequence, experimental
conditions and corresponding self-assembly phases. Utilizing the collected
data, ML models are trained and evaluated, demonstrating excellent accuracy
(>80\%) and efficiency in peptide assembly phase classification. Moreover, we
fine-tune our GPT model for peptide literature mining with the developed
dataset, which exhibits markedly superior performance in extracting information
from academic publications relative to the pre-trained model. We find that this
workflow can substantially improve efficiency when exploring potential
self-assembling peptide candidates, through guiding experimental work, while
also deepening our understanding of the mechanisms governing peptide
self-assembly. In doing so, novel structures can be accessed for a range of
applications including sensing, catalysis and biomaterials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gap-Filling <span class="highlight-title">Prompt</span>ing Enhances Code-Assisted Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ghiasvand Mohammadkhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the strong performance of large language models (LLMs) in tasks like
mathematical reasoning, their practical use is limited by high computational
demands and proprietary restrictions. Chain-of-thought (CoT) and
program-of-thought (PoT) fine-tuning are common methods to transfer LLM
knowledge to small language models (SLMs). However, CoT often leads to
calculation errors in SLMs, while PoT has shown more promise. While most
PoT-based approaches focus on direct problem-to-code conversion or extracting
only the key information from questions and then providing code solution for
it, this work emphasizes filling the gaps in the question to clearly illustrate
the solution path, which can be challenging for an SLM to understand when such
information is not explicitly provided. Therefore, this paper introduces
Gap-Filling Prompting (GFP), a novel two-step prompting strategy designed to
enhance the problem-solving process for SLMs. The first step identifies these
gaps and provides hints for filling them, while the second step adds the hints
to the question to generate a final code solution. Experimental results on two
benchmark datasets demonstrate that GFP significantly improves the mathematical
reasoning abilities of SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Distributional Alignment of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole Meister, Carlos Guestrin, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are increasingly used as simulacra for people, yet
their ability to match the distribution of views of a specific demographic
group and be \textit{distributionally aligned} remains uncertain. This notion
of distributional alignment is complex, as there is significant variation in
the types of attributes that are simulated. Prior works have underexplored the
role of three critical variables -- the question domain, steering method, and
distribution expression method -- which motivates our contribution of a
benchmark explicitly addressing these dimensions. We construct a dataset
expanding beyond political values, create human baselines for this task, and
evaluate the extent to which an LM can align with a particular group's opinion
distribution to inform design choices of such simulation systems. Our analysis
reveals open problems regarding if, and how, LMs can be used to simulate
humans, and that LLMs can more accurately describe the opinion distribution
than simulate such distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Low-Resource Harmful Meme Detection with LMM <span class="highlight-title">Agent</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Internet memes in the age of social media necessitates
effective identification of harmful ones. Due to the dynamic nature of memes,
existing data-driven models may struggle in low-resource scenarios where only a
few labeled examples are available. In this paper, we propose an agency-driven
framework for low-resource harmful meme detection, employing both outward and
inward analysis with few-shot annotated samples. Inspired by the powerful
capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first
retrieve relative memes with annotations to leverage label information as
auxiliary signals for the LMM agent. Then, we elicit knowledge-revising
behavior within the LMM agent to derive well-generalized insights into meme
harmfulness. By combining these strategies, our approach enables dialectical
reasoning over intricate and implicit harm-indicative patterns. Extensive
experiments conducted on three meme datasets demonstrate that our proposed
approach achieves superior performance than state-of-the-art methods on the
low-resource harmful meme detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word reuse and combination support efficient communication of emerging
  concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aotao Xu, Charles Kemp, Lea Frermann, Yang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key function of the lexicon is to express novel concepts as they emerge
over time through a process known as lexicalization. The most common
lexicalization strategies are the reuse and combination of existing words, but
they have typically been studied separately in the areas of word meaning
extension and word formation. Here we offer an information-theoretic account of
how both strategies are constrained by a fundamental tradeoff between competing
communicative pressures: word reuse tends to preserve the average length of
word forms at the cost of less precision, while word combination tends to
produce more informative words at the expense of greater word length. We test
our proposal against a large dataset of reuse items and compounds that appeared
in English, French and Finnish over the past century. We find that these
historically emerging items achieve higher levels of communicative efficiency
than hypothetical ways of constructing the lexicon, and both literal reuse
items and compounds tend to be more efficient than their non-literal
counterparts. These results suggest that reuse and combination are both
consistent with a unified account of lexicalization grounded in the theory of
efficient communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the National Academy of Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for
  Measuring the Capabilities of Spoken <span class="highlight-title">Language Model</span>s with 180 Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Fabian Ritter-Gutierrez, Ming To Chuang, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. Guimarães, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized
human-machine interactions by seamlessly integrating various forms of data.
Developing a universal spoken language model that comprehends a wide range of
natural language instructions is critical for bridging communication gaps and
facilitating more intuitive interactions. However, the absence of a
comprehensive evaluation benchmark poses a significant challenge. We present
Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive
evaluation of instruction-based universal speech models. Building upon the
first generation, this second version incorporates 125 new tasks contributed
collaboratively by the global research community, expanding the benchmark to a
total of 180 tasks, making it the largest benchmark for speech and audio
evaluation. While the first generation of Dynamic-SUPERB was limited to
classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation
capabilities by introducing a wide array of novel and diverse tasks, including
regression and sequence generation, across speech, music, and environmental
audio. Evaluation results indicate that none of the models performed well
universally. SALMONN-13B excelled in English ASR, while WavLLM demonstrated
high accuracy in emotion recognition, but current models still require further
innovations to handle a broader range of tasks. We will soon open-source all
task data and the evaluation pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Robustness of <span class="highlight-title">LLM</span>s to Adversarial Typographical Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esther Gan, Yiran Zhao, Liying Cheng, Yancan Mao, Anirudh Goyal, Kenji Kawaguchi, Min-Yen Kan, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by
users' instruction. In this work, we study the reasoning robustness of LLMs to
typographical errors, which can naturally occur in users' queries. We design an
Adversarial Typo Attack ($\texttt{ATA}$) algorithm that iteratively samples
typos for words that are important to the query and selects the edit that is
most likely to succeed in attacking. It shows that LLMs are sensitive to
minimal adversarial typographical changes. Notably, with 1 character edit,
Mistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8
character edits the performance further drops to 19.2%. To extend our
evaluation to larger and closed-source LLMs, we develop the $\texttt{R$^2$ATA}$
benchmark, which assesses models' $\underline{R}$easoning
$\underline{R}$obustness to $\underline{\texttt{ATA}}$. It includes adversarial
typographical questions derived from three widely used reasoning
datasets-GSM8K, BBH, and MMLU-by applying $\texttt{ATA}$ to open-source LLMs.
$\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable
performance drops across multiple super large and closed-source LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multi-Domain Task-Oriented Dialogue System with Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dharmendra Prajapat, Durga Toshniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) system is designed to accomplish user-defined
tasks through dialogues. The TOD system has progressed towards end-to-end
modeling by leveraging pre-trained large language models. Fine-tuning the
pre-trained language models using only supervised learning leads to the
exposure bias and token loss problem and it deviates the models from completing
the user's task. To address these issues, we propose a TOD system that
leverages a unified pre-trained language model, GPT2, as a base model. It is
optimized using supervised learning and reinforcement learning (RL). The issues
in the TOD system are mitigated using a non-differentiable reward function. The
reward is calculated using the weighted sum of the success rate and BLEU
evaluation metrics. The success rate and BLEU metrics in reward calculation
guide the language model for user task completion while ensuring a coherent and
fluent response. Our model is acquired by fine-tuning a pre-trained model on
the dialogue-session level which comprises user utterance, belief state, system
act, and system response. Experimental results on MultiWOZ2.1 demonstrate that
our model increases the inform rate by 1.60% and the success rate by 3.17%
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciDQA: A Deep Reading Comprehension <span class="highlight-title">Dataset</span> over Scientific Papers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shruti Singh, Nandan Sarkar, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific literature is typically dense, requiring significant background
knowledge and deep comprehension for effective engagement. We introduce SciDQA,
a new dataset for reading comprehension that challenges LLMs for a deep
understanding of scientific articles, consisting of 2,937 QA pairs. Unlike
other scientific QA datasets, SciDQA sources questions from peer reviews by
domain experts and answers by paper authors, ensuring a thorough examination of
the literature. We enhance the dataset's quality through a process that
carefully filters out lower quality questions, decontextualizes the content,
tracks the source document across different versions, and incorporates a
bibliography for multi-document question-answering. Questions in SciDQA
necessitate reasoning across figures, tables, equations, appendices, and
supplementary materials, and require multi-document reasoning. We evaluate
several open-source and proprietary LLMs across various configurations to
explore their capabilities in generating relevant and factual responses. Our
comprehensive evaluation, based on metrics for surface-level similarity and LLM
judgements, highlights notable performance discrepancies. SciDQA represents a
rigorously curated, naturally derived scientific QA dataset, designed to
facilitate research on complex scientific text understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Sun, Tianyi Zhou, Xun Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become essential in advancing natural
language processing (NLP) tasks, but their sequential token generation limits
inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising
solution by using a smaller draft model to generate multiple token sequences,
which the target LLM verifies in parallel. However, current heuristic
approaches, such as Recursive Rejection Sampling (RRS), suffer from low
acceptance rates in subsequent drafts, limiting the advantages of using
multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can
theoretically improve acceptance rates, but its computational cost is too high
for real-time use. We present SpecHub, a novel, efficient sampling-verification
method for MDSD that improves acceptance rates with only linear computational
overhead. By simplifying the OTM problem into a compact Linear Programming
model, SpecHub significantly reduces computational complexity. It further
accelerates sampling by leveraging a sparse joint distribution, focusing
computation on high-probability token sequences. In extensive experiments,
Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step
than RRS and RRS without replacement. We attach our code at
\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fox-1 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Hu, Jipeng Zhang, Rui Pan, Zhaozhuo Xu, Salman Avestimehr, Chaoyang He, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Fox-1, a series of small language models (SLMs) consisting of
Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3
trillion tokens of web-scraped document data and fine-tuned with 5 billion
tokens of instruction-following and multi-turn conversation data. Aiming to
improve the pre-training efficiency, Fox-1-1.6B model introduces a novel
3-stage data curriculum across all the training data with 2K-8K sequence
length. In architecture design, Fox-1 features a deeper layer structure, an
expanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a
performant and efficient architecture compared to other SLMs. Fox-1 achieves
better or on-par performance in various benchmarks compared to StableLM-2-1.6B,
Gemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and
throughput. The model weights have been released under the Apache 2.0 license,
where we aim to promote the democratization of LLMs and make them fully
accessible to the whole open-source community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Base model is available at
  https://huggingface.co/tensoropera/Fox-1-1.6B and the instruction-tuned
  version is available at
  https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Robustness of Watermarking to Paraphrasing Attacks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saksham Rastogi, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst rising concerns about the internet being proliferated with content
generated from language models (LMs), watermarking is seen as a principled way
to certify whether text was generated from a model. Many recent watermarking
techniques slightly modify the output probabilities of LMs to embed a signal in
the generated output that can later be detected. Since early proposals for text
watermarking, questions about their robustness to paraphrasing have been
prominently discussed. Lately, some techniques are deliberately designed and
claimed to be robust to paraphrasing. However, such watermarking schemes do not
adequately account for the ease with which they can be reverse-engineered. We
show that with access to only a limited number of generations from a black-box
watermarked model, we can drastically increase the effectiveness of
paraphrasing attacks to evade watermark detection, thereby rendering the
watermark ineffective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination
  Detection Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Thomas, Seth Rosen, Vishnu Vettrivel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comparative analysis of hallucination detection systems
for AI, focusing on automatic summarization and question answering tasks for
Large Language Models (LLMs). We evaluate different hallucination detection
systems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.
Our results indicate that although advanced models can perform better they come
at a much higher cost. We also demonstrate how an ideal hallucination detection
system needs to maintain performance across different model sizes. Our findings
highlight the importance of choosing a detection system aligned with specific
application needs and resource constraints. Future research will explore hybrid
systems and automated identification of underperforming components to enhance
AI reliability and efficiency in detecting and mitigating hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pags, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Report Generators: A Cyclic Vision-Language Adapter for
  Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Yijian Gao, Junzhi Ning, Zhiling Yue, Zhi Li, Simon LF Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in report generation methods, a critical
limitation remains: the lack of interpretability in the generated text. This
paper introduces an innovative approach to enhance the explainability of text
generated by report generation models. Our method employs cyclic text
manipulation and visual comparison to identify and elucidate the features in
the original content that influence the generated text. By manipulating the
generated reports and producing corresponding images, we create a comparative
framework that highlights key attributes and their impact on the text
generation process. This approach not only identifies the image features
aligned to the generated text but also improves transparency but also provides
deeper insights into the decision-making mechanisms of the report generation
models. Our findings demonstrate the potential of this method to significantly
enhance the interpretability and transparency of AI-generated reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What talking you?: Translating Code-Mixed Messaging Texts to English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynnette Hui Xian Ng, Luo Qi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translation of code-mixed texts to formal English allow a wider audience to
understand these code-mixed languages, and facilitate downstream analysis
applications such as sentiment analysis. In this work, we look at translating
Singlish, which is colloquial Singaporean English, to formal standard English.
Singlish is formed through the code-mixing of multiple Asian languages and
dialects. We analysed the presence of other Asian languages and variants which
can facilitate translation. Our dataset is short message texts, written as
informal communication between Singlish speakers. We use a multi-step prompting
scheme on five Large Language Models (LLMs) for language detection and
translation. Our analysis show that LLMs do not perform well in this task, and
we describe the challenges involved in translation of code-mixed languages. We
also release our dataset in this link https://github.com/luoqichan/singlish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Dark Patterns of Personalized Persuasion in <span class="highlight-title">Large Language Model</span>s:
  Exposing Persuasive Linguistic Features for Big Five Personality Traits in
  <span class="highlight-title">LLM</span>s Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiktoria Mieleszczenko-Kowszewicz, Dawid Płudowski, Filip Kołodziejczyk, Jakub Świstak, Julian Sienkiewicz, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores how the Large Language Models (LLMs) adjust linguistic
features to create personalized persuasive outputs. While research showed that
LLMs personalize outputs, a gap remains in understanding the linguistic
features of their persuasive capabilities. We identified 13 linguistic features
crucial for influencing personalities across different levels of the Big Five
model of personality. We analyzed how prompts with personality trait
information influenced the output of 19 LLMs across five model families. The
findings show that models use more anxiety-related words for neuroticism,
increase achievement-related words for conscientiousness, and employ fewer
cognitive processes words for openness to experience. Some model families excel
at adapting language for openness to experience, others for conscientiousness,
while only one model adapts language for neuroticism. Our findings show how
LLMs tailor responses based on personality cues in prompts, indicating their
potential to create persuasive content affecting the mind and well-being of the
recipients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUIDEQ: Framework for Guided Questioning for progressive informational
  collection and classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya Mishra, Suraj Racha, Kaustubh Ponkshe, Adit Akarsh, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) is an important part of tasks like text
classification through information gathering. These are finding increasing use
in sectors like healthcare, customer support, legal services, etc., to collect
and classify responses into actionable categories. LLMs, although can support
QA systems, they face a significant challenge of insufficient or missing
information for classification. Although LLMs excel in reasoning, the models
rely on their parametric knowledge to answer. However, questioning the user
requires domain-specific information aiding to collect accurate information.
Our work, GUIDEQ, presents a novel framework for asking guided questions to
further progress a partial information. We leverage the explainability derived
from the classifier model for along with LLMs for asking guided questions to
further enhance the information. This further information helps in more
accurate classification of a text. GUIDEQ derives the most significant
key-words representative of a label using occlusions. We develop GUIDEQ's
prompting strategy for guided questions based on the top-3 classifier label
outputs and the significant words, to seek specific and relevant information,
and classify in a targeted manner. Through our experimental results, we
demonstrate that GUIDEQ outperforms other LLM-based baselines, yielding
improved F1-Score through the accurate collection of relevant further
information. We perform various analytical studies and also report better
question quality compared to our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game-theoretic <span class="highlight-title">LLM</span>: <span class="highlight-title">Agent</span> Workflow for Negotiation Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Reward Optimization for Machine Translation using Error
  Severity Mappings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Moura Ramos, Tomás Almeida, Daniel Vareta, Filipe Azevedo, Sweta Agrawal, Patrick Fernandes, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been proven to be an effective and robust
method for training neural machine translation systems, especially when paired
with powerful reward models that accurately assess translation quality.
However, most research has focused on RL methods that use sentence-level
feedback, which leads to inefficient learning signals due to the reward
sparsity problem -- the model receives a single score for the entire sentence.
To address this, we introduce a novel approach that leverages fine-grained
token-level reward mechanisms with RL methods. We use xCOMET, a
state-of-the-art quality estimation system as our token-level reward model.
xCOMET provides detailed feedback by predicting fine-grained error spans and
their severity given source-translation pairs. We conduct experiments on small
and large translation datasets to compare the impact of sentence-level versus
fine-grained reward signals on translation quality. Our results show that
training with token-level rewards improves translation quality across language
pairs over baselines according to automatic and human evaluation. Furthermore,
token-level reward optimization also improves training stability, evidenced by
a steady increase in mean rewards over training epochs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, work-in-progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactLens: Benchmarking Fine-Grained Fact Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushan Mitra, Dan Zhang, Sajjadur Rahman, Estevam Hruschka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive capability in language
generation and understanding, but their tendency to hallucinate and produce
factually incorrect information remains a key limitation. To verify
LLM-generated contents and claims from other sources, traditional verification
approaches often rely on holistic models that assign a single factuality label
to complex claims, potentially obscuring nuanced errors. In this paper, we
advocate for a shift toward fine-grained verification, where complex claims are
broken down into smaller sub-claims for individual verification, allowing for
more precise identification of inaccuracies, improved transparency, and reduced
ambiguity in evidence retrieval. However, generating sub-claims poses
challenges, such as maintaining context and ensuring semantic equivalence with
respect to the original claim. We introduce FactLens, a benchmark for
evaluating fine-grained fact verification, with metrics and automated
evaluators of sub-claim quality. The benchmark data is manually curated to
ensure high-quality ground truth. Our results show alignment between automated
FactLens evaluators and human judgments, and we discuss the impact of sub-claim
characteristics on the overall verification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Empirical Impact of Data Sanitization on <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwesan Pal, Radhika Bhargava, Kyle Hinsz, Jacques Esterhuizen, Sudipta Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sanitization in the context of language modeling involves identifying
sensitive content, such as personally identifiable information (PII), and
redacting them from a dataset corpus. It is a common practice used in natural
language processing (NLP) to maintain privacy. Nevertheless, the impact of data
sanitization on the language understanding capability of a language model
remains less studied. This paper empirically analyzes the effects of data
sanitization across several benchmark language-modeling tasks including
comprehension question answering (Q&A), entailment, sentiment analysis, and
text classification. Our experiments cover a wide spectrum comprising
finetuning small-scale language models, to prompting large language models
(LLMs), on both original and sanitized datasets, and comparing their
performance across the tasks. Interestingly, our results suggest that for some
tasks such as sentiment analysis or entailment, the impact of redaction is
quite low, typically around 1-5%, while for tasks such as comprehension Q&A
there is a big drop of >25% in performance observed in redacted queries as
compared to the original. For tasks that have a higher impact, we perform a
deeper dive to inspect the presence of task-critical entities. Finally, we
investigate correlation between performance and number of redacted entities,
and also suggest a strategy to repair an already redacted dataset by means of
content-based subsampling. Additional details are available at
https://sites.google.com/view/datasan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at Safe Generative AI Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Transdisciplinary Approaches to Audio Deepfake Discernment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vandana P. Janeja, Christine Mallinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This perspective calls for scholars across disciplines to address the
challenge of audio deepfake detection and discernment through an
interdisciplinary lens across Artificial Intelligence methods and linguistics.
With an avalanche of tools for the generation of realistic-sounding fake speech
on one side, the detection of deepfakes is lagging on the other. Particularly
hindering audio deepfake detection is the fact that current AI models lack a
full understanding of the inherent variability of language and the complexities
and uniqueness of human speech. We see the promising potential in recent
transdisciplinary work that incorporates linguistic knowledge into AI
approaches to provide pathways for expert-in-the-loop and to move beyond expert
agnostic AI-based methods for more robust and comprehensive deepfake detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis of Cyberbullying Data in Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvapalli Sai Susmitha, Pradeep Pujari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media has become an integral part of modern life, but it has also
brought with it the pervasive issue of cyberbullying a serious menace in
today's digital age. Cyberbullying, a form of harassment that occurs on social
networks, has escalated alongside the growth of these platforms. Sentiment
analysis holds significant potential not only for detecting bullying phrases
but also for identifying victims who are at high risk of harm, whether to
themselves or others. Our work focuses on leveraging deep learning and natural
language understanding techniques to detect traces of bullying in social media
posts. We developed a Recurrent Neural Network with Long Short-Term Memory
(LSTM) cells, using different embeddings. One approach utilizes BERT
embeddings, while the other replaces the embeddings layer with the recently
released embeddings API from OpenAI. We conducted a performance comparison
between these two approaches to evaluate their effectiveness in sentiment
analysis of Formspring Cyberbullying data. Our Code is Available at
https://github.com/ppujari/xcs224u
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeKo: Toward Post Recognition <span class="highlight-title">Generative</span> Correction Large Language
  Models with Task-Oriented Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Ting Lin, Chao-Han Huck Yang, Zhehuai Chen, Piotr Zelasko, Xuesong Yang, Zih-Ching Chen, Krishna C Puvvada, Szu-Wei Fu, Ke Hu, Jun Wei Chiu, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction of a general-purpose post-recognition error corrector poses a
crucial question: how can we most effectively train a model on a large mixture
of domain datasets? The answer would lie in learning dataset-specific features
and digesting their knowledge in a single model. Previous methods achieve this
by having separate correction language models, resulting in a significant
increase in parameters. In this work, we present Mixture-of-Experts as a
solution, highlighting that MoEs are much more than a scalability tool. We
propose a Multi-Task Correction MoE, where we train the experts to become an
``expert'' of speech-to-text, language-to-text and vision-to-text datasets by
learning to route each dataset's tokens to its mapped expert. Experiments on
the Open ASR Leaderboard show that we explore a new state-of-the-art
performance by achieving an average relative $5.0$% WER reduction and
substantial improvements in BLEU scores for speech and translation tasks. On
zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to
$27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs
competitively on grammar and post-OCR correction as a multi-task model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeKo work has been done in June 2024. NeKo LMs will be open source on
  https://huggingface.co/nvidia under the MIT license</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying artificial intelligence through algebraic generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Ito, Murray Campbell, Lior Horesh, Tim Klinger, Parikshit Ram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of modern artificial intelligence (AI) systems has
created an urgent need for their scientific quantification. While their fluency
across a variety of domains is impressive, modern AI systems fall short on
tests requiring symbolic processing and abstraction - a glaring limitation
given the necessity for interpretable and reliable technology. Despite a surge
of reasoning benchmarks emerging from the academic community, no comprehensive
and theoretically-motivated framework exists to quantify reasoning (and more
generally, symbolic ability) in AI systems. Here, we adopt a framework from
computational complexity theory to explicitly quantify symbolic generalization:
algebraic circuit complexity. Many symbolic reasoning problems can be recast as
algebraic expressions. Thus, algebraic circuit complexity theory - the study of
algebraic expressions as circuit models (i.e., directed acyclic graphs) - is a
natural framework to study the complexity of symbolic computation. The tools of
algebraic circuit complexity enable the study of generalization by defining
benchmarks in terms of their complexity-theoretic properties (i.e., the
difficulty of a problem). Moreover, algebraic circuits are generic mathematical
objects; for a given algebraic circuit, an arbitrarily large number of samples
can be generated for a specific circuit, making it an optimal testbed for the
data-hungry machine learning algorithms that are used today. Here, we adopt
tools from algebraic circuit complexity theory, apply it to formalize a science
of symbolic generalization, and address key theoretical and empirical
challenges for its successful application to AI science and its impact on the
broader community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucination with ZeroG: An Advanced Knowledge Management
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anantha Sharma, Sheeba Elizabeth John, Fatemeh Rezapoor Nikroo, Krupali Bhatt, Mrunal Zambre, Aditi Wikhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of digital documents presents significant challenges in efficient
management and knowledge extraction. Traditional methods often struggle with
complex documents, leading to issues such as hallucinations and high latency in
responses from Large Language Models (LLMs). ZeroG, an innovative approach,
significantly mitigates these challenges by leveraging knowledge distillation
and prompt tuning to enhance model performance.
  ZeroG utilizes a smaller model that replicates the behavior of a larger
teacher model, ensuring contextually relevant and grounded responses, by
employing a black-box distillation approach, it creates a distilled dataset
without relying on intermediate features, optimizing computational efficiency.
This method significantly enhances accuracy and reduces response times,
providing a balanced solution for modern document management.
  Incorporating advanced techniques for document ingestion and metadata
utilization, ZeroG improves the accuracy of question-and-answer systems. The
integration of graph databases and robust metadata management further
streamlines information retrieval, allowing for precise and context-aware
responses. By transforming how organizations interact with complex data, ZeroG
enhances productivity and user experience, offering a scalable solution for the
growing demands of digital document management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BERTrend: Neural Topic Modeling for Emerging Trends Detection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allaa Boutaleb, Jerome Picault, Guillaume Grosjean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking emerging trends and weak signals in large, evolving
text corpora is vital for applications such as monitoring scientific
literature, managing brand reputation, surveilling critical infrastructure and
more generally to any kind of text-based event detection. Existing solutions
often fail to capture the nuanced context or dynamically track evolving
patterns over time. BERTrend, a novel method, addresses these limitations using
neural topic modeling in an online setting. It introduces a new metric to
quantify topic popularity over time by considering both the number of documents
and update frequency. This metric classifies topics as noise, weak, or strong
signals, flagging emerging, rapidly growing topics for further investigation.
Experimentation on two large real-world datasets demonstrates BERTrend's
ability to accurately detect and track meaningful weak signals while filtering
out noise, offering a comprehensive solution for monitoring emerging trends in
large-scale, evolving text corpora. The method can also be used for
retrospective analysis of past events. In addition, the use of Large Language
Models together with BERTrend offers efficient means for the interpretability
of trends of events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, FuturED 2024: Workshop on Future of Event
  Detection (CoLocated with EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Distraction in Long-Context <span class="highlight-title">Language Model</span>s by Focused Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Wu, Bingyuan Liu, Ran Yan, Lei Chen, Thomas Delteil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced their capacity to process long contexts. However, effectively
utilizing this long context remains a challenge due to the issue of
distraction, where irrelevant information dominates lengthy contexts, causing
LLMs to lose focus on the most relevant segments. To address this, we propose a
novel training method that enhances LLMs' ability to discern relevant
information through a unique combination of retrieval-based data augmentation
and contrastive learning. Specifically, during fine-tuning with long contexts,
we employ a retriever to extract the most relevant segments, serving as
augmented inputs. We then introduce an auxiliary contrastive learning objective
to explicitly ensure that outputs from the original context and the retrieved
sub-context are closely aligned. Extensive experiments on long single-document
and multi-document QA benchmarks demonstrate the effectiveness of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small
  <span class="highlight-title">Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Koska, Mojmír Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel 4.5B parameter small language model that can handle
multiple input and output modalities, including text, images, videos, and
audio. Despite its small size, the model achieves near state-of-the-art
performance on a variety of tasks, demonstrating the potential of multi-modal
models to tackle complex real-world problems. Our approach leverages recent
advancements in language modeling and multi-task learning to create a versatile
and high-performing model that can even be deployed for edge inference.
Experimental results show the model's strong performance across multiple
benchmarks, paving the way for further progress in multi-modal artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Autoregressive Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, <span class="highlight-author">Lingpeng Kong</span>, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive modeling has been a huge success in the field of natural
language processing (NLP). Recently, autoregressive models have emerged as a
significant area of focus in computer vision, where they excel in producing
high-quality visual content. Autoregressive models in NLP typically operate on
subword tokens. However, the representation strategy in computer vision can
vary in different levels, \textit{i.e.}, pixel-level, token-level, or
scale-level, reflecting the diverse and hierarchical nature of visual data
compared to the sequential structure of language. This survey comprehensively
examines the literature on autoregressive models applied to vision. To improve
readability for researchers from diverse research backgrounds, we start with
preliminary sequence representation and modeling in vision. Next, we divide the
fundamental frameworks of visual autoregressive models into three general
sub-categories, including pixel-based, token-based, and scale-based models
based on the strategy of representation. We then explore the interconnections
between autoregressive models and other generative models. Furthermore, we
present a multi-faceted categorization of autoregressive models in computer
vision, including image generation, video generation, 3D generation, and
multi-modal generation. We also elaborate on their applications in diverse
domains, including emerging domains such as embodied AI and 3D medical AI, with
about 250 related references. Finally, we highlight the current challenges to
autoregressive models in vision with suggestions about potential research
directions. We have also set up a Github repository to organize the papers
included in this survey at:
\url{https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humans Continue to Outperform <span class="highlight-title">Large Language Model</span>s in Complex Clinical
  Decision-Making: A Study with Medical Calculators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Wan, Qiao Jin, Joey Chan, Guangzhi Xiong, Serina Applebaum, Aidan Gilson, Reid McMurry, R. Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have been assessed for general medical
knowledge using medical licensing exams, their ability to effectively support
clinical decision-making tasks, such as selecting and using medical
calculators, remains uncertain. Here, we evaluate the capability of both
medical trainees and LLMs to recommend medical calculators in response to
various multiple-choice clinical scenarios such as risk stratification,
prognosis, and disease diagnosis. We assessed eight LLMs, including
open-source, proprietary, and domain-specific models, with 1,009
question-answer pairs across 35 clinical calculators and measured human
performance on a subset of 100 questions. While the highest-performing LLM,
GPT-4o, provided an answer accuracy of 74.3% (CI: 71.5-76.9%), human
annotators, on average, outperformed LLMs with an accuracy of 79.5% (CI:
73.5-85.0%). With error analysis showing that the highest-performing LLMs
continue to make mistakes in comprehension (56.6%) and calculator knowledge
(8.1%), our findings emphasize that humans continue to surpass LLMs on complex
clinical tasks such as calculator recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Small and One Large for Document-level <span class="highlight-title">Event</span> Argument Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaren Peng, Hongda Sun, Wenzhong Yang, Fuyuan Wei, Liang He, Liejun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level Event Argument Extraction (EAE) faces two challenges due to
increased input length: 1) difficulty in distinguishing semantic boundaries
between events, and 2) interference from redundant information. To address
these issues, we propose two methods. The first method introduces the Co and
Structure Event Argument Extraction model (CsEAE) based on Small Language
Models (SLMs). CsEAE includes a co-occurrences-aware module, which integrates
information about all events present in the current input through context
labeling and co-occurrences event prompts extraction. Additionally, CsEAE
includes a structure-aware module that reduces interference from redundant
information by establishing structural relationships between the sentence
containing the trigger and other sentences in the document. The second method
introduces new prompts to transform the extraction task into a generative task
suitable for Large Language Models (LLMs), addressing gaps in EAE performance
using LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned
multiple datasets to develop an LLM that performs better across most datasets.
Finally, we applied insights from CsEAE to LLMs, achieving further performance
improvements. This suggests that reliable insights validated on SLMs are also
applicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE
datasets. The CsEAE model achieved improvements of 2.1\%, 2.3\%, and 3.2\% in
the Arg-C F1 metric compared to the baseline, PAIE~\cite{PAIE}. For LLMs, we
demonstrated that their performance on document-level datasets is comparable to
that of SLMs~\footnote{All code is available at
https://github.com/simon-p-j-r/CsEAE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSSD: Simply-Scalable Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Marzollo, Jiawei Zhuang, Niklas Roemer, Lorenz K. Müller, Lukas Cavigelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past year, Speculative Decoding has gained popularity as a technique
for accelerating Large Language Model inference. While several methods have
been introduced, most struggle to deliver satisfactory performance at batch
sizes typical for data centers ($\geq 8$) and often involve significant
deployment complexities. In this work, we offer a theoretical explanation of
how Speculative Decoding can be effectively utilized with larger batch sizes.
We also introduce a method that integrates seamlessly into existing systems
without additional training or the complexity of deploying a small LLM. In a
continuous batching setting, we achieve a 4x increase in throughput without any
latency impact for short context generation, and a 1.7-2x improvement in both
latency and throughput for longer contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Decomposing Compound Ingredients in Meal Plans Using
  <span class="highlight-title">Large Language Model</span>s <span class="chip">KR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Kopitar, Leon Bedrac, Larissa J Strath, Jiang Bian, Gregor Stiglic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the effectiveness of Large Language Models in meal
planning, focusing on their ability to identify and decompose compound
ingredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral
(8x7b)-to assess their proficiency in recognizing and breaking down complex
ingredient combinations. Preliminary results indicate that while Llama-3 (70b)
and GPT-4o excels in accurate decomposition, all models encounter difficulties
with identifying essential elements like seasonings and oils. Despite strong
overall performance, variations in accuracy and completeness were observed
across models. These findings underscore LLMs' potential to enhance
personalized nutrition but highlight the need for further refinement in
ingredient decomposition. Future research should address these limitations to
improve nutritional recommendations and health outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When are 1.58 bits enough? A Bottom-up Exploration of BitNet
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary machine learning models, such as language models, are powerful,
but come with immense resource requirements both at training and inference
time. It has been shown that decoder-only language models can be trained to a
competitive state with ternary weights (1.58 bits per weight), facilitating
efficient inference. Here, we start our exploration with non-transformer model
architectures, investigating 1.58-bit training for multi-layer perceptrons and
graph neural networks. Then, we explore 1.58-bit training in other
transformer-based language models, namely encoder-only and encoder-decoder
models. Our results show that in all of these settings, 1.58-bit training is on
par with or sometimes even better than the standard 32/16-bit models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Generative</span> Adapter: Contextualizing <span class="highlight-title">Language Model</span>s in Parameters with A
  Single Forward Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, Hao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) are typically adapted to improve performance on
new contexts (\eg text prompts that define new tasks or domains) through
fine-tuning or prompting. However, there is an accuracy compute tradeoff --
fine-tuning incurs significant training cost and prompting increases inference
overhead. We introduce $GenerativeAdapter$, an effective and efficient
adaptation method that directly maps new contexts to low-rank LM adapters,
thereby significantly reducing inference overhead with no need for finetuning.
The adapter generator is trained via self-supervised learning, and can be used
to adapt a single frozen LM for any new task simply by mapping the associated
task or domain context to a new adapter. We apply $GenerativeAdapter$ to two
pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the
adapted models in three adaption scenarios: knowledge acquisition from
documents, learning from demonstrations, and personalization for users. In
StreamingQA, our approach is effective in injecting knowledge into the LM's
parameters, achieving a 63.5% improvement in F1 score over the model with
supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K
tokens. In the MetaICL in-context learning evaluation, our method achieves an
average accuracy of $44.9$ across 26 tasks, outperforming the base model. On
MSC, our method proves to be highly competitive in memorizing user information
from conversations with a 4x reduction in computation and memory costs compared
to prompting with full conversation history. Together, these results suggest
that $GenerativeAdapter$ should allow for general adaption to a wide range of
different contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Document Financial Question Answering using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalin Shah, Srikanth Ryali, Ramasubbu Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Chat<span class="highlight-title">GPT</span> Transforming Academics' Writing Style? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on one million arXiv papers submitted from May 2018 to January 2024, we
assess the textual density of ChatGPT's writing style in their abstracts
through a statistical analysis of word frequency changes. Our model is
calibrated and validated on a mixture of real abstracts and ChatGPT-modified
abstracts (simulated data) after a careful noise analysis. The words used for
estimation are not fixed but adaptive, including those with decreasing
frequency. We find that large language models (LLMs), represented by ChatGPT,
are having an increasing impact on arXiv abstracts, especially in the field of
computer science, where the fraction of LLM-style abstracts is estimated to be
approximately 35%, if we take the responses of GPT-3.5 to one simple prompt,
"revise the following sentences", as a baseline. We conclude with an analysis
of both positive and negative aspects of the penetration of LLMs into
academics' writing style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logits of API-Protected <span class="highlight-title">LLM</span>s Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Xiang Ren, Swabha Swayamdipta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) providers often hide the architectural details and
parameters of their proprietary models by restricting public access to a
limited API. In this work we show that, with only a conservative assumption
about the model architecture, it is possible to learn a surprisingly large
amount of non-public information about an API-protected LLM from a relatively
small number of API queries (e.g., costing under $1000 USD for OpenAI's
gpt-3.5-turbo). Our findings are centered on one key observation: most modern
LLMs suffer from a softmax bottleneck, which restricts the model outputs to a
linear subspace of the full output space. We exploit this fact to unlock
several capabilities, including (but not limited to) obtaining cheap
full-vocabulary outputs, auditing for specific types of model updates,
identifying the source LLM given a single full LLM output, and even efficiently
discovering the LLM's hidden size. Our empirical investigations show the
effectiveness of our methods, which allow us to estimate the embedding size of
OpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM
providers can guard against these attacks, as well as how these capabilities
can be viewed as a feature (rather than a bug) by allowing for greater
transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HPE-CogVLM: Advancing Vision <span class="highlight-title">Language Model</span>s with a Head Pose Grounding
  Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, Hsin-Tai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head pose estimation (HPE) requires a sophisticated understanding of 3D
spatial relationships to generate precise yaw, pitch, and roll angles. Previous
HPE models, primarily CNN-based, rely on cropped close-up human head images as
inputs and often lack robustness in real-world scenario. Vision Language Models
(VLMs) can analyze entire images while focusing on specific objects through
their attention mechanisms. In this paper, we propose a novel framework to
improve the HPE accuracy by leveraging the object detection grounding
capability of a VLM, referred to as CogVLM. We empirically find that directly
LoRA fine-tuning of this VLM for the HPE task fails to achieve desirable HPE
accuracy, while some model merging methods can improve accuracy but frequently
produce blended invalid response formats, struggling to handle both object
detection and HPE tasks simultaneously. To integrate HPE capability into CogVLM
effectively, we develop a novel LoRA layer-based model merging method. This
merging approach applies a high cosine similarity threshold and a
winner-takes-all layer selection strategy, aligning attention to the HPE task
while preserving original object detection knowledge. It successfully resolves
issues with blended invalid response formats and improves accuracy. Results
show that our HPE-CogVLM achieves a 31.5\% reduction in Mean Absolute Error
over the current state-of-the-art CNN model, 6DRepNet, in cross-dataset
evaluation. Furthermore, HPE-CogVLM outperforms both directly LoRA fine-tuned
and task arithmetic-based merged VLMs across all HPE metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Language Model</span>s can Infer Action Semantics for Symbolic Planners from
  Environment Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zhu, Ishika Singh, Robin Jia, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic planners can discover a sequence of actions from initial to goal
states given expert-defined, domain-specific logical action semantics. Large
Language Models (LLMs) can directly generate such sequences, but limitations in
reasoning and state-tracking often result in plans that are insufficient or
unexecutable. We propose Predicting Semantics of Actions with Language Models
(PSALM), which automatically learns action semantics by leveraging the
strengths of both symbolic planners and LLMs. PSALM repeatedly proposes and
executes plans, using the LLM to partially generate plans and to infer
domain-specific action semantics based on execution outcomes. PSALM maintains a
belief over possible action semantics that is iteratively updated until a goal
state is reached. Experiments on 7 environments show that when learning just
from one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to
100%, and explores the environment more efficiently than prior work to infer
ground truth domain action semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Formalized Problem Solving and Theorem Proving in Different
  Fields in Lean 4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formalizing mathematical proofs using computerized verification languages
like Lean 4 has the potential to significantly impact the field of mathematics,
it offers prominent capabilities for advancing mathematical reasoning. However,
existing efforts are largely limited to creating formalized versions of proofs
from extensive online mathematical corpora, struggling to keep pace with the
rapidly evolving nature of mathematics. To bridge the gap between traditional
and computerized proof techniques, this paper explores the use of Large
Language Models (LLMs) to generate formal proof steps and complete formalized
proofs. By converting natural language (NL) mathematical proofs into formalized
versions, this work introduces the basic structure and tactics of the Lean 4
language. The goal is to determine how AI can be leveraged to assist the
mathematical formalization process and improve its performance. Several
examples are provided that demonstrate solving problems using both traditional
and Lean 4-based approaches. Ultimately, this paper presents an explanation of
the foundations of Lean 4 and comparative analyses of the mathematical
formalization process using traditional and AI-augmented techniques. The
findings indicate that AI- powered tools have significant potential to
accelerate and enhance the formalization of mathematical proofs, paving the way
for more efficient and reliable theorem-proving for AI for Math in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>4Mat-Bench: Benchmarking <span class="highlight-title">Large Language Model</span>s for Materials Property
  Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andre Niyongabo Rubungo, Kangming Li, Jason Hattrick-Simpers, Adji Bousso Dieng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being used in materials
science. However, little attention has been given to benchmarking and
standardized evaluation for LLM-based materials property prediction, which
hinders progress. We present LLM4Mat-Bench, the largest benchmark to date for
evaluating the performance of LLMs in predicting the properties of crystalline
materials. LLM4Mat-Bench contains about 1.9M crystal structures in total,
collected from 10 publicly available materials data sources, and 45 distinct
properties. LLM4Mat-Bench features different input modalities: crystal
composition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B
tokens in total for each modality, respectively. We use LLM4Mat-Bench to
fine-tune models with different sizes, including LLM-Prop and MatBERT, and
provide zero-shot and few-shot prompts to evaluate the property prediction
capabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The
results highlight the challenges of general-purpose LLMs in materials science
and the need for task-specific predictive models and task-specific
instruction-tuned LLMs in materials property prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can
  be found at: https://github.com/vertaix/LLM4Mat-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eigen Attention: Attention in Low-Rank Space for KV Cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a groundbreaking advancement in the
domain of natural language processing due to their impressive reasoning
abilities. Recently, there has been considerable interest in increasing the
context lengths for these models to enhance their applicability to complex
tasks. However, at long context lengths and large batch sizes, the key-value
(KV) cache, which stores the attention keys and values, emerges as the new
bottleneck in memory usage during inference. To address this, we propose Eigen
Attention, which performs the attention operation in a low-rank space, thereby
reducing the KV cache memory overhead. Our proposed approach is orthogonal to
existing KV cache compression techniques and can be used synergistically with
them. Through extensive experiments over OPT, MPT, and Llama model families, we
demonstrate that Eigen Attention results in up to 40% reduction in KV cache
sizes and up to 60% reduction in attention operation latency with minimal drop
in performance. Code is available at
https://github.com/UtkarshSaxena1/EigenAttn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>KB: Building Very Large Knowledge Bases from <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Hu, Shrestha Ghosh, Tuan-Phong Nguyen, Simon Razniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-domain knowledge bases (KB), in particular the "big three" --
Wikidata, Yago and DBpedia -- are the backbone of many intelligent
applications. While these three have seen steady development, comprehensive KB
construction at large has seen few fresh attempts. In this work, we propose to
build a large general-domain KB entirely from a large language model (LLM). We
demonstrate the feasibility of large-scale KB construction from LLMs, while
highlighting specific challenges arising around entity recognition, entity and
property canonicalization, and taxonomy construction. As a prototype, we use
GPT-4o-mini to construct GPTKB, which contains 105 million triples for more
than 2.9 million entities, at a cost 100x less than previous KBC projects. Our
work is a landmark for two fields: For NLP, for the first time, it provides
\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the
Semantic Web, it shows novel ways forward for the long-standing challenge of
general-domain KB construction. GPTKB is accessible at http://gptkb.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned
  MT Evaluation Metrics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art trainable machine translation evaluation metrics like xCOMET
achieve high correlation with human judgment but rely on large encoders (up to
10.7B parameters), making them computationally expensive and inaccessible to
researchers with limited resources. To address this issue, we investigate
whether the knowledge stored in these large encoders can be compressed while
maintaining quality. We employ distillation, quantization, and pruning
techniques to create efficient xCOMET alternatives and introduce a novel data
collection pipeline for efficient black-box distillation. Our experiments show
that, using quantization, xCOMET can be compressed up to three times with no
quality degradation. Additionally, through distillation, we create an
278M-sized xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters,
but retains 92.1% of its quality. Besides, it surpasses strong small-scale
metrics like COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by
6.4%, despite using 50% fewer parameters. All code, dataset, and models are
available online at https://github.com/NL2G/xCOMET-lite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main Conference) Camera-Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Vision-Language Few-Shot Adaptation with Negative Learning <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision-Language Models (VLMs) have exhibited
impressive zero-shot performance and transferability, allowing them to adapt to
downstream tasks in a data-efficient manner. However, when only a few labeled
samples are available, adapting VLMs to distinguish subtle differences between
similar classes in specific downstream tasks remains challenging. In this work,
we propose a Simple yet effective Negative Learning approach, SimNL, to more
efficiently exploit the task-specific knowledge from few-shot labeled samples.
Unlike previous methods that focus on identifying a set of representative
positive features defining "what is a {CLASS}", SimNL discovers a complementary
set of negative features that define "what is not a {CLASS}", providing
additional insights that supplement the positive features to enhance
task-specific recognition capability. Further, we identify that current
adaptation approaches are particularly vulnerable to potential noise in the
few-shot sample set. To mitigate this issue, we introduce a plug-and-play
few-shot instance reweighting technique to suppress noisy outliers and amplify
clean samples for more stable adaptation. Our extensive experimental results
across 15 datasets validate that the proposed SimNL outperforms existing
state-of-the-art methods on both few-shot learning and domain generalization
tasks while achieving competitive computational efficiency. Code is available
at https://github.com/zhangce01/SimNL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025. Code is available at
  https://github.com/zhangce01/SimNL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BhasaAnuvaad: A Speech Translation <span class="highlight-title">Dataset</span> for 13 Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sparsh Jain, Ashwin Sankar, Devilal Choudhary, Dhairya Suman, Nikhil Narasimhan, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M Khapra, Raj Dabre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Translation (AST) datasets for Indian languages remain
critically scarce, with public resources covering fewer than 10 of the 22
official languages. This scarcity has resulted in AST systems for Indian
languages lagging far behind those available for high-resource languages like
English. In this paper, we first evaluate the performance of widely-used AST
systems on Indian languages, identifying notable performance gaps and
challenges. Our findings show that while these systems perform adequately on
read speech, they struggle significantly with spontaneous speech, including
disfluencies like pauses and hesitations. Additionally, there is a striking
absence of systems capable of accurately translating colloquial and informal
language, a key aspect of everyday communication. To this end, we introduce
BhasaAnuvaad, the largest publicly available dataset for AST involving 13 out
of 22 scheduled Indian languages and English spanning over 44,400 hours and 17M
text segments. BhasaAnuvaad contains data for English speech to Indic text, as
well as Indic speech to English text. This dataset comprises three key
categories: (1) Curated datasets from existing resources, (2) Large-scale web
mining, and (3) Synthetic data generation. By offering this diverse and
expansive dataset, we aim to bridge the resource gap and promote advancements
in AST for Indian languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Diversity in Automatic Poetry Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanran Chen, Hannes Gröner, Sina Zarrieß, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation (NLG), and more generally generative AI, are
among the currently most impactful research fields. Creative NLG, such as
automatic poetry generation, is a fascinating niche in this area. While most
previous research has focused on forms of the Turing test when evaluating
automatic poetry generation -- can humans distinguish between automatic and
human generated poetry -- we evaluate the diversity of automatically generated
poetry (with a focus on quatrains), by comparing distributions of generated
poetry to distributions of human poetry along structural, lexical, semantic and
stylistic dimensions, assessing different model types (word vs.
character-level, general purpose LLMs vs. poetry-specific models), including
the very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.
unconditioned). We find that current automatic poetry systems are considerably
underdiverse along multiple dimensions -- they often do not rhyme sufficiently,
are semantically too uniform and even do not match the length distribution of
human poetry. Our experiments reveal, however, that style-conditioning and
character-level modeling clearly increases diversity across virtually all
dimensions we explore. Our identified limitations may serve as the basis for
more genuinely diverse future poetry generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main; camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Your Step (by Step): Chain-of-<span class="highlight-title">Thought</span> can Reduce Performance on
  Tasks where Thinking Makes Humans Worse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting has become a widely used strategy for
working with large language and multimodal models. While CoT has been shown to
improve performance across many tasks, determining the settings in which it is
effective remains an ongoing effort. In particular, it is still an open
question in what settings CoT systematically reduces model performance. In this
paper, we seek to identify the characteristics of tasks where CoT reduces
performance by drawing inspiration from cognitive psychology, looking at cases
where (i) verbal thinking or deliberation hurts performance in humans, and (ii)
the constraints governing human performance generalize to language models.
Three such cases are implicit statistical learning, visual recognition, and
classifying with patterns containing exceptions. In extensive experiments
across all three settings, we find that a diverse collection of
state-of-the-art models exhibit significant drop-offs in performance (e.g., up
to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using
inference-time reasoning compared to zero-shot counterparts. We also identify
three tasks that satisfy condition (i) but not (ii), and find that while verbal
thinking reduces human performance in these tasks, CoT retains or increases
model performance. Overall, our results show that while there is not an exact
parallel between the cognitive processes of models and those of humans,
considering cases where thinking has negative consequences for human
performance can help us identify settings where it negatively impacts models.
By connecting the literature on human deliberation with evaluations of CoT, we
offer a new tool that can be used in understanding the impact of prompt choices
and inference-time reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Regulation Neurons in <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread use, the mechanisms by which large language models
(LLMs) represent and regulate uncertainty in next-token predictions remain
largely unexplored. This study investigates two critical components believed to
influence this uncertainty: the recently discovered entropy neurons and a new
set of components that we term token frequency neurons. Entropy neurons are
characterized by an unusually high weight norm and influence the final layer
normalization (LayerNorm) scale to effectively scale down the logits. Our work
shows that entropy neurons operate by writing onto an unembedding null space,
allowing them to impact the residual stream norm with minimal direct effect on
the logits themselves. We observe the presence of entropy neurons across a
range of models, up to 7 billion parameters. On the other hand, token frequency
neurons, which we discover and describe here for the first time, boost or
suppress each token's logit proportionally to its log frequency, thereby
shifting the output distribution towards or away from the unigram distribution.
Finally, we present a detailed case study where entropy neurons actively manage
confidence in the setting of induction, i.e. detecting and continuing repeated
subsequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Connect Speech Foundation Models and <span class="highlight-title">Large Language Model</span>s? What
  Matters and What Does Not 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaffold-BPE: Enhancing Byte Pair Encoding for <span class="highlight-title">Large Language Model</span>s
  with Simple and Effective Scaffold Token Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lian, Yizhe Xiong, Jianwei Niu, Shasha Mo, Zhenpeng Su, Zijia Lin, Hui Chen, Peng Liu, Jungong Han, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte Pair Encoding (BPE) serves as a foundation method for text tokenization
in the Natural Language Processing (NLP) field. Despite its wide adoption, the
original BPE algorithm harbors an inherent flaw: it inadvertently introduces a
frequency imbalance for tokens in the text corpus. Since BPE iteratively merges
the most frequent token pair in the text corpus to generate a new token and
keeps all generated tokens in the vocabulary, it unavoidably holds tokens that
primarily act as components of a longer token and appear infrequently on their
own. We term such tokens as Scaffold Tokens. Due to their infrequent
occurrences in the text corpus, Scaffold Tokens pose a learning imbalance
issue. To address that issue, we propose Scaffold-BPE, which incorporates a
dynamic scaffold token removal mechanism by parameter-free, computation-light,
and easy-to-implement modifications to the original BPE method. This novel
approach ensures the exclusion of low-frequency Scaffold Tokens from the token
representations for given texts, thereby mitigating the issue of frequency
imbalance and facilitating model training. On extensive experiments across
language modeling and even machine translation, Scaffold-BPE consistently
outperforms the original BPE, well demonstrating its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparison of correspondence analysis with PMI-based word embedding
  methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Qi, Ayoub Bagheri, David J. Hessen, Peter G. M. van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular word embedding methods such as GloVe and Word2Vec are related to the
factorization of the pointwise mutual information (PMI) matrix. In this paper,
we link correspondence analysis (CA) to the factorization of the PMI matrix. CA
is a dimensionality reduction method that uses singular value decomposition
(SVD), and we show that CA is mathematically close to the weighted
factorization of the PMI matrix. In addition, we present variants of CA that
turn out to be successful in the factorization of the word-context matrix, i.e.
CA applied to a matrix where the entries undergo a square-root transformation
(ROOT-CA) and a root-root transformation (ROOTROOT-CA). While this study
focuses on traditional static word embedding methods, to extend the
contribution of this paper, we also include a comparison of transformer-based
encoder BERT, i.e. contextual word embedding, with these traditional methods.
An empirical comparison among CA- and PMI-based methods as well as BERT shows
that overall results of ROOT-CA and ROOTROOT-CA are slightly better than those
of the PMI-based methods and are competitive with BERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of
  Metaphorical Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Wang, Ge Zhang, Han Wu, Tyler Loakman, Wenhao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) has developed rapidly since the release of Large
Language Models and current MT evaluation is performed through comparison with
reference human translations or by predicting quality scores from human-labeled
data. However, these mainstream evaluation methods mainly focus on fluency and
factual reliability, whilst paying little attention to figurative quality. In
this paper, we investigate the figurative quality of MT and propose a set of
human evaluation metrics focused on the translation of figurative language. We
additionally present a multilingual parallel metaphor corpus generated by
post-editing. Our evaluation protocol is designed to estimate four aspects of
MT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so,
we observe that translations of figurative expressions display different traits
from literal ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked
  Preferences <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique. We release the preference pairs used in
alignment at:
https://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2024 as long (findings) conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating the Influence of Sequentially Correlated Literary Properties
  in Textual Classification: A Data-Centric Hypothesis-Testing Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stylometry aims to distinguish authors by analyzing literary traits assumed
to reflect semi-conscious choices distinct from elements like genre or theme.
However, these components often overlap, complicating text classification based
solely on feature distributions. While some literary properties, such as
thematic content, are likely to manifest as correlations between adjacent text
units, others, like authorial style, may be independent thereof. We introduce a
hypothesis-testing approach to evaluate the influence of sequentially
correlated literary properties on text classification, aiming to determine when
these correlations drive classification. Using a multivariate binary
distribution, our method models sequential correlations between text units as a
stochastic process, assessing the likelihood of clustering across varying
adjacency scales. This enables us to examine whether classification is
dominated by sequentially correlated properties or remains independent. In
experiments on a diverse English prose corpus, our analysis integrates
traditional and neural embeddings within supervised and unsupervised
frameworks. Results demonstrate that our approach effectively identifies when
textual classification is not primarily influenced by sequentially correlated
literary properties, particularly in cases where texts differ in authorial
style or genre rather than by a single author within a similar genre.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust and Efficient Fine-tuning of <span class="highlight-title">LLM</span>s with Bayesian
  Reparameterization of Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Sengupta, Vaibhav Seth, Arinjay Pathak, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 10 figures, 10 tables, Code:
  https://github.com/LCS2-IIITD/MonteCLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Prompt</span> Optimization for Defending <span class="highlight-title">Language Model</span>s Against
  Jailbreaking Attacks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17263v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17263v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zhou, Bo Li, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in AI alignment, large language models (LLMs) remain
vulnerable to adversarial attacks or jailbreaking, in which adversaries can
modify prompts to induce unwanted behavior. While some defenses have been
proposed, they have not been adapted to newly proposed attacks and more
challenging threat models. To address this, we propose an optimization-based
objective for defending LLMs against jailbreaking attacks and an algorithm,
Robust Prompt Optimization (RPO) to create robust system-level defenses. Our
approach directly incorporates the adversary into the defensive objective and
optimizes a lightweight and transferable suffix, enabling RPO to adapt to
worst-case adaptive attacks. Our theoretical and experimental results show
improved robustness to both jailbreaks seen during optimization and unknown
jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2
to 0% on JailbreakBench, setting the state-of-the-art. Code can be found at
https://github.com/lapisrocks/rpo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight; code available at
  https://github.com/lapisrocks/rpo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Generation for Controllable <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjae Lee, Kyungmin Kim, Taesoo Kim, Sangdon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthiness of generative language models (GLMs) is crucial in their
deployment to critical decision making systems. Hence, certified risk control
methods such as selective prediction and conformal prediction have been applied
to mitigating the hallucination problem in various supervised downstream tasks.
However, the lack of appropriate correctness metric hinders applying such
principled methods to language generation tasks. In this paper, we circumvent
this problem by leveraging the concept of textual entailment to evaluate the
correctness of the generated sequence, and propose two selective generation
algorithms which control the false discovery rate with respect to the textual
entailment relation (FDR-E) with a theoretical guarantee:
$\texttt{SGen}^{\texttt{Sup}}$ and $\texttt{SGen}^{\texttt{Semi}}$.
$\texttt{SGen}^{\texttt{Sup}}$, a direct modification of the selective
prediction, is a supervised learning algorithm which exploits
entailment-labeled data, annotated by humans. Since human annotation is costly,
we further propose a semi-supervised version, $\texttt{SGen}^{\texttt{Semi}}$,
which fully utilizes the unlabeled data by pseudo-labeling, leveraging an
entailment set function learned via conformal prediction. Furthermore,
$\texttt{SGen}^{\texttt{Semi}}$ enables to use more general class of selection
functions, neuro-selection functions, and provides users with an optimal
selection function class given multiple candidates. Finally, we demonstrate the
efficacy of the $\texttt{SGen}$ family in achieving a desired FDR-E level with
comparable selection efficiency to those from baselines on both open and closed
source GLMs. Code and datasets are provided at
https://github.com/ml-postech/selective-generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Greedy Coordinate Gradient and General <span class="highlight-title">Prompt</span> Optimization
  via Probe Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi, Anirudh Goyal, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety of Large Language Models (LLMs) has become a critical issue given
their rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be
effective in constructing adversarial prompts to break the aligned LLMs, but
optimization of GCG is time-consuming. To reduce the time cost of GCG and
enable more comprehensive studies of LLM safety, in this work, we study a new
algorithm called $\texttt{Probe sampling}$. At the core of the algorithm is a
mechanism that dynamically determines how similar a smaller draft model's
predictions are to the target model's predictions for prompt candidates. When
the target model is similar to the draft model, we rely heavily on the draft
model to filter out a large number of potential prompt candidates. Probe
sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to
equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe
sampling is also able to accelerate other prompt optimization techniques and
adversarial methods, leading to acceleration of $1.8\times$ for AutoPrompt,
$2.4\times$ for APE and $2.4\times$ for AutoDAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Privileged Students: On the Value of Initialization in Multilingual
  Knowledge <span class="highlight-title">Distill</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haryo Akbarianto Wibowo, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has proven to be a successful strategy to improve
the performance of smaller models in many NLP tasks. However, most of the work
in KD only explores monolingual scenarios. In this paper, we investigate the
value of KD in multilingual settings. We find the significance of KD and model
initialization by analyzing how well the student model acquires multilingual
knowledge from the teacher model. Our proposed method emphasizes copying the
teacher model's weights directly to the student model to enhance
initialization. Our findings show that model initialization using copy-weight
from the fine-tuned teacher contributes the most compared to the distillation
process itself across various multilingual settings. Furthermore, we
demonstrate that efficient weight initialization preserves multilingual
capabilities even in low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the <span class="highlight-title">LLM</span> Journey from Cognition to Expression with Linear
  Representations <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzi Yan, Jialian Li, Yipin Zhang, Dong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an in-depth examination of the evolution and interplay of
cognitive and expressive capabilities in large language models (LLMs), with a
specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese
and English) LLM series. We define and explore the model's cognitive and
expressive capabilities through linear representations across three critical
phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning
from Human Feedback (RLHF). Cognitive capability is defined as the quantity and
quality of information conveyed by the neuron output vectors within the
network, similar to the neural signal processing in human cognition. Expressive
capability is defined as the model's capability to produce word-level output.
Our findings unveil a sequential development pattern, where cognitive abilities
are largely established during Pretraining, whereas expressive abilities
predominantly advance during SFT and RLHF. Statistical analyses confirm a
significant correlation between the two capabilities, suggesting that cognitive
capacity may limit expressive potential. The paper also explores the
theoretical underpinnings of these divergent developmental trajectories and
their connection to the LLMs' architectural design. Moreover, we evaluate
various optimization-independent strategies, such as few-shot learning and
repeated sampling, which bridge the gap between cognitive and expressive
capabilities. This research reveals the potential connection between the hidden
space and the output space, contributing valuable insights into the
interpretability and controllability of their training processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hal-Eval: A Universal and Fine-grained Hallucination Evaluation
  Framework for Large Vision <span class="highlight-title">Language Model</span>s <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoya Jiang, Hongrui Jia, Wei Ye, Mengfan Dong, Haiyang Xu, Ming Yan, Ji Zhang, Shikun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models exhibit remarkable capabilities but struggle
with hallucinations inconsistencies between images and their descriptions.
Previous hallucination evaluation studies on LVLMs have identified
hallucinations in terms of objects, attributes, and relations but overlooked
complex hallucinations that create an entire narrative around a fictional
entity. In this paper, we introduce a refined taxonomy of hallucinations,
featuring a new category: Event Hallucination. We then utilize advanced LLMs to
generate and filter fine grained hallucinatory data consisting of various types
of hallucinations, with a particular focus on event hallucinations, laying the
groundwork for integrating discriminative and generative evaluation methods
within our universal evaluation framework. The proposed benchmark distinctively
assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it
a reliable and comprehensive tool for gauging LVLMs efficacy in handling
hallucinations. We will release our code and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRS-QA -- Graph Reasoning-Structured Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Pahilajani, Devasha Trivedi, Jincen Shuai, Khin S. Yone, Samyak Rajesh Jain, Namyong Park, Ryan A. Rossi, Nesreen K. Ahmed, Franck Dernoncourt, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have excelled in multi-hop question-answering
(M-QA) due to their advanced reasoning abilities. However, the impact of the
inherent reasoning structures on LLM M-QA performance remains unclear, largely
due to the absence of QA datasets that provide fine-grained reasoning
structures. To address this gap, we introduce the Graph Reasoning-Structured
Question Answering Dataset (GRS-QA), which includes both semantic contexts and
reasoning structures for QA pairs. Unlike existing M-QA datasets, where
different reasoning structures are entangled together, GRS-QA explicitly
captures intricate reasoning pathways by constructing reasoning graphs, where
nodes represent textual contexts and edges denote logical flows. These
reasoning graphs of different structures enable a fine-grained evaluation of
LLM reasoning capabilities across various reasoning structures. Our empirical
analysis reveals that LLMs perform differently when handling questions with
varying reasoning structures. This finding facilitates the exploration of
textual structures as compared with semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 24 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Tuning For Toxicity Mitigation Generalizes Across Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detoxifying multilingual Large Language Models (LLMs) has become crucial due
to their increasing global use. In this work, we explore zero-shot
cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike
previous studies that show limited cross-lingual generalization for other
safety tasks, we demonstrate that Direct Preference Optimization (DPO) training
with only English data can significantly reduce toxicity in multilingual
open-ended generations. For example, the probability of mGPT-1.3B generating
toxic continuations drops from 46.8% to 3.9% across 17 different languages
after training. Our results also extend to other multilingual LLMs, such as
BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal
intervention and activation analysis, we identified the dual multilinguality
property of MLP layers in LLMs, which explains the cross-lingual generalization
of DPO. Finally, we show that bilingual sentence retrieval can predict the
cross-lingual transferability of DPO preference tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toxicity Detection for Free 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanhao Hu, Julien Piet, Geng Zhao, Jiantao Jiao, David Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LLMs are generally aligned to follow safety requirements and tend to
refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be
overcautious and refuse benign examples. In addition, state-of-the-art toxicity
detectors have low TPRs at low FPR, incurring high costs in real-world
applications where toxic examples are rare. In this paper, we introduce
Moderation Using LLM Introspection (MULI), which detects toxic prompts using
the information extracted directly from LLMs themselves. We found we can
distinguish between benign and toxic prompts from the distribution of the first
response token's logits. Using this idea, we build a robust detector of toxic
prompts using a sparse logistic regression model on the first response token
logits. Our scheme outperforms SOTA detectors under multiple metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limpeh ga li gong: Challenges in Singlish Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Qi Chan, Lynnette Hui Xian Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or Colloquial Singapore English, is a language formed from oral and
social communication within multicultural Singapore. In this work, we work on a
fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)
tagging of Singlish sentences. For our analysis, we build a parallel Singlish
dataset containing direct English translations and POS tags, with translation
and POS annotation done by native Singlish speakers. Our experiments show that
automatic transition- and transformer- based taggers perform with only $\sim
80\%$ accuracy when evaluated against human-annotated POS labels, suggesting
that there is indeed room for improvement on computation analysis of the
language. We provide an exposition of challenges in Singlish annotation: its
inconsistencies in form and semantics, the highly context-dependent particles
of the language, its structural unique expressions, and the variation of the
language on different mediums. Our task definition, resultant labels and
results reflects the challenges in analysing colloquial languages formulated
from a variety of dialects, and paves the way for future studies beyond POS
tagging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An
  Evaluation Using TORGO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Macarious Hui, Jinda Zhang, Aanchan Mohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS)
frequently face challenges with articulation, leading to dysarthria and
resulting in atypical speech patterns. In healthcare settings, communication
breakdowns reduce the quality of care. While building an augmentative and
alternative communication (AAC) tool to enable fluid communication we found
that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like
Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack
of training data. Our work looks to leverage SOTA ASR followed by domain
specific error-correction. English dysarthric ASR performance is often
evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this
dataset where phrases overlap between training and test speakers. Our work
proposes an algorithm to break this prompt-overlap. After reducing
prompt-overlap, results with SOTA ASR models produce extremely high word error
rates for speakers with mild and severe dysarthria. Furthermore, to improve
ASR, our work looks at the impact of n-gram language models and large-language
model (LLM) based multi-modal generative error-correction algorithms like
Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs
to be done to improve ASR for atypical speakers to enable equitable healthcare
access both in-person and in e-health settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Framework with <span class="highlight-title">Large Language Model</span>s for Rare Disease
  Phenotyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinge Wu, Hang Dong, Zexi Li, Haowei Wang, Runci Li, Arijit Patra, Chengliang Dai, Waqar Ali, Phil Scordis, Honghan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare diseases pose significant challenges in diagnosis and treatment due to
their low prevalence and heterogeneous clinical presentations. Unstructured
clinical notes contain valuable information for identifying rare diseases, but
manual curation is time-consuming and prone to subjectivity. This study aims to
develop a hybrid approach combining dictionary-based natural language
processing (NLP) tools with large language models (LLMs) to improve rare
disease identification from unstructured clinical reports. We propose a novel
hybrid framework that integrates the Orphanet Rare Disease Ontology (ORDO) and
the Unified Medical Language System (UMLS) to create a comprehensive rare
disease vocabulary. The proposed hybrid approach demonstrates superior
performance compared to traditional NLP systems and standalone LLMs. Notably,
the approach uncovers a significant number of potential rare disease cases not
documented in structured diagnostic records, highlighting its ability to
identify previously unrecognized patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transducer Consistency Regularization for Speech to Text Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cindy Tseng, Yun Tang, Vijendra Raj Apsingekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency regularization is a commonly used practice to encourage the model
to generate consistent representation from distorted input features and improve
model generalization. It shows significant improvement on various speech
applications that are optimized with cross entropy criterion. However, it is
not straightforward to apply consistency regularization for the
transducer-based approaches, which are widely adopted for speech applications
due to the competitive performance and streaming characteristic. The main
challenge is from the vast alignment space of the transducer optimization
criterion and not all the alignments within the space contribute to the model
optimization equally. In this study, we present Transducer Consistency
Regularization (TCR), a consistency regularization method for transducer
models. We apply distortions such as spec augmentation and dropout to create
different data views and minimize the distribution difference. We utilize
occupational probabilities to give different weights on transducer output
distributions, thus only alignments close to oracle alignments would contribute
to the model learning. Our experiments show the proposed method is superior to
other consistency regularization implementations and could effectively reduce
word error rate (WER) by 4.3\% relatively comparing with a strong baseline on
the \textsc{Librispeech} dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures. Accepted in IEEE Spoken Language Technology
  Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Hallucination in Fictional Character Role-Play <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing has wide-ranging applications in customer support, embodied
agents, and computational social science. The influence of parametric world
knowledge of large language models (LLMs) often causes role-playing characters
to act out of character and to hallucinate about things outside the scope of
their knowledge. In this work, we focus on the evaluation and mitigation of
hallucination in fictional character role-play. We introduce a dataset with
over 2,000 characters and 72,000 interviews, including 18,000 adversarial
questions. We propose RoleFact, a role-playing method that mitigates
hallucination by modulating the influence of parametric knowledge using a
pre-calibrated confidence threshold. Experiments show that the proposed method
improves the factual precision of generated responses by 18% for adversarial
questions with a 44% reduction in temporal hallucination for time-sensitive
interviews. The code and the dataset are available at
https://github.com/NafisSadeq/rolefact.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards More Realistic Extraction Attacks: An Adversarial Perspective <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash More, Prakhar Ganesh, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are prone to memorizing parts of their training data which
makes them vulnerable to extraction attacks. Existing research often examines
isolated setups--such as evaluating extraction risks from a single model or
with a fixed prompt design. However, a real-world adversary could access models
across various sizes and checkpoints, as well as exploit prompt sensitivity,
resulting in a considerably larger attack surface than previously studied. In
this paper, we revisit extraction attacks from an adversarial perspective,
focusing on how to leverage the brittleness of language models and the
multi-faceted access to the underlying data. We find significant churn in
extraction trends, i.e., even unintuitive changes to the prompt, or targeting
smaller models and earlier checkpoints, can extract distinct information. By
combining information from multiple attacks, our adversary is able to increase
the extraction risks by up to $2 \times$. Furthermore, even with mitigation
strategies like data deduplication, we find the same escalation of extraction
risks against a real-world adversary. We conclude with a set of case studies,
including detecting pre-training data, copyright violations, and extracting
personally identifiable information, showing how our more realistic adversary
can outperform existing adversaries in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at PrivateNLP@ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adversarial Perspective on Machine Unlearning for AI Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are finetuned to refuse questions about hazardous
knowledge, but these protections can often be bypassed. Unlearning methods aim
at completely removing hazardous capabilities from models and make them
inaccessible to adversaries. This work challenges the fundamental differences
between unlearning and traditional safety post-training from an adversarial
perspective. We demonstrate that existing jailbreak methods, previously
reported as ineffective against unlearning, can be successful when applied
carefully. Furthermore, we develop a variety of adaptive methods that recover
most supposedly unlearned capabilities. For instance, we show that finetuning
on 10 unrelated examples or removing specific directions in the activation
space can recover most hazardous capabilities for models edited with RMU, a
state-of-the-art unlearning method. Our findings challenge the robustness of
current unlearning approaches and question their advantages over safety
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight paper at Neurips 2024 SoLaR workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Usefulness of <span class="highlight-title">LLM</span>s as an Author Checklist Assistant for Scientific
  Papers: NeurIPS'24 Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a promising, but controversial, tool
in aiding scientific peer review. This study evaluates the usefulness of LLMs
in a conference setting as a tool for vetting paper submissions against
submission standards. We conduct an experiment at the 2024 Neural Information
Processing Systems (NeurIPS) conference, where 234 papers were voluntarily
submitted to an "LLM-based Checklist Assistant." This assistant validates
whether papers adhere to the author checklist used by NeurIPS, which includes
questions to ensure compliance with research and manuscript preparation
standards. Evaluation of the assistant by NeurIPS paper authors suggests that
the LLM-based assistant was generally helpful in verifying checklist
completion. In post-usage surveys, over 70% of authors found the assistant
useful, and 70% indicate that they would revise their papers or checklist
responses based on its feedback. While causal attribution to the assistant is
not definitive, qualitative evidence suggests that the LLM contributed to
improving some submissions. Survey responses and analysis of re-submissions
indicate that authors made substantive revisions to their submissions in
response to specific feedback from the LLM. The experiment also highlights
common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)
were the most frequent issues flagged by authors. We also conduct experiments
to understand potential gaming of the system, which reveal that the assistant
could be manipulated to enhance scores through fabricated justifications,
highlighting potential vulnerabilities of automated review tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AstroMLab 1: Who Wins Astronomy Jeopardy!? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Sen Ting, Tuan Dung Nguyen, Tirthankar Ghosal, Rui Pan, Hardik Arora, Zechang Sun, Tijmen de Haan, Nesar Ramachandra, Azton Wells, Sandeep Madireddy, Alberto Accomazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of proprietary and open-weights large
language models using the first astronomy-specific benchmarking dataset. This
dataset comprises 4,425 multiple-choice questions curated from the Annual
Review of Astronomy and Astrophysics, covering a broad range of astrophysical
topics. Our analysis examines model performance across various astronomical
subfields and assesses response calibration, crucial for potential deployment
in research environments. Claude-3.5-Sonnet outperforms competitors by up to
4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we
observed a universal reduction in cost every 3-to-12 months to achieve similar
score in this particular astronomy benchmark. open-weights models have rapidly
improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with
some of the best proprietary models. We identify performance variations across
topics, with non-English-focused models generally struggling more in
exoplanet-related fields, stellar astrophysics, and instrumentation related
questions. These challenges likely stem from less abundant training data,
limited historical context, and rapid recent developments in these areas. This
pattern is observed across both open-weights and proprietary models, with
regional dependencies evident, highlighting the impact of training data
diversity on model performance in specialized scientific domains.
Top-performing models demonstrate well-calibrated confidence, with correlations
above 0.9 between confidence and correctness, though they tend to be slightly
underconfident. The development for fast, low-cost inference of open-weights
models presents new opportunities for affordable deployment in astronomy. The
rapid progress observed suggests that LLM-driven research in astronomy may
become feasible in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures, 7 tables. Published in Astronomy & Computing.
  AstroMLab homepage: https://astromlab.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESM+: Modern Insights into Perspective on Text-to-SQL Evaluation in the
  Age of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin G. Ascoli, Yasoda Sai Ram Kandikonda, Jinho D. Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Text-to-SQL enables anyone to retrieve information from SQL
databases using natural language. Despite several challenges, recent models
have made remarkable advancements in this task using large language models
(LLMs). Interestingly, we find that LLM-based models without fine-tuning
exhibit distinct natures compared to their fine-tuned counterparts, leading to
inadequacies in current evaluation metrics to accurately convey their
performance. Thus, we analyze the two primary metrics, Test Suite Execution
Accuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their
robustness for this task and address shortcomings. We compare the performance
of 9 LLM-based models using EXE, the original ESM, and our improved ESM (called
ESM+). Our results show that EXE and ESM have high false positive and negative
rates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively,
providing a significantly more stable evaluation. We release the ESM+ script as
open-source for the community to contribute, while enjoying a more reliable
assessment of Text-to-SQL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing High-Level Song Descriptors towards Natural Language-Based
  Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena V. Epure, Gabriel Meseguer Brocal, Darius Afchar, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems relying on Language Models (LMs) have gained popularity
in assisting users to navigate large catalogs. LMs often exploit item
high-level descriptors, i.e. categories or consumption contexts, from training
data or user preferences. This has been proven effective in domains like movies
or products. However, in the music domain, understanding how effectively LMs
utilize song descriptors for natural language-based music recommendation is
relatively limited. In this paper, we assess LMs effectiveness in recommending
songs based on user natural language descriptions and items with descriptors
like genres, moods, and listening contexts. We formulate the recommendation
task as a dense retrieval problem and assess LMs as they become increasingly
familiar with data pertinent to the task and domain. Our findings reveal
improved performance as LMs are fine-tuned for general language similarity,
information retrieval, and mapping longer descriptions to shorter, high-level
descriptors in music.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why These Documents? Explainable <span class="highlight-title">Generative</span> Retrieval with Hierarchical
  Category Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangam Lee, Ryang Heo, SeongKu Kang, Susik Yoon, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval has recently emerged as a new alternative of traditional
information retrieval approaches. However, existing generative retrieval
methods directly decode docid when a query is given, making it impossible to
provide users with explanations as an answer for "Why this document is
retrieved?". To address this limitation, we propose Hierarchical Category
Path-Enhanced Generative Retrieval(HyPE), which enhances explainability by
generating hierarchical category paths step-by-step before decoding docid. HyPE
leverages hierarchical category paths as explanation, progressing from broad to
specific semantic categories. This approach enables diverse explanations for
the same document depending on the query by using shared category paths between
the query and the document, and provides reasonable explanation by reflecting
the document's semantic structure through a coarse-to-fine manner. HyPE
constructs category paths with external high-quality semantic hierarchy,
leverages LLM to select appropriate candidate paths for each document, and
optimizes the generative retrieval model with path-augmented dataset. During
inference, HyPE utilizes path-aware reranking strategy to aggregate diverse
topic information, allowing the most relevant documents to be prioritized in
the final ranked list of docids. Our extensive experiments demonstrate that
HyPE not only offers a high level of explainability but also improves the
retrieval performance in the document retrieval task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IntellBot: Retrieval Augmented <span class="highlight-title">LLM</span> Chatbot for Cyber Threat Knowledge
  Delivery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dincy R. Arikkat, Abhinav M., Navya Binu, Parvathi M., Navya Biju, K. S. Arunima, Vinod P., Rafidha Rehiman K. A., Mauro Conti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of cyber security, intelligent chatbots are
gaining prominence. Artificial Intelligence, Machine Learning, and Natural
Language Processing empower these chatbots to handle user inquiries and deliver
threat intelligence. This helps cyber security knowledge readily available to
both professionals and the public. Traditional rule-based chatbots often lack
flexibility and struggle to adapt to user interactions. In contrast, Large
Language Model-based chatbots offer contextually relevant information across
multiple domains and adapt to evolving conversational contexts. In this work,
we develop IntellBot, an advanced cyber security Chatbot built on top of
cutting-edge technologies like Large Language Models and Langchain alongside a
Retrieval-Augmented Generation model to deliver superior capabilities. This
chatbot gathers information from diverse data sources to create a comprehensive
knowledge base covering known vulnerabilities, recent cyber attacks, and
emerging threats. It delivers tailored responses, serving as a primary hub for
cyber security insights. By providing instant access to relevant information
and resources, this IntellBot enhances threat intelligence, incident response,
and overall security posture, saving time and empowering users with knowledge
of cyber security best practices. Moreover, we analyzed the performance of our
copilot using a two-stage evaluation strategy. We achieved BERT score above 0.8
by indirect approach and a cosine similarity score ranging from 0.8 to 1, which
affirms the accuracy of our copilot. Additionally, we utilized RAGAS to
evaluate the RAG model, and all evaluation metrics consistently produced scores
above 0.77, highlighting the efficacy of our system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multi-Domain Task-Oriented Dialogue System with Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dharmendra Prajapat, Durga Toshniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) system is designed to accomplish user-defined
tasks through dialogues. The TOD system has progressed towards end-to-end
modeling by leveraging pre-trained large language models. Fine-tuning the
pre-trained language models using only supervised learning leads to the
exposure bias and token loss problem and it deviates the models from completing
the user's task. To address these issues, we propose a TOD system that
leverages a unified pre-trained language model, GPT2, as a base model. It is
optimized using supervised learning and reinforcement learning (RL). The issues
in the TOD system are mitigated using a non-differentiable reward function. The
reward is calculated using the weighted sum of the success rate and BLEU
evaluation metrics. The success rate and BLEU metrics in reward calculation
guide the language model for user task completion while ensuring a coherent and
fluent response. Our model is acquired by fine-tuning a pre-trained model on
the dialogue-session level which comprises user utterance, belief state, system
act, and system response. Experimental results on MultiWOZ2.1 demonstrate that
our model increases the inform rate by 1.60% and the success rate by 3.17%
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effect of different feature selection methods on models created with
  XGBoost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Neyra, Vishal B. Siramshetty, Huthaifa I. Ashqar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the effect that different feature selection methods have
on models created with XGBoost, a popular machine learning algorithm with
superb regularization methods. It shows that three different ways for reducing
the dimensionality of features produces no statistically significant change in
the prediction accuracy of the model. This suggests that the traditional idea
of removing the noisy training data to make sure models do not overfit may not
apply to XGBoost. But it may still be viable in order to reduce computational
complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucination with ZeroG: An Advanced Knowledge Management
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anantha Sharma, Sheeba Elizabeth John, Fatemeh Rezapoor Nikroo, Krupali Bhatt, Mrunal Zambre, Aditi Wikhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of digital documents presents significant challenges in efficient
management and knowledge extraction. Traditional methods often struggle with
complex documents, leading to issues such as hallucinations and high latency in
responses from Large Language Models (LLMs). ZeroG, an innovative approach,
significantly mitigates these challenges by leveraging knowledge distillation
and prompt tuning to enhance model performance.
  ZeroG utilizes a smaller model that replicates the behavior of a larger
teacher model, ensuring contextually relevant and grounded responses, by
employing a black-box distillation approach, it creates a distilled dataset
without relying on intermediate features, optimizing computational efficiency.
This method significantly enhances accuracy and reduces response times,
providing a balanced solution for modern document management.
  Incorporating advanced techniques for document ingestion and metadata
utilization, ZeroG improves the accuracy of question-and-answer systems. The
integration of graph databases and robust metadata management further
streamlines information retrieval, allowing for precise and context-aware
responses. By transforming how organizations interact with complex data, ZeroG
enhances productivity and user experience, offering a scalable solution for the
growing demands of digital document management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BERTrend: Neural Topic Modeling for Emerging Trends Detection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allaa Boutaleb, Jerome Picault, Guillaume Grosjean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking emerging trends and weak signals in large, evolving
text corpora is vital for applications such as monitoring scientific
literature, managing brand reputation, surveilling critical infrastructure and
more generally to any kind of text-based event detection. Existing solutions
often fail to capture the nuanced context or dynamically track evolving
patterns over time. BERTrend, a novel method, addresses these limitations using
neural topic modeling in an online setting. It introduces a new metric to
quantify topic popularity over time by considering both the number of documents
and update frequency. This metric classifies topics as noise, weak, or strong
signals, flagging emerging, rapidly growing topics for further investigation.
Experimentation on two large real-world datasets demonstrates BERTrend's
ability to accurately detect and track meaningful weak signals while filtering
out noise, offering a comprehensive solution for monitoring emerging trends in
large-scale, evolving text corpora. The method can also be used for
retrospective analysis of past events. In addition, the use of Large Language
Models together with BERTrend offers efficient means for the interpretability
of trends of events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, FuturED 2024: Workshop on Future of Event
  Detection (CoLocated with EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Decomposing Compound Ingredients in Meal Plans Using
  <span class="highlight-title">Large Language Model</span>s <span class="chip">KR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Kopitar, Leon Bedrac, Larissa J Strath, Jiang Bian, Gregor Stiglic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the effectiveness of Large Language Models in meal
planning, focusing on their ability to identify and decompose compound
ingredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral
(8x7b)-to assess their proficiency in recognizing and breaking down complex
ingredient combinations. Preliminary results indicate that while Llama-3 (70b)
and GPT-4o excels in accurate decomposition, all models encounter difficulties
with identifying essential elements like seasonings and oils. Despite strong
overall performance, variations in accuracy and completeness were observed
across models. These findings underscore LLMs' potential to enhance
personalized nutrition but highlight the need for further refinement in
ingredient decomposition. Future research should address these limitations to
improve nutritional recommendations and health outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Document Financial Question Answering using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalin Shah, Srikanth Ryali, Ramasubbu Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logic Query of <span class="highlight-title">Thought</span>s: Guiding <span class="highlight-title">Large Language Model</span>s to Answer Complex
  Logic Queries with Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04264v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04264v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the superb performance in many tasks, large language models (LLMs)
bear the risk of generating hallucination or even wrong answers when confronted
with tasks that demand the accuracy of knowledge. The issue becomes even more
noticeable when addressing logic queries that require multiple logic reasoning
steps. On the other hand, knowledge graph (KG) based question answering methods
are capable of accurately identifying the correct answers with the help of
knowledge graph, yet its accuracy could quickly deteriorate when the knowledge
graph itself is sparse and incomplete. It remains a critical challenge on how
to integrate knowledge graph reasoning with LLMs in a mutually beneficial way
so as to mitigate both the hallucination problem of LLMs as well as the
incompleteness issue of knowledge graphs. In this paper, we propose
'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs
with knowledge graph based logic query reasoning. LGOT seamlessly combines
knowledge graph reasoning and LLMs, effectively breaking down complex logic
queries into easy to answer subquestions. Through the utilization of both
knowledge graph reasoning and LLMs, it successfully derives answers for each
subquestion. By aggregating these results and selecting the highest quality
candidate answers for each step, LGOT achieves accurate results to complex
questions. Our experimental findings demonstrate substantial performance
enhancements, with up to 20% improvement over ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cluster-based Graph Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Shuai Zhao, Zhiyong Cheng, Liqiang Nie, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolution Networks (GCNs) have significantly succeeded in learning
user and item representations for recommendation systems. The core of their
efficacy is the ability to explicitly exploit the collaborative signals from
both the first- and high-order neighboring nodes. However, most existing
GCN-based methods overlook the multiple interests of users while performing
high-order graph convolution. Thus, the noisy information from unreliable
neighbor nodes (e.g., users with dissimilar interests) negatively impacts the
representation learning of the target node. Additionally, conducting graph
convolution operations without differentiating high-order neighbors suffers the
over-smoothing issue when stacking more layers, resulting in performance
degradation. In this paper, we aim to capture more valuable information from
high-order neighboring nodes while avoiding noise for better representation
learning of the target node. To achieve this goal, we propose a novel GCN-based
recommendation model, termed Cluster-based Graph Collaborative Filtering
(ClusterGCF). This model performs high-order graph convolution on
cluster-specific graphs, which are constructed by capturing the multiple
interests of users and identifying the common interests among them.
Specifically, we design an unsupervised and optimizable soft node clustering
approach to classify user and item nodes into multiple clusters. Based on the
soft node clustering results and the topology of the user-item interaction
graph, we assign the nodes with probabilities for different clusters to
construct the cluster-specific graphs. To evaluate the effectiveness of
ClusterGCF, we conducted extensive experiments on four publicly available
datasets. Experimental results demonstrate that our model can significantly
improve recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> On <span class="highlight-title">Generative</span> <span class="highlight-title">Agent</span>s in Recommendation <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10108v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10108v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, <span class="highlight-author">Tat-Seng Chua</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are the cornerstone of today's information dissemination,
yet a disconnect between offline metrics and online performance greatly hinders
their development. Addressing this challenge, we envision a recommendation
simulator, capitalizing on recent breakthroughs in human-level intelligence
exhibited by Large Language Models (LLMs). We propose Agent4Rec, a user
simulator in recommendation, leveraging LLM-empowered generative agents
equipped with user profile, memory, and actions modules specifically tailored
for the recommender system. In particular, these agents' profile modules are
initialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),
capturing users' unique tastes and social traits; memory modules log both
factual and emotional memories and are integrated with an emotion-driven
reflection mechanism; action modules support a wide variety of behaviors,
spanning both taste-driven and emotion-driven actions. Each agent interacts
with personalized recommender models in a page-by-page manner, relying on a
pre-implemented collaborative filtering-based recommendation algorithm. We
delve into both the capabilities and limitations of Agent4Rec, aiming to
explore an essential research question: ``To what extent can LLM-empowered
generative agents faithfully simulate the behavior of real, autonomous humans
in recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec
highlight both the alignment and deviation between agents and user-personalized
preferences. Beyond mere performance comparison, we explore insightful
experiments, such as emulating the filter bubble effect and discovering the
underlying causal relationships in recommendation tasks. Our codes are
available at https://github.com/LehengTHU/Agent4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2024 perspective paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Noise Resilient for QoS Prediction with Probabilistic Deep
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02580v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02580v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Sheng Huang, Meng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate Quality of Service (QoS) prediction is essential for enhancing user
satisfaction in web recommendation systems, yet existing prediction models
often overlook feature noise, focusing predominantly on label noise. In this
paper, we present the Probabilistic Deep Supervision Network (PDS-Net), a
robust framework designed to effectively identify and mitigate feature noise,
thereby improving QoS prediction accuracy. PDS-Net operates with a dual-branch
architecture: the main branch utilizes a decoder network to learn a
Gaussian-based prior distribution from known features, while the second branch
derives a posterior distribution based on true labels. A key innovation of
PDS-Net is its condition-based noise recognition loss function, which enables
precise identification of noisy features in objects (users or services). Once
noisy features are identified, PDS-Net refines the feature's prior
distribution, aligning it with the posterior distribution, and propagates this
adjusted distribution to intermediate layers, effectively reducing noise
interference. Extensive experiments conducted on two real-world QoS datasets
demonstrate that PDS-Net consistently outperforms existing models, achieving an
average improvement of 8.91% in MAE on Dataset D1 and 8.32% on Dataset D2
compared to the ate-of-the-art. These results highlight PDS-Net's ability to
accurately capture complex user-service relationships and handle feature noise,
underscoring its robustness and versatility across diverse QoS prediction
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limpeh ga li gong: Challenges in Singlish Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Qi Chan, Lynnette Hui Xian Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or Colloquial Singapore English, is a language formed from oral and
social communication within multicultural Singapore. In this work, we work on a
fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)
tagging of Singlish sentences. For our analysis, we build a parallel Singlish
dataset containing direct English translations and POS tags, with translation
and POS annotation done by native Singlish speakers. Our experiments show that
automatic transition- and transformer- based taggers perform with only $\sim
80\%$ accuracy when evaluated against human-annotated POS labels, suggesting
that there is indeed room for improvement on computation analysis of the
language. We provide an exposition of challenges in Singlish annotation: its
inconsistencies in form and semantics, the highly context-dependent particles
of the language, its structural unique expressions, and the variation of the
language on different mediums. Our task definition, resultant labels and
results reflects the challenges in analysing colloquial languages formulated
from a variety of dialects, and paves the way for future studies beyond POS
tagging.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway
  Tree Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Jacovella, Ali Keshavarzi, Elsa Angelini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances with deep learning (DL), automated airway segmentation from
chest CT scans continues to face challenges in segmentation quality and
generalization across cohorts. To address these, we propose integrating
Curriculum Learning (CL) into airway segmentation networks, distributing the
training set into batches according to ad-hoc complexity scores derived from CT
scans and corresponding ground-truth tree features. We specifically investigate
few-shot domain adaptation, targeting scenarios where manual annotation of a
full fine-tuning dataset is prohibitively expensive. Results are reported on
two large open-cohorts (ATM22 and AIIB23) with high performance using CL for
full training (Source domain) and few-shot fine-tuning (Target domain), but
with also some insights on potential detrimental effects if using a classic
Bootstrapping scoring function or if not using proper scan sequencing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for 22nd IEEE International Symposium on Biomedical
  Imaging (ISBI), Houston, TX, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketched Equivariant Imaging Regularization and Deep Internal Learning
  for Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guixian Xu, Jinglai Li, Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Imaging (EI) regularization has become the de-facto technique for
unsupervised training of deep imaging networks, without any need of
ground-truth data. Observing that the EI-based unsupervised training paradigm
currently has significant computational redundancy leading to inefficiency in
high-dimensional applications, we propose a sketched EI regularization which
leverages the randomized sketching techniques for acceleration. We then extend
our sketched EI regularization to develop an accelerated deep internal learning
framework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be
efficiently applied for single-image and task-adapted reconstruction. Our
numerical study on X-ray CT image reconstruction tasks demonstrate that our
approach can achieve order-of-magnitude computational acceleration over
standard EI-based counterpart in single-input setting, and network adaptation
at test time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinDVer: Explainable Claim Verification over Long and Hybrid-Content
  Financial Documents <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zhao, Yitao Long, Yuru Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Yiming Zhang, Xiangru Tang, Chen Zhao, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FinDVer, a comprehensive benchmark specifically designed to
evaluate the explainable claim verification capabilities of LLMs in the context
of understanding and analyzing long, hybrid-content financial documents.
FinDVer contains 2,400 expert-annotated examples, divided into three subsets:
information extraction, numerical reasoning, and knowledge-intensive reasoning,
each addressing common scenarios encountered in real-world financial contexts.
We assess a broad spectrum of LLMs under long-context and RAG settings. Our
results show that even the current best-performing system, GPT-4o, still lags
behind human experts. We further provide in-depth analysis on long-context and
RAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering
insights to drive future advancements. We believe that FinDVer can serve as a
valuable benchmark for evaluating LLMs in claim verification over complex,
expert-domain documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tract-RLFormer: A Tract-Specific RL policy based Decoder-only
  <span class="highlight-title">Transformer</span> Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fiber tractography is a cornerstone of neuroimaging, enabling the detailed
mapping of the brain's white matter pathways through diffusion MRI. This is
crucial for understanding brain connectivity and function, making it a valuable
tool in neurological applications. Despite its importance, tractography faces
challenges due to its complexity and susceptibility to false positives,
misrepresenting vital pathways. To address these issues, recent strategies have
shifted towards deep learning, utilizing supervised learning, which depends on
precise ground truth, or reinforcement learning, which operates without it. In
this work, we propose Tract-RLFormer, a network utilizing both supervised and
reinforcement learning, in a two-stage policy refinement process that markedly
improves the accuracy and generalizability across various data-sets. By
employing a tract-specific approach, our network directly delineates the tracts
of interest, bypassing the traditional segmentation process. Through rigorous
validation on datasets such as TractoInferno, HCP, and ISMRM-2015, our
methodology demonstrates a leap forward in tractography, showcasing its ability
to accurately map the brain's white matter tracts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FisherMask: Enhancing Neural Network Labeling Efficiency in Image
  Classification Using Fisher Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreen Gul, Mohamed Elmahallawy, Sanjay Madria, Ardhendu Tripathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models are popular across various domains due to their
remarkable performance and efficiency. However, their effectiveness relies
heavily on large amounts of labeled data, which are often time-consuming and
labor-intensive to generate manually. To overcome this challenge, it is
essential to develop strategies that reduce reliance on extensive labeled data
while preserving model performance. In this paper, we propose FisherMask, a
Fisher information-based active learning (AL) approach that identifies key
network parameters by masking them based on their Fisher information values.
FisherMask enhances batch AL by using Fisher information to select the most
critical parameters, allowing the identification of the most impactful samples
during AL training. Moreover, Fisher information possesses favorable
statistical properties, offering valuable insights into model behavior and
providing a better understanding of the performance characteristics within the
AL pipeline. Our extensive experiments demonstrate that FisherMask
significantly outperforms state-of-the-art methods on diverse datasets,
including CIFAR-10 and FashionMNIST, especially under imbalanced settings.
These improvements lead to substantial gains in labeling efficiency. Hence
serving as an effective tool to measure the sensitivity of model parameters to
data samples. Our code is available on
\url{https://github.com/sgchr273/FisherMask}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Differentially Private String Distances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Yao-Chieh Hu, Erzhi Liu, Han Liu, Zhao Song, Lichen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamental
data structure task is to estimate the distances between a given query $B\in
\{0,1\}^n$ with all the strings in the database. In addition, one might further
want to ensure the integrity of the database by releasing these distance
statistics in a secure manner. In this work, we propose differentially private
(DP) data structures for this type of tasks, with a focus on Hamming and edit
distance. On top of the strong privacy guarantees, our data structures are also
time- and space-efficient. In particular, our data structure is $\epsilon$-DP
against any sequence of queries of arbitrary length, and for any query $B$ such
that the maximum distance to any string in the database is at most $k$, we
output $m$ distance estimates. Moreover,
  - For Hamming distance, our data structure answers any query in $\widetilde
O(mk+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/\log k})$;
  - For edit distance, our data structure answers any query in $\widetilde
O(mk^2+n)$ time and each estimate deviates from the true distance by at most
$\widetilde O(k/e^{\epsilon/(\log k \log n)})$.
  For moderate $k$, both data structures support sublinear query operations. We
obtain these results via a novel adaptation of the randomized response
technique as a bit flipping procedure, applied to the sketched strings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous-Time Analysis of Adaptive Optimization and Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhys Gould, Hidenori Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive optimization algorithms, particularly Adam and its variant AdamW,
are fundamental components of modern deep learning. However, their training
dynamics lack comprehensive theoretical understanding, with limited insight
into why common practices - such as specific hyperparameter choices and
normalization layers - contribute to successful generalization. This work
presents a continuous-time formulation of Adam and AdamW, facilitating a
tractable analysis of training dynamics that can shed light on such practical
questions. We theoretically derive a stable region for Adam's hyperparameters
$(\beta, \gamma)$ that ensures bounded updates, empirically verifying these
predictions by observing unstable exponential growth of parameter updates
outside this region. Furthermore, we theoretically justify the success of
normalization layers by uncovering an implicit meta-adaptive effect of
scale-invariant architectural components. This insight leads to an explicit
optimizer, $2$-Adam, which we generalize to $k$-Adam - an optimizer that
applies an adaptive normalization procedure $k$ times, encompassing Adam
(corresponding to $k=1$) and Adam with a normalization layer (corresponding to
$k=2$). Overall, our continuous-time formulation of Adam facilitates a
principled analysis, offering deeper understanding of optimal hyperparameter
choices and architectural decisions in modern deep learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Pollock, Igor Shilov, Euodia Dodd, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks (MIAs) are widely used to empirically assess the
privacy risks of samples used to train a target machine learning model.
State-of-the-art methods however require training hundreds of shadow models,
with the same size and architecture of the target model, solely to evaluate the
privacy risk. While one might be able to afford this for small models, the cost
often becomes prohibitive for medium and large models.
  We here instead propose a novel approach to identify the at-risk samples
using only artifacts available during training, with little to no additional
computational overhead. Our method analyzes individual per-sample loss traces
and uses them to identify the vulnerable data samples. We demonstrate the
effectiveness of our artifact-based approach through experiments on the CIFAR10
dataset, showing high precision in identifying vulnerable samples as determined
by a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the
same precision as another SOTA MIA when measured against LiRA, despite it being
orders of magnitude cheaper. We then show LT-IQR to outperform alternative loss
aggregation methods, perform ablation studies on hyperparameters, and validate
the robustness of our method to the target metric. Finally, we study the
evolution of the vulnerability score distribution throughout training as a
metric for model-level risk assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-aware Reinforcement Feature Space Reconstruction for Graph Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyang Ying, Haoyue Bai, Kunpeng Liu, Yanjie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature space is an environment where data points are vectorized to represent
the original dataset. Reconstructing a good feature space is essential to
augment the AI power of data, improve model generalization, and increase the
availability of downstream ML models. Existing literature, such as feature
transformation and feature selection, is labor-intensive (e.g., heavy reliance
on empirical experience) and mostly designed for tabular data. Moreover, these
methods regard data samples as independent, which ignores the unique
topological structure when applied to graph data, thus resulting in a
suboptimal reconstruction feature space. Can we consider the topological
information to automatically reconstruct feature space for graph data without
heavy experiential knowledge? To fill this gap, we leverage topology-aware
reinforcement learning to automate and optimize feature space reconstruction
for graph data. Our approach combines the extraction of core subgraphs to
capture essential structural information with a graph neural network (GNN) to
encode topological features and reduce computing complexity. Then we introduce
three reinforcement agents within a hierarchical structure to systematically
generate meaningful features through an iterative process, effectively
reconstructing the feature space. This framework provides a principled solution
for attributed graph feature space reconstruction. The extensive experiments
demonstrate the effectiveness and efficiency of including topological
awareness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aioli: A Unified Optimization Framework for <span class="highlight-title">Language Model</span> Data Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity per group. In this paper, we study the cause of this inconsistency
by unifying existing methods into a standard optimization framework. We show
that all methods set proportions to minimize total loss, subject to a
method-specific mixing law -- an assumption on how loss is a function of
mixture proportions. We find that existing parameterizations of mixing laws can
express the true loss-proportion relationship empirically, but the methods
themselves often set the mixing law parameters inaccurately, resulting in poor
and inconsistent performance. Finally, we leverage the insights from our
framework to derive a new online method named Aioli, which directly estimates
the mixing law parameters throughout training and uses them to dynamically
adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out
of 6 datasets by an average of 0.28 test perplexity points, whereas existing
methods fail to consistently beat stratified sampling, doing up to 6.9 points
worse. Moreover, in a practical setting where proportions are learned on
shorter runs due to computational constraints, Aioli can dynamically adjust
these proportions over the full training run, consistently improving
performance over existing methods by up to 12.01 test perplexity points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Privacy Under Class Imbalance: Methods and Empirical
  Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Rosenblatt, Yuliia Lut, Eitan Turok, Marco Avella-Medina, Rachel Cummings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imbalanced learning occurs in classification settings where the distribution
of class-labels is highly skewed in the training data, such as when predicting
rare diseases or in fraud detection. This class imbalance presents a
significant algorithmic challenge, which can be further exacerbated when
privacy-preserving techniques such as differential privacy are applied to
protect sensitive training data. Our work formalizes these challenges and
provides a number of algorithmic solutions. We consider DP variants of
pre-processing methods that privately augment the original dataset to reduce
the class imbalance; these include oversampling, SMOTE, and private synthetic
data generation. We also consider DP variants of in-processing techniques,
which adjust the learning algorithm to account for the imbalance; these include
model bagging, class-weighted empirical risk minimization and class-weighted
deep learning. For each method, we either adapt an existing imbalanced learning
technique to the private setting or demonstrate its incompatibility with
differential privacy. Finally, we empirically evaluate these privacy-preserving
imbalanced learning methods under various data and distributional settings. We
find that private synthetic data methods perform well as a data pre-processing
step, while class-weighted ERMs are an alternative in higher-dimensional
settings where private synthetic data suffers from the curse of dimensionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Subsystem Dynamics in Nonlinear Systems via Port-Hamiltonian
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. J. E. van Otterdijk, S. Moradi, S. Weiland, R. Tóth, N. O. Jaensson, M. Schoukens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Port-Hamiltonian neural networks (pHNNs) are emerging as a powerful modeling
tool that integrates physical laws with deep learning techniques. While most
research has focused on modeling the entire dynamics of interconnected systems,
the potential for identifying and modeling individual subsystems while
operating as part of a larger system has been overlooked. This study addresses
this gap by introducing a novel method for using pHNNs to identify such
subsystems based solely on input-output measurements. By utilizing the inherent
compositional property of the port-Hamiltonian systems, we developed an
algorithm that learns the dynamics of individual subsystems, without requiring
direct access to their internal states. On top of that, by choosing an output
error (OE) model structure, we have been able to handle measurement noise
effectively. The effectiveness of the proposed approach is demonstrated through
tests on interconnected systems, including multi-physics scenarios,
demonstrating its potential for identifying subsystem dynamics and facilitating
their integration into new interconnected models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to ECC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Dictionary Signal Model for Sparse Representations of Multivariate
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Cappelletti, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing and exploiting multivariate signals require capturing complex
relations between variables. We define a novel Graph-Dictionary signal model,
where a finite set of graphs characterizes relationships in data distribution
through a weighted sum of their Laplacians. We propose a framework to infer the
graph dictionary representation from observed data, along with a bilinear
generalization of the primal-dual splitting algorithm to solve the learning
problem. Our new formulation allows to include a priori knowledge on signal
properties, as well as on underlying graphs and their coefficients. We show the
capability of our method to reconstruct graphs from signals in multiple
synthetic settings, where our model outperforms previous baselines. Then, we
exploit graph-dictionary representations in a motor imagery decoding task on
brain activity data, where we classify imagined motion better than standard
methods relying on many more features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust,
  Reliable, and Safe Learning Techniques for Real-world Robotics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puze Liu, Jonas Günster, Niklas Funk, Simon Gröger, Dong Chen, Haitham Bou-Ammar, Julius Jankowski, Ante Marić, Sylvain Calinon, Andrej Orsula, Miguel Olivares-Mendez, Hongyi Zhou, Rudolf Lioutikov, Gerhard Neumann, Amarildo Likmeta Amirhossein Zhalehmehrabi, Thomas Bonenfant, Marcello Restelli, Davide Tateo, Ziyuan Liu, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods have a groundbreaking impact in many application
domains, but their application on real robotic platforms is still limited.
Despite the many challenges associated with combining machine learning
technology with robotics, robot learning remains one of the most promising
directions for enhancing the capabilities of robots. When deploying
learning-based approaches on real robots, extra effort is required to address
the challenges posed by various real-world factors. To investigate the key
factors influencing real-world deployment and to encourage original solutions
from different researchers, we organized the Robot Air Hockey Challenge at the
NeurIPS 2023 conference. We selected the air hockey task as a benchmark,
encompassing low-level robotics problems and high-level tactics. Different from
other machine learning-centric benchmarks, participants need to tackle
practical challenges in robotics, such as the sim-to-real gap, low-level
control issues, safety problems, real-time requirements, and the limited
availability of real-world data. Furthermore, we focus on a dynamic
environment, removing the typical assumption of quasi-static motions of other
real-world benchmarks. The competition's results show that solutions combining
learning-based approaches with prior knowledge outperform those relying solely
on data when real-world deployment is challenging. Our ablation study reveals
which real-world factors may be overlooked when building a learning-based
solution. The successful real-world air hockey deployment of best-performing
agents sets the foundation for future competitions and follow-up research
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept at NeurIPS 2024 Dataset and Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STARS: Sensor-agnostic <span class="highlight-title">Transformer</span> Architecture for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan King, Jaime Rodriguez, Diego Llanes, Timothy Doster, Tegan Emerson, James Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a sensor-agnostic spectral transformer as the basis for spectral
foundation models. To that end, we introduce a Universal Spectral
Representation (USR) that leverages sensor meta-data, such as sensing kernel
specifications and sensing wavelengths, to encode spectra obtained from any
spectral instrument into a common representation, such that a single model can
ingest data from any sensor. Furthermore, we develop a methodology for
pre-training such models in a self-supervised manner using a novel random
sensor-augmentation and reconstruction pipeline to learn spectral features
independent of the sensing paradigm. We demonstrate that our architecture can
learn sensor independent spectral features that generalize effectively to
sensors not seen during training. This work sets the stage for training
foundation models that can both leverage and be effective for the growing
diversity of spectral data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Task-Optimized Models of the Primate Visual Ventral
  Stream 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Gokce, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When trained on large-scale object classification datasets, certain
artificial neural network models begin to approximate core object recognition
(COR) behaviors and neural response patterns in the primate visual ventral
stream (VVS). While recent machine learning advances suggest that scaling model
size, dataset size, and compute resources improve task performance, the impact
of scaling on brain alignment remains unclear. In this study, we explore
scaling laws for modeling the primate VVS by systematically evaluating over 600
models trained under controlled conditions on benchmarks spanning V1, V2, V4,
IT and COR behaviors. We observe that while behavioral alignment continues to
scale with larger models, neural alignment saturates. This observation remains
true across model architectures and training datasets, even though models with
stronger inductive bias and datasets with higher-quality images are more
compute-efficient. Increased scaling is especially beneficial for higher-level
visual areas, where small models trained on few samples exhibit only poor
alignment. Finally, we develop a scaling recipe, indicating that a greater
proportion of compute should be allocated to data samples over model size. Our
results suggest that while scaling alone might suffice for alignment with human
core object recognition behavior, it will not yield improved models of the
brain's visual ventral stream with current architectures and datasets,
highlighting the need for novel strategies in building brain-like models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages for the main paper, 20 pages in total. 6 main figures and 10
  supplementary figures. Code, model weights, and benchmark results can be
  accessed at https://github.com/epflneuroailab/scaling-primate-vvs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample and Computationally Efficient Robust Learning of Gaussian
  Single-Index Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A single-index model (SIM) is a function of the form
$\sigma(\mathbf{w}^{\ast} \cdot \mathbf{x})$, where $\sigma: \mathbb{R} \to
\mathbb{R}$ is a known link function and $\mathbf{w}^{\ast}$ is a hidden unit
vector. We study the task of learning SIMs in the agnostic (a.k.a. adversarial
label noise) model with respect to the $L^2_2$-loss under the Gaussian
distribution. Our main result is a sample and computationally efficient
agnostic proper learner that attains $L^2_2$-error of
$O(\mathrm{OPT})+\epsilon$, where $\mathrm{OPT}$ is the optimal loss. The
sample complexity of our algorithm is $\tilde{O}(d^{\lceil
k^{\ast}/2\rceil}+d/\epsilon)$, where $k^{\ast}$ is the information-exponent of
$\sigma$ corresponding to the degree of its first non-zero Hermite coefficient.
This sample bound nearly matches known CSQ lower bounds, even in the realizable
setting. Prior algorithmic work in this setting had focused on learning in the
realizable case or in the presence of semi-random noise. Prior computationally
efficient robust learners required significantly stronger assumptions on the
link function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc
  Explainability in Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio De Santis, Riccardo Campi, Matteo Bianchi, Marco Brambilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have seen significant performance
improvements in recent years. However, due to their size and complexity, they
function as black-boxes, leading to transparency concerns. State-of-the-art
saliency methods generate local explanations that highlight the area in the
input image where a class is identified but cannot explain how a concept of
interest contributes to the prediction, which is essential for bias mitigation.
On the other hand, concept-based methods, such as TCAV (Testing with Concept
Activation Vectors), provide insights into how sensitive is the network to a
concept, but cannot compute its attribution in a specific prediction nor show
its location within the input image. This paper introduces a novel post-hoc
explainability framework, Visual-TCAV, which aims to bridge the gap between
these methods by providing both local and global explanations for CNN-based
image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to
generate saliency maps that show where concepts are recognized by the network.
Moreover, it can estimate the attribution of these concepts to the output of
any class using a generalization of Integrated Gradients. This framework is
evaluated on popular CNN architectures, with its validity further confirmed via
experiments where ground truth for explanations is known, and a comparison with
TCAV. Our code will be made available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPMN Risk Assessment under Federated Learning Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Pan, Ziliang Hong, Gorkem Durak, Elif Keles, Halil Ertugrul Aktas, Yavuz Taktak, Alpay Medetalibeyoglu, Zheyuan Zhang, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Frank Miller, Rajesh N. Keswani, Michael B. Wallace, Ziyue Xu, Ulas Bagci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) is
essential for identifying high-risk cases that require timely intervention. In
this study, we develop a federated learning framework for multi-center IPMN
classification utilizing a comprehensive pancreas MRI dataset. This dataset
includes 653 T1-weighted and 656 T2-weighted MRI images, accompanied by
corresponding IPMN risk scores from 7 leading medical institutions, making it
the largest and most diverse dataset for IPMN classification to date. We assess
the performance of DenseNet-121 in both centralized and federated settings for
training on distributed data. Our results demonstrate that the federated
learning approach achieves high classification accuracy comparable to
centralized learning while ensuring data privacy across institutions. This work
marks a significant advancement in collaborative IPMN classification,
facilitating secure and high-accuracy model training across multiple centers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural
  Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Zhichun Guo, Guanpeng Li, Bingzhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have become essential tools for analyzing
non-Euclidean data across various domains. During training stage, sampling
plays an important role in reducing latency by limiting the number of nodes
processed, particularly in large-scale applications. However, as the demand for
better prediction performance grows, existing sampling algorithms become
increasingly complex, leading to significant overhead. To mitigate this, we
propose YOSO (You-Only-Sample-Once), an algorithm designed to achieve efficient
training while preserving prediction accuracy. YOSO introduces a compressed
sensing (CS)-based sampling and reconstruction framework, where nodes are
sampled once at input layer, followed by a lossless reconstruction at the
output layer per epoch. By integrating the reconstruction process with the loss
function of specific learning tasks, YOSO not only avoids costly computations
in traditional compressed sensing (CS) methods, such as orthonormal basis
calculations, but also ensures high-probability accuracy retention which
equivalent to full node participation. Experimental results on node
classification and link prediction demonstrate the effectiveness and efficiency
of YOSO, reducing GNN training by an average of 75\% compared to
state-of-the-art methods, while maintaining accuracy on par with top-performing
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell What You Hear From What You See -- Video to Audio Generation
  Through Text <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulong Liu, Kun Su, Eli Shlizerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The content of visual and audio scenes is multi-faceted such that a video can
be paired with various audio and vice-versa. Thereby, in video-to-audio
generation task, it is imperative to introduce steering approaches for
controlling the generated audio. While Video-to-Audio generation is a
well-established generative task, existing methods lack such controllability.
In this work, we propose VATT, a multi-modal generative framework that takes a
video and an optional text prompt as input, and generates audio and optional
textual description of the audio. Such a framework has two advantages: i)
Video-to-Audio generation process can be refined and controlled via text which
complements the context of visual information, and ii) The model can suggest
what audio to generate for the video by generating audio captions. VATT
consists of two key modules: VATT Converter, a LLM that is fine-tuned for
instructions and includes a projection layer that maps video features to the
LLM vector space; and VATT Audio, a transformer that generates audio tokens
from visual frames and from optional text prompt using iterative parallel
decoding. The audio tokens are converted to a waveform by pretrained neural
codec. Experiments show that when VATT is compared to existing video-to-audio
generation methods in objective metrics, it achieves competitive performance
when the audio caption is not provided. When the audio caption is provided as a
prompt, VATT achieves even more refined performance (lowest KLD score of 1.41).
Furthermore, subjective studies show that VATT Audio has been chosen as
preferred generated audio than audio generated by existing methods. VATT
enables controllable video-to-audio generation through text as well as
suggesting text prompts for videos through audio captions, unlocking novel
applications such as text-guided video-to-audio generation and video-to-audio
captioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Molecular Graph Generation with Flow Matching and Optimal
  Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Hou, Tian Zhu, Milong Ren, Dongbo Bu, Xin Gao, Chunming Zhang, Shiwei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating molecular graphs is crucial in drug design and discovery but
remains challenging due to the complex interdependencies between nodes and
edges. While diffusion models have demonstrated their potentiality in molecular
graph design, they often suffer from unstable training and inefficient
sampling. To enhance generation performance and training stability, we propose
GGFlow, a discrete flow matching generative model incorporating optimal
transport for molecular graphs and it incorporates an edge-augmented graph
transformer to enable the direct communications among chemical bounds.
Additionally, GGFlow introduces a novel goal-guided generation framework to
control the generative trajectory of our model, aiming to design novel
molecular structures with the desired properties. GGFlow demonstrates superior
performance on both unconditional and conditional molecule generation tasks,
outperforming existing baselines and underscoring its effectiveness and
potential for wider application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online-LoRA: Task-free Online <span class="highlight-title">Continual</span> Learning via Low Rank Adaptation <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Wei, Guihong Li, Radu Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting is a significant challenge in online continual
learning (OCL), especially for non-stationary data streams that do not have
well-defined task boundaries. This challenge is exacerbated by the memory
constraints and privacy concerns inherent in rehearsal buffers. To tackle
catastrophic forgetting, in this paper, we introduce Online-LoRA, a novel
framework for task-free OCL. Online-LoRA allows to finetune pre-trained Vision
Transformer (ViT) models in real-time to address the limitations of rehearsal
buffers and leverage pre-trained models' performance benefits. As the main
contribution, our approach features a novel online weight regularization
strategy to identify and consolidate important model parameters. Moreover,
Online-LoRA leverages the training dynamics of loss values to enable the
automatic recognition of the data distribution shifts. Extensive experiments
across many task-free OCL scenarios and benchmark datasets (including
CIFAR-100, ImageNet-R, ImageNet-S, CUB-200 and CORe50) demonstrate that
Online-LoRA can be robustly adapted to various ViT architectures, while
achieving better performance compared to SOTA methods. Our code will be
publicly available at:
https://github.com/Christina200/Online-LoRA-official.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-armed Bandits with Missing Outcome 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilia Mahrooghi, Mahshad Moradi, Sina Akbari, Negar Kiyavash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant progress has been made in designing algorithms that
minimize regret in online decision-making, real-world scenarios often introduce
additional complexities, perhaps the most challenging of which is missing
outcomes. Overlooking this aspect or simply assuming random missingness
invariably leads to biased estimates of the rewards and may result in linear
regret. Despite the practical relevance of this challenge, no rigorous
methodology currently exists for systematically handling missingness,
especially when the missingness mechanism is not random. In this paper, we
address this gap in the context of multi-armed bandits (MAB) with missing
outcomes by analyzing the impact of different missingness mechanisms on
achievable regret bounds. We introduce algorithms that account for missingness
under both missing at random (MAR) and missing not at random (MNAR) models.
Through both analytical and simulation studies, we demonstrate the drastic
improvements in decision-making by accounting for missingness in these
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 5 figures, multi-armed bandits, missing data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Model Fairness and Accuracy with Similarity Networks: A
  Methodological Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Maghool, Paolo Ceravolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an innovative approach to thoroughly explore
dataset features that introduce bias in downstream machine-learning tasks.
Depending on the data format, we use different techniques to map instances into
a similarity feature space. Our method's ability to adjust the resolution of
pairwise similarity provides clear insights into the relationship between the
dataset classification complexity and model fairness. Experimental results
confirm the promising applicability of the similarity network in promoting fair
models. Moreover, leveraging our methodology not only seems promising in
providing a fair downstream task such as classification, it also performs well
in imputation and augmentation of the dataset satisfying the fairness criteria
such as demographic parity and imbalanced classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video RWKV:Video Action Recognition Based RWKV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowen Yin, Chengru Li, Xingbo Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges of high computational costs and long-distance
dependencies in exist ing video understanding methods, such as CNNs and
Transformers, this work introduces RWKV to the video domain in a novel way. We
propose a LSTM CrossRWKV (LCR) framework, designed for spatiotemporal
representation learning to tackle the video understanding task. Specifically,
the proposed linear complexity LCR incorporates a novel Cross RWKV gate to
facilitate interaction be tween current frame edge information and past
features, enhancing the focus on the subject through edge features and globally
aggregating inter-frame features over time. LCR stores long-term mem ory for
video processing through an enhanced LSTM recurrent execution mechanism. By
leveraging the Cross RWKV gate and recurrent execution, LCR effectively
captures both spatial and temporal features. Additionally, the edge information
serves as a forgetting gate for LSTM, guiding long-term memory management.Tube
masking strategy reduces redundant information in food and reduces
overfitting.These advantages enable LSTM CrossRWKV to set a new benchmark in
video under standing, offering a scalable and efficient solution for
comprehensive video analysis. All code and models are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-constrained coupled neural differential equations for one
  dimensional blood flow modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hunor Csala, Arvind Mohan, Daniel Livescu, Amirhossein Arzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational cardiovascular flow modeling plays a crucial role in
understanding blood flow dynamics. While 3D models provide acute details, they
are computationally expensive, especially with fluid-structure interaction
(FSI) simulations. 1D models offer a computationally efficient alternative, by
simplifying the 3D Navier-Stokes equations through axisymmetric flow assumption
and cross-sectional averaging. However, traditional 1D models based on finite
element methods (FEM) often lack accuracy compared to 3D averaged solutions.
This study introduces a novel physics-constrained machine learning technique
that enhances the accuracy of 1D blood flow models while maintaining
computational efficiency. Our approach, utilizing a physics-constrained coupled
neural differential equation (PCNDE) framework, demonstrates superior
performance compared to conventional FEM-based 1D models across a wide range of
inlet boundary condition waveforms and stenosis blockage ratios. A key
innovation lies in the spatial formulation of the momentum conservation
equation, departing from the traditional temporal approach and capitalizing on
the inherent temporal periodicity of blood flow. This spatial neural
differential equation formulation switches space and time and overcomes issues
related to coupling stability and smoothness, while simplifying boundary
condition implementation. The model accurately captures flow rate, area, and
pressure variations for unseen waveforms and geometries. We evaluate the
model's robustness to input noise and explore the loss landscapes associated
with the inclusion of different physics terms. This advanced 1D modeling
technique offers promising potential for rapid cardiovascular simulations,
achieving computational efficiency and accuracy. By combining the strengths of
physics-based and data-driven modeling, this approach enables fast and accurate
cardiovascular simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-validating causal discovery via Leave-One-Variable-Out 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Schkoda, Philipp Faller, Patrick Blöbaum, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach to falsify causal discovery algorithms without
ground truth, which is based on testing the causal model on a pair of variables
that has been dropped when learning the causal model. To this end, we use the
"Leave-One-Variable-Out (LOVO)" prediction where $Y$ is inferred from $X$
without any joint observations of $X$ and $Y$, given only training data from
$X,Z_1,\dots,Z_k$ and from $Z_1,\dots,Z_k,Y$. We demonstrate that causal models
on the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), often
entail conclusions on the dependencies between $X$ and $Y$, enabling this type
of prediction. The prediction error can then be estimated since the joint
distribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have only
been omitted for the purpose of falsification. After presenting this graphical
method, which is applicable to general causal discovery algorithms, we
illustrate how to construct a LOVO predictor tailored towards algorithms
relying on specific a priori assumptions, such as linear additive noise models.
Simulations indicate that the LOVO prediction error is indeed correlated with
the accuracy of the causal outputs, affirming the method's effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WHALE: Towards Generalizable and Scalable World Models for Embodied
  Decision-making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilong Zhang, Ruifeng Chen, Junyin Ye, Yihao Sun, Pengyuan Wang, Jingcheng Pang, Kaiyuan Li, Tianshuo Liu, Haoxin Lin, Yang Yu, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models play a crucial role in decision-making within embodied
environments, enabling cost-free explorations that would otherwise be expensive
in the real world. To facilitate effective decision-making, world models must
be equipped with strong generalizability to support faithful imagination in
out-of-distribution (OOD) regions and provide reliable uncertainty estimation
to assess the credibility of the simulated experiences, both of which present
significant challenges for prior scalable approaches. This paper introduces
WHALE, a framework for learning generalizable world models, consisting of two
key techniques: behavior-conditioning and retracing-rollout.
Behavior-conditioning addresses the policy distribution shift, one of the
primary sources of the world model generalization error, while
retracing-rollout enables efficient uncertainty estimation without the
necessity of model ensembles. These techniques are universal and can be
combined with any neural network architecture for world model learning.
Incorporating these two techniques, we present Whale-ST, a scalable
spatial-temporal transformer-based world model with enhanced generalizability.
We demonstrate the superiority of Whale-ST in simulation tasks by evaluating
both value estimation accuracy and video generation fidelity. Additionally, we
examine the effectiveness of our uncertainty estimation technique, which
enhances model-based policy optimization in fully offline scenarios.
Furthermore, we propose Whale-X, a 414M parameter world model trained on 970K
trajectories from Open X-Embodiment datasets. We show that Whale-X exhibits
promising scalability and strong generalizability in real-world manipulation
scenarios using minimal demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">Distill</span>ation Neural Network for Predicting Car-following
  Behaviour of Human-driven and Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayobami Adewale, Chris Lee, Amnir Hadachi, Nicolly Lima da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and
Human-driven vehicles (HDVs), understanding the car-following behaviour is
important to improve traffic efficiency and road safety. Using a real-world
trajectory dataset, this study uses descriptive and statistical analysis to
investigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV
and HDV-HDV in mixed traffic. The ANOVA test showed that car-following
behaviours across different vehicle pairs are statistically significant
(p-value < 0.05).
  We also introduce a data-driven Knowledge Distillation Neural Network (KDNN)
model for predicting car-following behaviour in terms of speed. The KDNN model
demonstrates comparable predictive accuracy to its teacher network, a Long
Short-Term Memory (LSTM) network, and outperforms both the standalone student
network, a Multilayer Perceptron (MLP), and traditional physics-based models
like the Gipps model. Notably, the KDNN model better prevents collisions,
measured by minimum Time-to-Collision (TTC), and operates with lower
computational power, making it ideal for AVs or driving simulators requiring
efficient computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27th IEEE International Conference on Intelligent Transportation
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acceleration for Deep Reinforcement Learning using Parallel and
  Distributed Computing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Liu, Xin Xu, Peng Qiao, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has led to dramatic breakthroughs in the field of
artificial intelligence for the past few years. As the amount of rollout
experience data and the size of neural networks for deep reinforcement learning
have grown continuously, handling the training process and reducing the time
consumption using parallel and distributed computing is becoming an urgent and
essential desire. In this paper, we perform a broad and thorough investigation
on training acceleration methodologies for deep reinforcement learning based on
parallel and distributed computing, providing a comprehensive survey in this
field with state-of-the-art methods and pointers to core references. In
particular, a taxonomy of literature is provided, along with a discussion of
emerging topics and open issues. This incorporates learning system
architectures, simulation parallelism, computing parallelism, distributed
synchronization mechanisms, and deep evolutionary reinforcement learning.
Further, we compare 16 current open-source libraries and platforms with
criteria of facilitating rapid development. Finally, we extrapolate future
directions that deserve further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-Step Concept-Based Approach for Enhanced Interpretability and
  Trust in Skin Lesion Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Patrício, Luís F. Teixeira, João C. Neves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main challenges hindering the adoption of deep learning-based systems in
clinical settings are the scarcity of annotated data and the lack of
interpretability and trust in these systems. Concept Bottleneck Models (CBMs)
offer inherent interpretability by constraining the final disease prediction on
a set of human-understandable concepts. However, this inherent interpretability
comes at the cost of greater annotation burden. Additionally, adding new
concepts requires retraining the entire system. In this work, we introduce a
novel two-step methodology that addresses both of these challenges. By
simulating the two stages of a CBM, we utilize a pretrained Vision Language
Model (VLM) to automatically predict clinical concepts, and a Large Language
Model (LLM) to generate disease diagnoses based on the predicted concepts. We
validate our approach on three skin lesion datasets, demonstrating that it
outperforms traditional CBMs and state-of-the-art explainable methods, all
without requiring any training and utilizing only a few annotated examples. The
code is available at
https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Stroke through Retinal Graphs and Multimodal Self-supervised
  Learning <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Huang, Bastian Wittmann, Olga Demler, Bjoern Menze, Neda Davoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early identification of stroke is crucial for intervention, requiring
reliable models. We proposed an efficient retinal image representation together
with clinical information to capture a comprehensive overview of cardiovascular
health, leveraging large multimodal datasets for new medical insights. Our
approach is one of the first contrastive frameworks that integrates graph and
tabular data, using vessel graphs derived from retinal images for efficient
representation. This method, combined with multimodal contrastive learning,
significantly enhances stroke prediction accuracy by integrating data from
multiple sources and using contrastive learning for transfer learning. The
self-supervised learning techniques employed allow the model to learn
effectively from unlabeled data, reducing the dependency on large annotated
datasets. Our framework showed an AUROC improvement of 3.78% from supervised to
self-supervised approaches. Additionally, the graph-level representation
approach achieved superior performance to image encoders while significantly
reducing pre-training and fine-tuning runtimes. These findings indicate that
retinal images are a cost-effective method for improving cardiovascular disease
predictions and pave the way for future research into retinal and cerebral
vessel connections and the use of graph-based retinal vessel representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as oral paper at ML-CDS workshop, MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning-driven Anomaly Detection and <span class="highlight-title">Forecast</span>ing for Euclid
  Space Telescope Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Gómez, Roland D. Vavrek, Guillermo Buenadicha, John Hoar, Sandor Kruk, Jan Reerink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art space science missions increasingly rely on automation due
to spacecraft complexity and the costs of human oversight. The high volume of
data, including scientific and telemetry data, makes manual inspection
challenging. Machine learning offers significant potential to meet these
demands.
  The Euclid space telescope, in its survey phase since February 2024,
exemplifies this shift. Euclid's success depends on accurate monitoring and
interpretation of housekeeping telemetry and science-derived data. Thousands of
telemetry parameters, monitored as time series, may or may not impact the
quality of scientific data. These parameters have complex interdependencies,
often due to physical relationships (e.g., proximity of temperature sensors).
Optimising science operations requires careful anomaly detection and
identification of hidden parameter states. Moreover, understanding the
interactions between known anomalies and physical quantities is crucial yet
complex, as related parameters may display anomalies with varied timing and
intensity.
  We address these challenges by analysing temperature anomalies in Euclid's
telemetry from February to August 2024, focusing on eleven temperature
parameters and 35 covariates. We use a predictive XGBoost model to forecast
temperatures based on historical values, detecting anomalies as deviations from
predictions. A second XGBoost model predicts anomalies from covariates,
capturing their relationships to temperature anomalies. We identify the top
three anomalies per parameter and analyse their interactions with covariates
using SHAP (Shapley Additive Explanations), enabling rapid, automated analysis
of complex parameter relationships.
  Our method demonstrates how machine learning can enhance telemetry
monitoring, offering scalable solutions for other missions with similar data
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network EM Algorithm for Gaussian Mixture Model in Decentralized
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyuan Wu, Bin Du, Xuetong Li, Hansheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We systematically study various network Expectation-Maximization (EM)
algorithms for the Gaussian mixture model within the framework of decentralized
federated learning. Our theoretical investigation reveals that directly
extending the classical decentralized supervised learning method to the EM
algorithm exhibits poor estimation accuracy with heterogeneous data across
clients and struggles to converge numerically when Gaussian components are
poorly-separated. To address these issues, we propose two novel solutions.
First, to handle heterogeneous data, we introduce a momentum network EM (MNEM)
algorithm, which uses a momentum parameter to combine information from both the
current and historical estimators. Second, to tackle the challenge of
poorly-separated Gaussian components, we develop a semi-supervised MNEM
(semi-MNEM) algorithm, which leverages partially labeled data. Rigorous
theoretical analysis demonstrates that MNEM can achieve statistical efficiency
comparable to that of the whole sample estimator when the mixture components
satisfy certain separation conditions, even in heterogeneous scenarios.
Moreover, the semi-MNEM estimator enhances the convergence speed of the MNEM
algorithm, effectively addressing the numerical convergence challenges in
poorly-separated scenarios. Extensive simulation and real data analyses are
conducted to justify our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Real-Time Simulation of Elastoplastic Deformation Using
  Multi-Task Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Schmeitz, Joris Remmers, Olga Mula, Olaf van der Sluis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a surrogate modeling framework merging proper
orthogonal decomposition, long short-term memory networks, and multi-task
learning, to accurately predict elastoplastic deformations in real-time.
Superior to single-task neural networks, this approach achieves a mean absolute
error below 0.40\% across various state variables, with the multi-task model
showing enhanced generalization by mitigating overfitting through shared
layers. Moreover, in our use cases, a pre-trained multi-task model can
effectively train additional variables with as few as 20 samples, demonstrating
its deep understanding of complex scenarios. This is notably efficient compared
to single-task models, which typically require around 100 samples.
  Significantly faster than traditional finite element analysis, our model
accelerates computations by approximately a million times, making it a
substantial advancement for real-time predictive modeling in engineering
applications. While it necessitates further testing on more intricate models,
this framework shows substantial promise in elevating both efficiency and
accuracy in engineering applications, particularly for real-time scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-set object detection: towards unified problem formulation and
  benchmarking <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejer Ammar, Nikita Kiselov, Guillaume Lapouge, Romaric Audigier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications where confidence is key, like autonomous driving,
the accurate detection and appropriate handling of classes differing from those
used during training are crucial. Despite the proposal of various unknown
object detection approaches, we have observed widespread inconsistencies among
them regarding the datasets, metrics, and scenarios used, alongside a notable
absence of a clear definition for unknown objects, which hampers meaningful
evaluation. To counter these issues, we introduce two benchmarks: a unified
VOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear
hierarchical object definition besides new evaluation metrics. Complementing
the benchmark, we exploit recent self-supervised Vision Transformers
performance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD),
through OW-DETR++. State-of-the-art methods are extensively evaluated on the
proposed benchmarks. This study provides a clear problem definition, ensures
consistent evaluations, and draws new conclusions about effectiveness of OSOD
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024 Workshop: "The 3rd Workshop for
  Out-of-Distribution Generalization in Computer Vision Foundation Models"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training objective drives the consistency of representational similarity
  across <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laure Ciernik, Lorenz Linhardt, Marco Morik, Jonas Dippel, Simon Kornblith, Lukas Muttenthaler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Platonic Representation Hypothesis claims that recent foundation models
are converging to a shared representation space as a function of their
downstream task performance, irrespective of the objectives and data modalities
used to train these models. Representational similarity is generally measured
for individual datasets and is not necessarily consistent across datasets.
Thus, one may wonder whether this convergence of model representations is
confounded by the datasets commonly used in machine learning. Here, we propose
a systematic way to measure how representational similarity between models
varies with the set of stimuli used to construct the representations. We find
that the objective function is the most crucial factor in determining the
consistency of representational similarities across datasets. Specifically,
self-supervised vision models learn representations whose relative pairwise
similarities generalize better from one dataset to another compared to those of
image classification or image-text models. Moreover, the correspondence between
representational similarities and the models' task behavior is
dataset-dependent, being most strongly pronounced for single-domain datasets.
Our work provides a framework for systematically measuring similarities of
model representations across datasets and linking those similarities to
differences in task behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards <span class="highlight-title">Lifelong</span> Few-Shot Customization of Text-to-Image <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Song, Xiaofeng Yang, Ze Yang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong few-shot customization for text-to-image diffusion aims to
continually generalize existing models for new tasks with minimal data while
preserving old knowledge. Current customization diffusion models excel in
few-shot tasks but struggle with catastrophic forgetting problems in lifelong
generations. In this study, we identify and categorize the catastrophic
forgetting problems into two folds: relevant concepts forgetting and previous
concepts forgetting. To address these challenges, we first devise a data-free
knowledge distillation strategy to tackle relevant concepts forgetting. Unlike
existing methods that rely on additional real data or offline replay of
original concept data, our approach enables on-the-fly knowledge distillation
to retain the previous concepts while learning new ones, without accessing any
previous data. Second, we develop an In-Context Generation (ICGen) paradigm
that allows the diffusion model to be conditioned upon the input vision
context, which facilitates the few-shot generation and mitigates the issue of
previous concepts forgetting. Extensive experiments show that the proposed
Lifelong Few-Shot Diffusion (LFS-Diffusion) method can produce high-quality and
accurate images while maintaining previously learned knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Active Flow Control Strategies Through Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricard Montalà, Bernat Font, Pol Suárez, Jean Rabault, Oriol Lehmkuhl, Ivette Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a deep reinforcement learning (DRL) framework for active
flow control (AFC) to reduce drag in aerodynamic bodies. Tested on a 3D
cylinder at Re = 100, the DRL approach achieved a 9.32% drag reduction and a
78.4% decrease in lift oscillations by learning advanced actuation strategies.
The methodology integrates a CFD solver with a DRL model using an in-memory
database for efficient communication between
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECOMMAS 2024 conference proceeding paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGGP: Fixed-Rate Gradient-First Gradual Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingkai Zhu, Can Deniz Bezek, Orcun Goksel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the increasing size of deep learning models and their
growing demand for computational resources have drawn significant attention to
the practice of pruning neural networks, while aiming to preserve their
accuracy. In unstructured gradual pruning, which sparsifies a network by
gradually removing individual network parameters until a targeted network
sparsity is reached, recent works show that both gradient and weight magnitudes
should be considered. In this work, we show that such mechanism, e.g., the
order of prioritization and selection criteria, is essential. We introduce a
gradient-first magnitude-next strategy for choosing the parameters to prune,
and show that a fixed-rate subselection criterion between these steps works
better, in contrast to the annealing approach in the literature. We validate
this on CIFAR-10 dataset, with multiple randomized initializations on both
VGG-19 and ResNet-50 network backbones, for pruning targets of 90, 95, and 98%
sparsity and for both initially dense and 50% sparse networks. Our proposed
fixed-rate gradient-first gradual pruning (FGGP) approach outperforms its
state-of-the-art alternatives in most of the above experimental settings, even
occasionally surpassing the upperbound of corresponding dense network results,
and having the highest ranking across the considered experimental settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Histopathological Foundation Models Eliminate Batch Effects? A
  Comparative Study <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonah Kömen, Hannah Marienwald, Jonas Dippel, Julius Hense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has led to remarkable advancements in computational
histopathology, e.g., in diagnostics, biomarker prediction, and outcome
prognosis. Yet, the lack of annotated data and the impact of batch effects,
e.g., systematic technical data differences across hospitals, hamper model
robustness and generalization. Recent histopathological foundation models --
pretrained on millions to billions of images -- have been reported to improve
generalization performances on various downstream tasks. However, it has not
been systematically assessed whether they fully eliminate batch effects. In
this study, we empirically show that the feature embeddings of the foundation
models still contain distinct hospital signatures that can lead to biased
predictions and misclassifications. We further find that the signatures are not
removed by stain normalization methods, dominate distances in feature space,
and are evident across various principal components. Our work provides a novel
perspective on the evaluation of medical foundation models, paving the way for
more robust pretraining strategies and downstream predictors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AIM-FM Workshop @ NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling geometrical variability in nonlinear reduced order modeling
  through Continuous Geometry-Aware DL-ROMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Brivio, Stefania Fresca, Andrea Manzoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning-based Reduced Order Models (DL-ROMs) provide nowadays a
well-established class of accurate surrogate models for complex physical
systems described by parametrized PDEs, by nonlinearly compressing the solution
manifold into a handful of latent coordinates. Until now, design and
application of DL-ROMs mainly focused on physically parameterized problems.
Within this work, we provide a novel extension of these architectures to
problems featuring geometrical variability and parametrized domains, namely, we
propose Continuous Geometry-Aware DL-ROMs (CGA-DL-ROMs). In particular, the
space-continuous nature of the proposed architecture matches the need to deal
with multi-resolution datasets, which are quite common in the case of
geometrically parametrized problems. Moreover, CGA-DL-ROMs are endowed with a
strong inductive bias that makes them aware of geometrical parametrizations,
thus enhancing both the compression capability and the overall performance of
the architecture. Within this work, we justify our findings through a thorough
theoretical analysis, and we practically validate our claims by means of a
series of numerical tests encompassing physically-and-geometrically
parametrized PDEs, ranging from the unsteady Navier-Stokes equations for fluid
dynamics to advection-diffusion-reaction equations for mathematical biology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Limits of Differential Privacy in Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Wei Wang, Peng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) is a formal notion that restricts the privacy
leakage of an algorithm when running on sensitive data, in which
privacy-utility trade-off is one of the central problems in private data
analysis. In this work, we investigate the fundamental limits of differential
privacy in online learning algorithms and present evidence that separates three
types of constraints: no DP, pure DP, and approximate DP. We first describe a
hypothesis class that is online learnable under approximate DP but not online
learnable under pure DP under the adaptive adversarial setting. This indicates
that approximate DP must be adopted when dealing with adaptive adversaries. We
then prove that any private online learner must make an infinite number of
mistakes for almost all hypothesis classes. This essentially generalizes
previous results and shows a strong separation between private and non-private
settings since a finite mistake bound is always attainable (as long as the
class is online learnable) when there is no privacy requirement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap between Learning and Inference for <span class="highlight-title">Diffusion</span>-Based
  Molecule Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peidong Liu, Wenbo Zhang, Xue Zhe, Jiancheng Lv, Xianggen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of diffusion models in generating a spectrum of data modalities,
including images, text, and videos, has spurred inquiries into their utility in
molecular generation, yielding significant advancements in the field. However,
the molecular generation process with diffusion models involves multiple
autoregressive steps over a finite time horizon, leading to exposure bias
issues inherently. To address the exposure bias issue, we propose a training
framework named GapDiff. The core idea of GapDiff is to utilize model-predicted
conformations as ground truth probabilistically during training, aiming to
mitigate the data distributional disparity between training and inference,
thereby enhancing the affinity of generated molecules. We conduct experiments
using a 3D molecular generation model on the CrossDocked2020 dataset, and the
vina energy and diversity demonstrate the potency of our framework with
superior affinity. GapDiff is available at
\url{https://github.com/HUGHNew/gapdiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization, Expressivity, and Universality of Graph Neural Networks
  on Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levi Rauchwerger, Stefanie Jegelka, Ron Levie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the universality and generalization of graph neural networks
(GNNs) on attributed graphs, i.e., with node attributes. To this end, we
propose pseudometrics over the space of all attributed graphs that describe the
fine-grained expressivity of GNNs. Namely, GNNs are both Lipschitz continuous
with respect to our pseudometrics and can separate attributed graphs that are
distant in the metric. Moreover, we prove that the space of all attributed
graphs is relatively compact with respect to our metrics. Based on these
properties, we prove a universal approximation theorem for GNNs and
generalization bounds for GNNs on any data distribution of attributed graphs.
The proposed metrics compute the similarity between the structures of
attributed graphs via a hierarchical optimal transport between computation
trees. Our work extends and unites previous approaches which either derived
theory only for graphs with no attributes, derived compact metrics under which
GNNs are continuous but without separation power, or derived metrics under
which GNNs are continuous and separate points but the space of graphs is not
relatively compact, which prevents universal approximation and generalization
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Study of Probabilistic Atlas and Deep Learning Approaches
  for Automatic Brain Tissue Segmentation from MRI Using N4 Bias Field
  Correction and Anisotropic <span class="highlight-title">Diffusion</span> Pre-processing Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Imran Hossain, Muhammad Zain Amin, Daniel Tweneboah Anyimadu, Taofik Ahmed Suleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic brain tissue segmentation from Magnetic Resonance Imaging (MRI)
images is vital for accurate diagnosis and further analysis in medical imaging.
Despite advancements in segmentation techniques, a comprehensive comparison
between traditional statistical methods and modern deep learning approaches
using pre-processing techniques like N4 Bias Field Correction and Anisotropic
Diffusion remains underexplored. This study provides a comparative analysis of
various segmentation models, including Probabilistic ATLAS, U-Net, nnU-Net, and
LinkNet, enhanced with these pre-processing techniques to segment brain tissues
(white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF)) on the
Internet Brain Segmentation Repository (IBSR18) dataset. Our results
demonstrate that the 3D nnU-Net model outperforms others, achieving the highest
mean Dice Coefficient score (0.937 +- 0.012), while the 2D nnU-Net model
recorded the lowest mean Hausdorff Distance (5.005 +- 0.343 mm) and the lowest
mean Absolute Volumetric Difference (3.695 +- 2.931 mm) across five unseen test
samples. The findings highlight the superiority of nnU-Net models in brain
tissue segmentation, particularly when combined with N4 Bias Field Correction
and Anisotropic Diffusion pre-processing techniques. Our implemented code can
be accessed via GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The sampling complexity of learning invertible residual neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Li, Philipp Grohs, Philipp Petersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent work it has been shown that determining a feedforward ReLU neural
network to within high uniform accuracy from point samples suffers from the
curse of dimensionality in terms of the number of samples needed. As a
consequence, feedforward ReLU neural networks are of limited use for
applications where guaranteed high uniform accuracy is required.
  We consider the question of whether the sampling complexity can be improved
by restricting the specific neural network architecture. To this end, we
investigate invertible residual neural networks which are foundational
architectures in deep learning and are widely employed in models that power
modern generative methods. Our main result shows that the residual neural
network architecture and invertibility do not help overcome the complexity
barriers encountered with simpler feedforward architectures. Specifically, we
demonstrate that the computational complexity of approximating invertible
residual neural networks from point samples in the uniform norm suffers from
the curse of dimensionality. Similar results are established for invertible
convolutional Residual neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICE-T: A Multi-Faceted Concept for Teaching Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Krone, Pierre Haritz, Thomas Liebig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The topics of Artificial intelligence (AI) and especially Machine Learning
(ML) are increasingly making their way into educational curricula. To
facilitate the access for students, a variety of platforms, visual tools, and
digital games are already being used to introduce ML concepts and strengthen
the understanding of how AI works. We take a look at didactic principles that
are employed for teaching computer science, define criteria, and, based on
those, evaluate a selection of prominent existing platforms, tools, and games.
Additionally, we criticize the approach of portraying ML mostly as a black-box
and the resulting missing focus on creating an understanding of data,
algorithms, and models that come with it. To tackle this issue, we present a
concept that covers intermodal transfer, computational and explanatory
thinking, ICE-T, as an extension of known didactic principles. With our
multi-faceted concept, we believe that planners of learning units, creators of
learning platforms and educators can improve on teaching ML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at the 17th International Conference on
  Informatics in Schools (ISSEP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeatherGFM: Learning A Weather Generalist Foundation Model via
  In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Zhiwang Zhou, Wenlong Zhang, Yihao Liu, Xiangyu Chen, Junchao Gong, Hao Chen, Ben Fei, Shiqi Chen, Wanli Ouyang, Xiao-Ming Wu, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Earth's weather system encompasses intricate weather data modalities and
diverse weather understanding tasks, which hold significant value to human
life. Existing data-driven models focus on single weather understanding tasks
(e.g., weather forecasting). Although these models have achieved promising
results, they fail to tackle various complex tasks within a single and unified
model. Moreover, the paradigm that relies on limited real observations for a
single scenario hinders the model's performance upper bound. In response to
these limitations, we draw inspiration from the in-context learning paradigm
employed in state-of-the-art visual foundation models and large language
models. In this paper, we introduce the first generalist weather foundation
model (WeatherGFM), designed to address a wide spectrum of weather
understanding tasks in a unified manner. More specifically, we initially unify
the representation and definition of the diverse weather understanding tasks.
Subsequently, we devised weather prompt formats to manage different weather
data modalities, namely single, multiple, and temporal modalities. Finally, we
adopt a visual prompting question-answering paradigm for the training of
unified weather understanding tasks. Extensive experiments indicate that our
WeatherGFM can effectively handle up to ten weather understanding tasks,
including weather forecasting, super-resolution, weather image translation, and
post-processing. Our method also showcases generalization ability on unseen
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-Hoc Robustness Enhancement in Graph Neural Networks with
  Conditional Random Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine Abbahaddou, Sofiane Ennadir, Johannes F. Lutzeyer, Fragkiskos D. Malliaros, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs), which are nowadays the benchmark approach in
graph representation learning, have been shown to be vulnerable to adversarial
attacks, raising concerns about their real-world applicability. While existing
defense techniques primarily concentrate on the training phase of GNNs,
involving adjustments to message passing architectures or pre-processing
methods, there is a noticeable gap in methods focusing on increasing robustness
during inference. In this context, this study introduces RobustCRF, a post-hoc
approach aiming to enhance the robustness of GNNs at the inference stage. Our
proposed method, founded on statistical relational learning using a Conditional
Random Field, is model-agnostic and does not require prior knowledge about the
underlying model architecture. We validate the efficacy of this approach across
various models, leveraging benchmark node classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Meteorological <span class="highlight-title">Forecast</span>ing: AI-based Approach to Synoptic
  Weather Map Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yo-Hwan Choi, Seon-Yu Kang, Minjong Cheon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As global warming increases the complexity of weather patterns; the precision
of weather forecasting becomes increasingly important. Our study proposes a
novel preprocessing method and convolutional autoencoder model developed to
improve the interpretation of synoptic weather maps. These are critical for
meteorologists seeking a thorough understanding of weather conditions. This
model could recognize historical synoptic weather maps that nearly match
current atmospheric conditions, marking a significant step forward in modern
technology in meteorological forecasting. This comprises unsupervised learning
models like VQ-VQE, as well as supervised learning models like VGG16, VGG19,
Xception, InceptionV3, and ResNet50 trained on the ImageNet dataset, as well as
research into newer models like EfficientNet and ConvNeXt. Our findings proved
that, while these models perform well in various settings, their ability to
identify comparable synoptic weather maps has certain limits. Our research,
motivated by the primary goal of significantly increasing meteorologists'
efficiency in labor-intensive tasks, discovered that cosine similarity is the
most effective metric, as determined by a combination of quantitative and
qualitative assessments to accurately identify relevant historical weather
patterns. This study broadens our understanding by shifting the emphasis from
numerical precision to practical application, ensuring that our model is
effective in theory practical, and accessible in the complex and dynamic field
of meteorology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning for prediction of dose-volume histograms of
  organs-at-risk in prostate cancer from simple structure volume parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saheli Saha, Debasmita Banerjee, Rishi Ram, Gowtham Reddy, Debashree Guha, Arnab Sarkar, Bapi Dutta, Moses ArunSingh S, Suman Chakraborty, Indranil Mallick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dose prediction is an area of ongoing research that facilitates radiotherapy
planning. Most commercial models utilise imaging data and intense computing
resources. This study aimed to predict the dose-volume of rectum and bladder
from volumes of target, at-risk structure organs and their overlap regions
using machine learning. Dose-volume information of 94 patients with prostate
cancer planned for 6000cGy in 20 fractions was exported from the treatment
planning system as text files and mined to create a training dataset. Several
statistical modelling, machine learning methods, and a new fuzzy rule-based
prediction (FRBP) model were explored and validated on an independent dataset
of 39 patients. The median absolute error was 2.0%-3.7% for bladder and
1.7-2.4% for rectum in the 4000-6420cGy range. For 5300cGy, 5600cGy and
6000cGy, the median difference was less than 2.5% for rectum and 3.8% for
bladder. The FRBP model produced errors of 1.2%, 1.3%, 0.9% and 1.6%, 1.2%,
0.1% for the rectum and bladder respectively at these dose levels. These
findings indicate feasibility of obtaining accurate predictions of the
clinically important dose-volume parameters for rectum and bladder using just
the volumes of these structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Grokking with Nonlinearity and Data Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Salah, David Yevick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates that grokking behavior in modular arithmetic with a
modulus P in a neural network can be controlled by modifying the profile of the
activation function as well as the depth and width of the model. Plotting the
even PCA projections of the weights of the last NN layer against their odd
projections further yields patterns which become significantly more uniform
when the nonlinearity is increased by incrementing the number of layers. These
patterns can be employed to factor P when P is nonprime. Finally, a metric for
the generalization ability of the network is inferred from the entropy of the
layer weights while the degree of nonlinearity is related to correlations
between the local entropy of the weights of the neurons in the final layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RED: Residual Estimation <span class="highlight-title">Diffusion</span> for Low-Dose PET Sinogram
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ai, Bin Huang, Fang Chen, Liu Shi, Binxuan Li, Shaoyu Wang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have demonstrated exceptional performance
in generative tasks across vari-ous fields. In positron emission tomography
(PET), the reduction in tracer dose leads to information loss in sino-grams.
Using diffusion models to reconstruct missing in-formation can improve imaging
quality. Traditional diffu-sion models effectively use Gaussian noise for image
re-constructions. However, in low-dose PET reconstruction, Gaussian noise can
worsen the already sparse data by introducing artifacts and inconsistencies. To
address this issue, we propose a diffusion model named residual esti-mation
diffusion (RED). From the perspective of diffusion mechanism, RED uses the
residual between sinograms to replace Gaussian noise in diffusion process,
respectively sets the low-dose and full-dose sinograms as the starting point
and endpoint of reconstruction. This mechanism helps preserve the original
information in the low-dose sinogram, thereby enhancing reconstruction
reliability. From the perspective of data consistency, RED introduces a drift
correction strategy to reduce accumulated prediction errors during the reverse
process. Calibrating the inter-mediate results of reverse iterations helps
maintain the data consistency and enhances the stability of reconstruc-tion
process. Experimental results show that RED effec-tively improves the quality
of low-dose sinograms as well as the reconstruction results. The code is
available at: https://github.com/yqx7150/RED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Adaptive Resource Scheduling in Complex
  System Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pochun Li, Yuyang Xiao, Jinghua Yan, Xuan Li, Xiaoye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel computer system performance optimization and
adaptive workload management scheduling algorithm based on Q-learning. In
modern computing environments, characterized by increasing data volumes, task
complexity, and dynamic workloads, traditional static scheduling methods such
as Round-Robin and Priority Scheduling fail to meet the demands of efficient
resource allocation and real-time adaptability. By contrast, Q-learning, a
reinforcement learning algorithm, continuously learns from system state
changes, enabling dynamic scheduling and resource optimization. Through
extensive experiments, the superiority of the proposed approach is demonstrated
in both task completion time and resource utilization, outperforming
traditional and dynamic resource allocation (DRA) algorithms. These findings
are critical as they highlight the potential of intelligent scheduling
algorithms based on reinforcement learning to address the growing complexity
and unpredictability of computing environments. This research provides a
foundation for the integration of AI-driven adaptive scheduling in future
large-scale systems, offering a scalable, intelligent solution to enhance
system performance, reduce operating costs, and support sustainable energy
consumption. The broad applicability of this approach makes it a promising
candidate for next-generation computing frameworks, such as edge computing,
cloud computing, and the Internet of Things.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quality-Centric Framework for Generic Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentang Song, Zhiyuan Yan, Yuzhen Lin, Taiping Yao, Changsheng Chen, Shen Chen, Yandan Zhao, Shouhong Ding, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the generalization issue in deepfake detection by
harnessing forgery quality in training data. Generally, the forgery quality of
different deepfakes varies: some have easily recognizable forgery clues, while
others are highly realistic. Existing works often train detectors on a mix of
deepfakes with varying forgery qualities, potentially leading detectors to
short-cut the easy-to-spot artifacts from low-quality forgery samples, thereby
hurting generalization performance. To tackle this issue, we propose a novel
quality-centric framework for generic deepfake detection, which is composed of
a Quality Evaluator, a low-quality data enhancement module, and a learning
pacing strategy that explicitly incorporates forgery quality into the training
process. The framework is inspired by curriculum learning, which is designed to
gradually enable the detector to learn more challenging deepfake samples,
starting with easier samples and progressing to more realistic ones. We employ
both static and dynamic assessments to assess the forgery quality, combining
their scores to produce a final rating for each training sample. The rating
score guides the selection of deepfake samples for training, with higher-rated
samples having a higher probability of being chosen. Furthermore, we propose a
novel frequency data augmentation method specifically designed for low-quality
forgery samples, which helps to reduce obvious forgery traces and improve their
overall realism. Extensive experiments show that our method can be applied in a
plug-and-play manner and significantly enhance the generalization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Latent Structural Causal Models from Spatio-<span class="highlight-title">Temporal</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wang, Sumanth Varambally, Duncan Watson-Parris, Yi-An Ma, Rose Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many important phenomena in scientific fields such as climate, neuroscience,
and epidemiology are naturally represented as spatiotemporal gridded data with
complex interactions. For example, in climate science, researchers aim to
uncover how large-scale events, such as the North Atlantic Oscillation (NAO)
and the Antarctic Oscillation (AAO), influence other global processes.
Inferring causal relationships from these data is a challenging problem
compounded by the high dimensionality of such data and the correlations between
spatially proximate points. We present SPACY (SPAtiotemporal Causal discoverY),
a novel framework based on variational inference, designed to explicitly model
latent time-series and their causal relationships from spatially confined modes
in the data. Our method uses an end-to-end training process that maximizes an
evidence-lower bound (ELBO) for the data likelihood. Theoretically, we show
that, under some conditions, the latent variables are identifiable up to
transformation by an invertible matrix. Empirically, we show that SPACY
outperforms state-of-the-art baselines on synthetic data, remains scalable for
large grids, and identifies key known phenomena from real-world climate data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inversion-based Latent Bayesian Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewon Chu, Jinyoung Park, Seunghun Lee, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent Bayesian optimization (LBO) approaches have successfully adopted
Bayesian optimization over a continuous latent space by employing an
encoder-decoder architecture to address the challenge of optimization in a high
dimensional or discrete input space. LBO learns a surrogate model to
approximate the black-box objective function in the latent space. However, we
observed that most LBO methods suffer from the `misalignment problem`, which is
induced by the reconstruction error of the encoder-decoder architecture. It
hinders learning an accurate surrogate model and generating high-quality
solutions. In addition, several trust region-based LBO methods select the
anchor, the center of the trust region, based solely on the objective function
value without considering the trust region`s potential to enhance the
optimization process. To address these issues, we propose Inversion-based
Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO
consists of two components: an inversion method and a potential-aware trust
region anchor selection. The inversion method searches the latent code that
completely reconstructs the given target data. The potential-aware trust region
anchor selection considers the potential capability of the trust region for
better local optimization. Experimental results demonstrate the effectiveness
of InvBO on nine real-world benchmarks, such as molecule design and arithmetic
expression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SASWISE-UE: Segmentation and Synthesis with Interpretable Scalable
  Ensembles for Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Chen, Alan McMillan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an efficient sub-model ensemble framework aimed at
enhancing the interpretability of medical deep learning models, thus increasing
their clinical applicability. By generating uncertainty maps, this framework
enables end-users to evaluate the reliability of model outputs. We developed a
strategy to develop diverse models from a single well-trained checkpoint,
facilitating the training of a model family. This involves producing multiple
outputs from a single input, fusing them into a final output, and estimating
uncertainty based on output disagreements. Implemented using U-Net and UNETR
models for segmentation and synthesis tasks, this approach was tested on CT
body segmentation and MR-CT synthesis datasets. It achieved a mean Dice
coefficient of 0.814 in segmentation and a Mean Absolute Error of 88.17 HU in
synthesis, improved from 89.43 HU by pruning. Additionally, the framework was
evaluated under corruption and undersampling, maintaining correlation between
uncertainty and error, which highlights its robustness. These results suggest
that the proposed approach not only maintains the performance of well-trained
models but also enhances interpretability through effective uncertainty
estimation, applicable to both convolutional and transformer models in a range
of imaging tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness in Monotone $k$-submodular Maximization: Algorithms and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhui Zhu, Samik Basu, A. Pavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Submodular optimization has become increasingly prominent in machine learning
and fairness has drawn much attention. In this paper, we propose to study the
fair $k$-submodular maximization problem and develop a
$\frac{1}{3}$-approximation greedy algorithm with a running time of
$\mathcal{O}(knB)$. To the best of our knowledge, our work is the first to
incorporate fairness in the context of $k$-submodular maximization, and our
theoretical guarantee matches the best-known $k$-submodular maximization
results without fairness constraints. In addition, we have developed a faster
threshold-based algorithm that achieves a $(\frac{1}{3} - \epsilon)$
approximation with $\mathcal{O}(\frac{kn}{\epsilon} \log \frac{B}{\epsilon})$
evaluations of the function $f$. Furthermore, for both algorithms, we provide
approximation guarantees when the $k$-submodular function is not accessible but
only can be approximately accessed. We have extensively validated our
theoretical findings through empirical research and examined the practical
implications of fairness. Specifically, we have addressed the question: ``What
is the price of fairness?" through case studies on influence maximization with
$k$ topics and sensor placement with $k$ types. The experimental results show
that the fairness constraints do not significantly undermine the quality of
solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages. To appear in IEEE BigData 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Alignment Landscape: <span class="highlight-title">LLM</span>s and Geometric Deep Models in
  Protein Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Bingbing Duan, Kai Guo, Kaixiong Zhou, Jiliang Tang, Mengnan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent representation alignment has become a foundational technique for
constructing multimodal large language models (MLLM) by mapping embeddings from
different modalities into a shared space, often aligned with the embedding
space of large language models (LLMs) to enable effective cross-modal
understanding. While preliminary protein-focused MLLMs have emerged, they have
predominantly relied on heuristic approaches, lacking a fundamental
understanding of optimal alignment practices across representations. In this
study, we explore the alignment of multimodal representations between LLMs and
Geometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate
three state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with
four protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines
alignment factors from both model and protein perspectives, identifying
challenges in current alignment methodologies and proposing strategies to
improve the alignment process. Our key findings reveal that GDMs incorporating
both graph and 3D structural information align better with LLMs, larger LLMs
demonstrate improved alignment capabilities, and protein rarity significantly
impacts alignment performance. We also find that increasing GDM embedding
dimensions, using two-layer projection heads, and fine-tuning LLMs on
protein-specific data substantially enhance alignment quality. These strategies
offer potential enhancements to the performance of protein-related multimodal
models. Our code and data are available at
https://github.com/Tizzzzy/LLM-GDM-alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Calibration of Inexact Stochastic Simulation Models via
  Kernel Score Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Su, Diego Klabjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic simulation models are generative models that mimic complex systems
to help with decision-making. The reliability of these models heavily depends
on well-calibrated input model parameters. However, in many practical
scenarios, only output-level data are available to learn the input model
parameters, which is challenging due to the often intractable likelihood of the
stochastic simulation model. Moreover, stochastic simulation models are
frequently inexact, with discrepancies between the model and the target system.
No existing methods can effectively learn and quantify the uncertainties of
input parameters using only output-level data. In this paper, we propose to
learn differentiable input parameters of stochastic simulation models using
output-level data via kernel score minimization with stochastic gradient
descent. We quantify the uncertainties of the learned input parameters using a
frequentist confidence set procedure based on a new asymptotic normality result
that accounts for model inexactness. The proposed method is evaluated on exact
and inexact G/G/1 queueing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 12 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Training of Kolmogorov-Arnold Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shairoz Sohail
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks have recently been introduced as a flexible
alternative to multi-layer Perceptron architectures. In this paper, we examine
the training dynamics of different KAN architectures and compare them with
corresponding MLP formulations. We train with a variety of different
initialization schemes, optimizers, and learning rates, as well as utilize back
propagation free approaches like the HSIC Bottleneck. We find that (when judged
by test accuracy) KANs are an effective alternative to MLP architectures on
high-dimensional datasets and have somewhat better parameter efficiency, but
suffer from more unstable training dynamics. Finally, we provide
recommendations for improving training stability of larger KAN models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MicroScopiQ: Accelerating Foundational Models through Outlier-Aware
  Microscaling Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Ramachandran, Souvik Kundu, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization of foundational models (FMs) is significantly more challenging
than traditional DNNs due to the emergence of large magnitude features called
outliers. Existing outlier-aware algorithm/architecture co-design techniques
either use mixed-precision, retaining outliers at high precision but compromise
hardware efficiency, or quantize inliers and outliers at the same precision,
improving hardware efficiency at the cost of accuracy. To address this mutual
exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique
that leverages pruning to complement outlier-aware quantization. MicroScopiQ
retains outliers at higher precision while pruning a certain fraction of least
important weights to distribute the additional outlier bits; ensuring high
accuracy, aligned memory and hardware efficiency. We design a high-throughput,
low overhead accelerator architecture composed of simple multi-precision INT
processing elements and a novel network-on-chip called ReCoN that efficiently
abstracts the complexity of supporting high-precision outliers. Additionally,
unlike existing alternatives, MicroScopiQ does not assume any locality of
outlier weights, enabling applicability to a broad range of FMs. Extensive
experiments across various quantization settings show that MicroScopiQ achieves
SoTA quantization performance while simultaneously improving inference
performance by 3x and reducing energy by 2x over existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fox-1 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Hu, Jipeng Zhang, Rui Pan, Zhaozhuo Xu, Salman Avestimehr, Chaoyang He, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Fox-1, a series of small language models (SLMs) consisting of
Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3
trillion tokens of web-scraped document data and fine-tuned with 5 billion
tokens of instruction-following and multi-turn conversation data. Aiming to
improve the pre-training efficiency, Fox-1-1.6B model introduces a novel
3-stage data curriculum across all the training data with 2K-8K sequence
length. In architecture design, Fox-1 features a deeper layer structure, an
expanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a
performant and efficient architecture compared to other SLMs. Fox-1 achieves
better or on-par performance in various benchmarks compared to StableLM-2-1.6B,
Gemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and
throughput. The model weights have been released under the Apache 2.0 license,
where we aim to promote the democratization of LLMs and make them fully
accessible to the whole open-source community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Base model is available at
  https://huggingface.co/tensoropera/Fox-1-1.6B and the instruction-tuned
  version is available at
  https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Robustness of Watermarking to Paraphrasing Attacks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saksham Rastogi, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst rising concerns about the internet being proliferated with content
generated from language models (LMs), watermarking is seen as a principled way
to certify whether text was generated from a model. Many recent watermarking
techniques slightly modify the output probabilities of LMs to embed a signal in
the generated output that can later be detected. Since early proposals for text
watermarking, questions about their robustness to paraphrasing have been
prominently discussed. Lately, some techniques are deliberately designed and
claimed to be robust to paraphrasing. However, such watermarking schemes do not
adequately account for the ease with which they can be reverse-engineered. We
show that with access to only a limited number of generations from a black-box
watermarked model, we can drastically increase the effectiveness of
paraphrasing attacks to evade watermark detection, thereby rendering the
watermark ineffective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span> Semantic Cache: Reducing <span class="highlight-title">LLM</span> Costs and Latency via Semantic
  Embedding Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajal Regmi, Chetan Phakami Pun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as GPT (Radford et al., 2019), have
significantly advanced artificial intelligence by enabling sophisticated
natural language understanding and generation. However, the high computational
and financial costs associated with frequent API calls to these models present
a substantial bottleneck, especially for applications like customer service
chatbots that handle repetitive queries. In this paper, we introduce GPT
Semantic Cache, a method that leverages semantic caching of query embeddings in
in-memory storage (Redis). By storing embeddings of user queries, our approach
efficiently identifies semantically similar questions, allowing for the
retrieval of pre-generated responses without redundant API calls to the LLM.
This technique reduces operational costs and improves response times, enhancing
the efficiency of LLM-powered applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed-Order Fractional Graph Operating Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhao, Xuhao Li, Qiyu Kang, Feng Ji, Qinxu Ding, Yanan Zhao, Wenfei Liang, Wee Peng Tay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Distributed-order fRActional Graph Operating Network
(DRAGON), a novel continuous Graph Neural Network (GNN) framework that
incorporates distributed-order fractional calculus. Unlike traditional
continuous GNNs that utilize integer-order or single fractional-order
differential equations, DRAGON uses a learnable probability distribution over a
range of real numbers for the derivative orders. By allowing a flexible and
learnable superposition of multiple derivative orders, our framework captures
complex graph feature updating dynamics beyond the reach of conventional
models. We provide a comprehensive interpretation of our framework's capability
to capture intricate dynamics through the lens of a non-Markovian graph random
walk with node feature updating driven by an anomalous diffusion process over
the graph. Furthermore, to highlight the versatility of the DRAGON framework,
we conduct empirical evaluations across a range of graph learning tasks. The
results consistently demonstrate superior performance when compared to
traditional continuous GNN models. The implementation code is available at
\url{https://github.com/zknus/NeurIPS-2024-DRAGON}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-World Offline Reinforcement Learning from Vision <span class="highlight-title">Language Model</span>
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyas Venkataraman, Yufei Wang, Ziyu Wang, Zackory Erickson, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning can enable policy learning from pre-collected,
sub-optimal datasets without online interactions. This makes it ideal for
real-world robots and safety-critical scenarios, where collecting online data
or expert demonstrations is slow, costly, and risky. However, most existing
offline RL works assume the dataset is already labeled with the task rewards, a
process that often requires significant human effort, especially when
ground-truth states are hard to ascertain (e.g., in the real-world). In this
paper, we build on prior work, specifically RL-VLM-F, and propose a novel
system that automatically generates reward labels for offline datasets using
preference feedback from a vision-language model and a text description of the
task. Our method then learns a policy using offline RL with the reward-labeled
dataset. We demonstrate the system's applicability to a complex real-world
robot-assisted dressing task, where we first learn a reward function using a
vision-language model on a sub-optimal offline dataset, and then we use the
learned reward to employ Implicit Q learning to develop an effective dressing
policy. Our method also performs well in simulation tasks involving the
manipulation of rigid and deformable objects, and significantly outperform
baselines such as behavior cloning and inverse RL. In summary, we propose a new
system that enables automatic reward labeling and policy learning from
unlabeled, sub-optimal offline datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages. Accepted at the LangRob Workshop 2024 @ CoRL, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer-Net SCa-Synth: An Open Access Synthetically Generated 2D Skin
  Lesion <span class="highlight-title">Dataset</span> for Skin Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-en Amy Tai, Oustan Ding, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the United States, skin cancer ranks as the most commonly diagnosed
cancer, presenting a significant public health issue due to its high rates of
occurrence and the risk of serious complications if not caught early. Recent
advancements in dataset curation and deep learning have shown promise in quick
and accurate detection of skin cancer. However, current open-source datasets
have significant class imbalances which impedes the effectiveness of these deep
learning models. In healthcare, generative artificial intelligence (AI) models
have been employed to create synthetic data, addressing data imbalance in
datasets by augmenting underrepresented classes and enhancing the overall
quality and performance of machine learning models. In this paper, we build on
top of previous work by leveraging new advancements in generative AI, notably
Stable Diffusion and DreamBooth. We introduce Cancer-Net SCa-Synth, an open
access synthetically generated 2D skin lesion dataset for skin cancer
classification. Further analysis on the data effectiveness by comparing the
ISIC 2020 test set performance for training with and without these synthetic
images for a simple model highlights the benefits of leveraging synthetic data
to improve performance. Cancer-Net SCa-Synth is publicly available at
https://github.com/catai9/Cancer-Net-SCa-Synth as part of a global open-source
initiative for accelerating machine learning for cancer care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Chat<span class="highlight-title">GPT</span> Transforming Academics' Writing Style? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on one million arXiv papers submitted from May 2018 to January 2024, we
assess the textual density of ChatGPT's writing style in their abstracts
through a statistical analysis of word frequency changes. Our model is
calibrated and validated on a mixture of real abstracts and ChatGPT-modified
abstracts (simulated data) after a careful noise analysis. The words used for
estimation are not fixed but adaptive, including those with decreasing
frequency. We find that large language models (LLMs), represented by ChatGPT,
are having an increasing impact on arXiv abstracts, especially in the field of
computer science, where the fraction of LLM-style abstracts is estimated to be
approximately 35%, if we take the responses of GPT-3.5 to one simple prompt,
"revise the following sentences", as a baseline. We conclude with an analysis
of both positive and negative aspects of the penetration of LLMs into
academics' writing style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logits of API-Protected <span class="highlight-title">LLM</span>s Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Xiang Ren, Swabha Swayamdipta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) providers often hide the architectural details and
parameters of their proprietary models by restricting public access to a
limited API. In this work we show that, with only a conservative assumption
about the model architecture, it is possible to learn a surprisingly large
amount of non-public information about an API-protected LLM from a relatively
small number of API queries (e.g., costing under $1000 USD for OpenAI's
gpt-3.5-turbo). Our findings are centered on one key observation: most modern
LLMs suffer from a softmax bottleneck, which restricts the model outputs to a
linear subspace of the full output space. We exploit this fact to unlock
several capabilities, including (but not limited to) obtaining cheap
full-vocabulary outputs, auditing for specific types of model updates,
identifying the source LLM given a single full LLM output, and even efficiently
discovering the LLM's hidden size. Our empirical investigations show the
effectiveness of our methods, which allow us to estimate the embedding size of
OpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM
providers can guard against these attacks, as well as how these capabilities
can be viewed as a feature (rather than a bug) by allowing for greater
transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly more
memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both
weights and activations are highly sensitive, where conventional post-training
quantization methods for large language models like smoothing become
insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit
quantization paradigm. Different from smoothing which redistributes outliers
between weights and activations, our approach absorbs these outliers using a
low-rank branch. We first consolidate the outliers by shifting them from
activations to weights, then employ a high-precision low-rank branch to take in
the weight outliers with Singular Value Decomposition (SVD). This process eases
the quantization on both sides. However, na\"{\i}vely running the low-rank
branch independently incurs significant overhead due to extra data movement of
activations, negating the quantization speedup. To address this, we co-design
an inference engine Nunchaku that fuses the kernels of the low-rank branch into
those of the low-bit branch to cut off redundant memory access. It can also
seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for
re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1
validate the effectiveness of SVDQuant in preserving image quality. We reduce
the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving
3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB
laptop 4090 GPU, paving the way for more interactive applications on PCs. Our
quantization library and inference engine are open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Quantization Library: https://github.com/mit-han-lab/deepcompressor
  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:
  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:
  https://hanlab.mit.edu/blog/svdquant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Aware Combinatorial Assembly Sequence Planning using Data-free
  Action Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial assembly uses standardized unit primitives to build objects
that satisfy user specifications. This paper studies assembly sequence planning
(ASP) for physical combinatorial assembly. Given the shape of the desired
object, the goal is to find a sequence of actions for placing unit primitives
to build the target object. In particular, we aim to ensure the planned
assembly sequence is physically executable. However, ASP for combinatorial
assembly is particularly challenging due to its combinatorial nature. To
address the challenge, we employ deep reinforcement learning to learn a
construction policy for placing unit primitives sequentially to build the
desired object. Specifically, we design an online physics-aware action mask
that filters out invalid actions, which effectively guides policy learning and
ensures violation-free deployment. In the end, we apply the proposed method to
Lego assembly with more than 250 3D structures. The experiment results
demonstrate that the proposed method plans physically valid assembly sequences
to build all structures, achieving a $100\%$ success rate, whereas the best
comparable baseline fails more than $40$ structures. Our implementation is
available at
\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-models for transfer learning in source localisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lawrence A. Bull, Matthew R. Jones, Elizabeth J. Cross, Andrew Duncan, Mark Girolami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In practice, non-destructive testing (NDT) procedures tend to consider
experiments (and their respective models) as distinct, conducted in isolation
and associated with independent data. In contrast, this work looks to capture
the interdependencies between acoustic emission (AE) experiments (as
meta-models) and then use the resulting functions to predict the model
hyperparameters for previously unobserved systems. We utilise a Bayesian
multilevel approach (similar to deep Gaussian Processes) where a higher level
meta-model captures the inter-task relationships. Our key contribution is how
knowledge of the experimental campaign can be encoded between tasks as well as
within tasks. We present an example of AE time-of-arrival mapping for source
localisation, to illustrate how multilevel models naturally lend themselves to
representing aggregate systems in engineering. We constrain the meta-model
based on domain knowledge, then use the inter-task functions for transfer
learning, predicting hyperparameters for models of previously unobserved
experiments (for a specific design).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization without Retraction on the Random Generalized Stiefel
  Manifold <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Vary, Pierre Ablin, Bin Gao, P. -A. Absil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization over the set of matrices $X$ that satisfy $X^\top B X = I_p$,
referred to as the generalized Stiefel manifold, appears in many applications
involving sampled covariance matrices such as the canonical correlation
analysis (CCA), independent component analysis (ICA), and the generalized
eigenvalue problem (GEVP). Solving these problems is typically done by
iterative methods that require a fully formed $B$. We propose a cheap
stochastic iterative method that solves the optimization problem while having
access only to random estimates of $B$. Our method does not enforce the
constraint in every iteration; instead, it produces iterations that converge to
critical points on the generalized Stiefel manifold defined in expectation. The
method has lower per-iteration cost, requires only matrix multiplications, and
has the same convergence rates as its Riemannian optimization counterparts that
require the full matrix $B$. Experiments demonstrate its effectiveness in
various machine learning applications involving generalized orthogonality
constraints, including CCA, ICA, and the GEVP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This v3 is a corrected version of the ICML 2024 paper (PMLR
  235:49226-49248); see the errata at the end</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness-Aware Estimation of Graphical Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Qi Long, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the issue of fairness in the estimation of graphical
models (GMs), particularly Gaussian, Covariance, and Ising models. These models
play a vital role in understanding complex relationships in high-dimensional
data. However, standard GMs can result in biased outcomes, especially when the
underlying data involves sensitive characteristics or protected groups. To
address this, we introduce a comprehensive framework designed to reduce bias in
the estimation of GMs related to protected attributes. Our approach involves
the integration of the pairwise graph disparity error and a tailored loss
function into a nonsmooth multi-objective optimization problem, striving to
achieve fairness across different sensitive groups while maintaining the
effectiveness of the GMs. Experimental evaluations on synthetic and real-world
datasets demonstrate that our framework effectively mitigates bias without
undermining GMs' performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at NeurIPS 2024, 34 Pages, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ log-RRIM: Yield Prediction via Local-to-global Reaction Representation
  Learning and Interaction Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Hu, Ziqi Chen, Bo Peng, Daniel Adu-Ampratwum, Xia Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of chemical reaction yields is crucial for optimizing
organic synthesis, potentially reducing time and resources spent on
experimentation. With the rise of artificial intelligence (AI), there is
growing interest in leveraging AI-based methods to accelerate yield predictions
without conducting in vitro experiments. We present log-RRIM, an innovative
graph transformer-based framework designed for predicting chemical reaction
yields. Our approach implements a unique local-to-global reaction
representation learning strategy. This approach initially captures detailed
molecule-level information and then models and aggregates intermolecular
interactions, ensuring that the impact of varying-sizes molecular fragments on
yield is accurately accounted for. Another key feature of log-RRIM is its
integration of a cross-attention mechanism that focuses on the interplay
between reagents and reaction centers. This design reflects a fundamental
principle in chemical reactions: the crucial role of reagents in influencing
bond-breaking and formation processes, which ultimately affect reaction yields.
log-RRIM outperforms existing methods in our experiments, especially for medium
to high-yielding reactions, proving its reliability as a predictor. Its
advanced modeling of reactant-reagent interactions and sensitivity to small
molecular fragments make it a valuable tool for reaction planning and
optimization in chemical synthesis. The data and codes of log-RRIM are
accessible through https://github.com/ninglab/Yield_log_RRIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xAI-Drop: Don't Use What You Cannot Explain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincenzo Marco De Luca, Antonio Longa, Andrea Passerini, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the predominant paradigm for
learning from graph-structured data, offering a wide range of applications from
social network analysis to bioinformatics. Despite their versatility, GNNs face
challenges such as lack of generalization and poor interpretability, which
hinder their wider adoption and reliability in critical applications. Dropping
has emerged as an effective paradigm for improving the generalization
capabilities of GNNs. However, existing approaches often rely on random or
heuristic-based selection criteria, lacking a principled method to identify and
exclude nodes that contribute to noise and over-complexity in the model. In
this work, we argue that explainability should be a key indicator of a model's
quality throughout its training phase. To this end, we introduce xAI-Drop, a
novel topological-level dropping regularizer that leverages explainability to
pinpoint noisy network elements to be excluded from the GNN propagation
mechanism. An empirical evaluation on diverse real-world datasets demonstrates
that our method outperforms current state-of-the-art dropping approaches in
accuracy, and improves explanation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Fairness by Combining Factual and Counterfactual
  Predictions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Zhou, Tianci Liu, Ruqi Bai, Jing Gao, Murat Kocaoglu, David I. Inouye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-stake domains such as healthcare and hiring, the role of machine
learning (ML) in decision-making raises significant fairness concerns. This
work focuses on Counterfactual Fairness (CF), which posits that an ML model's
outcome on any individual should remain unchanged if they had belonged to a
different demographic group. Previous works have proposed methods that
guarantee CF. Notwithstanding, their effects on the model's predictive
performance remains largely unclear. To fill in this gap, we provide a
theoretical study on the inherent trade-off between CF and predictive
performance in a model-agnostic manner. We first propose a simple but effective
method to cast an optimal but potentially unfair predictor into a fair one
without losing the optimality. By analyzing its excess risk in order to achieve
CF, we quantify this inherent trade-off. Further analysis on our method's
performance with access to only incomplete causal knowledge is also conducted.
Built upon it, we propose a performant algorithm that can be applied in such
scenarios. Experiments on both synthetic and semi-synthetic datasets
demonstrate the validity of our analysis and methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Human-like Representations to Enable Learning Human Values <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Wynn, Ilia Sucholutsky, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we build AI systems that can learn any set of individual human values
both quickly and safely, avoiding causing harm or violating societal standards
for acceptable behavior during the learning process? We explore the effects of
representational alignment between humans and AI agents on learning human
values. Making AI systems learn human-like representations of the world has
many known benefits, including improving generalization, robustness to domain
shifts, and few-shot learning performance. We demonstrate that this kind of
representational alignment can also support safely learning and exploring human
values in the context of personalization. We begin with a theoretical
prediction, show that it applies to learning human morality judgments, then
show that our results generalize to ten different aspects of human values --
including ethics, honesty, and fairness -- training AI agents on each set of
values in a multi-armed bandit setting, where rewards reflect human value
judgments over the chosen action. Using a set of textual action descriptions,
we collect value judgments from humans, as well as similarity judgments from
both humans and multiple language models, and demonstrate that representational
alignment enables both safe exploration and improved generalization when
learning human values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum
  annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Kaniewski, Fariba Yousefi, Yeman Brhane Hagos, Talha Qaiser, Nikolay Burlutskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In drug discovery, accurate lung tumor segmentation is an important step for
assessing tumor size and its progression using \textit{in-vivo} imaging such as
MRI. While deep learning models have been developed to automate this process,
the focus has predominantly been on human subjects, neglecting the pivotal role
of animal models in pre-clinical drug development. In this work, we focus on
optimizing lung tumor segmentation in mice. First, we demonstrate that the
nnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most
importantly, we achieve better results with nnU-Net 3D models than 2D models,
indicating the importance of spatial context for segmentation tasks in MRI mice
scans. This study demonstrates the importance of 3D input over 2D input images
for lung tumor segmentation in MRI scans. Finally, we outperform the prior
state-of-the-art approach that involves the combined segmentation of lungs and
tumors within the lungs. Our work achieves comparable results using only lung
tumor annotations requiring fewer annotations, saving time and annotation
efforts. This work
(https://anonymous.4open.science/r/lung-tumour-mice-mri-64BB) is an important
step in automating pre-clinical animal studies to quantify the efficacy of
experimental drugs, particularly in assessing tumor changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable and Stable Parallelization of Nonlinear RNNs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Gonzalez, Andrew Warrington, Jimmy T. H. Smith, Scott W. Linderman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional nonlinear RNNs are not naturally parallelizable across the
sequence length, unlike transformers and linear RNNs. Lim et. al. (2024)
therefore tackle parallelized evaluation of nonlinear RNNs, posing it as a
fixed point problem solved with Newton's method. By deriving and applying a
parallelized form of Newton's method, they achieve large speedups over
sequential evaluation. However, their approach inherits cubic computational
complexity and numerical instability. We tackle these weaknesses. To reduce the
computational complexity, we apply quasi-Newton approximations and show they
converge comparably, use less memory, and are faster, compared to full-Newton.
To stabilize Newton's method, we leverage a connection between Newton's method
damped with trust regions and Kalman smoothing. This connection allows us to
stabilize the iteration, per the trust region, and use efficient parallelized
Kalman algorithms to retain performance. We compare these methods empirically
and highlight use cases where each algorithm excels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Structure-Aware Quantum Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hala Hawashin, Mehrnoosh Sadrzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have advanced the field of natural
language processing (NLP), their "black box" nature obscures their
decision-making processes. To address this, researchers developed structured
approaches using higher order tensors. These are able to model linguistic
relations, but stall when training on classical computers due to their
excessive size. Tensors are natural inhabitants of quantum systems and training
on quantum computers provides a solution by translating text to variational
quantum circuits. In this paper, we develop MultiQ-NLP: a framework for
structure-aware data processing with multimodal text+image data. Here,
"structure" refers to syntactic and grammatical relationships in language, as
well as the hierarchical organization of visual elements in images. We enrich
the translation with new types and type homomorphisms and develop novel
architectures to represent structure. When tested on a main stream image
classification task (SVO Probes), our best model showed a par performance with
the state of the art classical models; moreover the best model was fully
structured.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 16 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring stochastic low-rank recurrent neural networks from neural data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthijs Pals, A Erdem Sağtekin, Felix Pei, Manuel Gloeckler, Jakob H Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central aim in computational neuroscience is to relate the activity of
large populations of neurons to an underlying dynamical system. Models of these
neural dynamics should ideally be both interpretable and fit the observed data
well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability
by having tractable dynamics. However, it is unclear how to best fit low-rank
RNNs to data consisting of noisy observations of an underlying stochastic
system. Here, we propose to fit stochastic low-rank RNNs with variational
sequential Monte Carlo methods. We validate our method on several datasets
consisting of both continuous and spiking neural data, where we obtain lower
dimensional latent dynamics than current state of the art methods.
Additionally, for low-rank models with piecewise linear nonlinearities, we show
how to efficiently identify all fixed points in polynomial rather than
exponential cost in the number of units, making analysis of the inferred
dynamics tractable for large RNNs. Our method both elucidates the dynamical
systems underlying experimental recordings and provides a generative model
whose trajectories match observed variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop and analyze a principled approach to kernel ridge regression under
covariate shift. The goal is to learn a regression function with small mean
squared error over a target distribution, based on unlabeled data from there
and labeled data that may have a different feature distribution. We propose to
split the labeled data into two subsets, and conduct kernel ridge regression on
them separately to obtain a collection of candidate models and an imputation
model. We use the latter to fill the missing labels and then select the best
candidate accordingly. Our non-asymptotic excess risk bounds demonstrate that
our estimator adapts effectively to both the structure of the target
distribution and the covariate shift. This adaptation is quantified through a
notion of effective sample size that reflects the value of labeled source data
for the target regression task. Our estimator achieves the minimax optimal
error rate up to a polylogarithmic factor, and we find that using pseudo-labels
for model selection does not significantly hinder performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triple Component Matrix Factorization: Untangling Global, Local, and
  Noisy Components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naichen Shi, Salar Fattahi, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the problem of common and unique feature extraction
from noisy data. When we have N observation matrices from N different and
associated sources corrupted by sparse and potentially gross noise, can we
recover the common and unique components from these noisy observations? This is
a challenging task as the number of parameters to estimate is approximately
thrice the number of observations. Despite the difficulty, we propose an
intuitive alternating minimization algorithm called triple component matrix
factorization (TCMF) to recover the three components exactly. TCMF is
distinguished from existing works in literature thanks to two salient features.
First, TCMF is a principled method to separate the three components given noisy
observations provably. Second, the bulk of the computation in TCMF can be
distributed. On the technical side, we formulate the problem as a constrained
nonconvex nonsmooth optimization problem. Despite the intricate nature of the
problem, we provide a Taylor series characterization of its solution by solving
the corresponding Karush-Kuhn-Tucker conditions. Using this characterization,
we can show that the alternating minimization algorithm makes significant
progress at each iteration and converges into the ground truth at a linear
rate. Numerical experiments in video segmentation and anomaly detection
highlight the superior feature extraction abilities of TCMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Energy Consumption Modelling for Electric Micromobility
  using an Open <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating challenges of traffic congestion and environmental degradation
underscore the critical importance of embracing E-Mobility solutions in urban
spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,
play a pivotal role in this transition, offering sustainable alternatives for
urban commuters. However, the energy consumption patterns for these tools are a
critical aspect that impacts their effectiveness in real-world scenarios and is
essential for trip planning and boosting user confidence in using these. To
this effect, recent studies have utilised physical models customised for
specific mobility tools and conditions, but these models struggle with
generalization and effectiveness in real-world scenarios due to a notable
absence of open datasets for thorough model evaluation and verification. To
fill this gap, our work presents an open dataset, collected in Dublin, Ireland,
specifically designed for energy modelling research related to E-Scooters and
E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption
modelling based on the dataset using a set of representative machine learning
algorithms and compare their performance against the contemporary mathematical
models as a baseline. Our results demonstrate a notable advantage for
data-driven models in comparison to the corresponding mathematical models for
estimating energy consumption. Specifically, data-driven models outperform
physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for
E-Scooters based on an in-depth analysis of the dataset under certain
assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables. This manuscript has been accepted by
  the IEEE ITEC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FALCON: Feedback-driven Adaptive Long/short-term memory reinforced
  Coding Optimization system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language models (LLMs) have achieved significant progress in
automated code generation. Despite their strong instruction-following
capabilities, these models frequently struggled to align with user intent in
coding scenarios. In particular, they were hampered by datasets that lacked
diversity and failed to address specialized tasks or edge cases. Furthermore,
challenges in supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) led to failures in generating precise,
human-intent-aligned code. To tackle these challenges and improve the code
generation performance for automated programming systems, we propose
Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization
(i.e., FALCON). FALCON is structured into two hierarchical levels. From the
global level, long-term memory improves code quality by retaining and applying
learned knowledge. At the local level, short-term memory allows for the
incorporation of immediate feedback from compilers and AI systems.
Additionally, we introduce meta-reinforcement learning with feedback rewards to
solve the global-local bi-level optimization problem and enhance the model's
adaptability across diverse code generation tasks. Extensive experiments
demonstrate that our technique achieves state-of-the-art performance, leading
other reinforcement learning methods by more than 4.5 percentage points on the
MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The
open-sourced code is publicly available at https://github.com/titurte/FALCON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boulder2Vec: Modeling Climber Performances in Professional Bouldering
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Baron, Victor Hau, Zeke Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using data from professional bouldering competitions from 2008 to 2022, we
train a logistic regression to predict climber results and measure climber
skill. However, this approach is limited, as a single numeric coefficient per
climber cannot adequately capture the intricacies of climbers' varying
strengths and weaknesses in different boulder problems. For example, some
climbers might prefer more static, technical routes while other climbers may
specialize in powerful, dynamic problems.
  To this end, we apply Probabilistic Matrix Factorization (PMF), a framework
commonly used in recommender systems, to represent the unique characteristics
of climbers and problems with latent, multi-dimensional vectors. In this
framework, a climber's performance on a given problem is predicted by taking
the dot product of the corresponding climber vector and problem vectors. PMF
effectively handles sparse datasets, such as our dataset where only a subset of
climbers attempt each particular problem, by extrapolating patterns from
similar climbers.
  We contrast the empirical performance of PMF to the logistic regression
approach and investigate the multivariate representations produced by PMF to
gain insights into climber characteristics. Our results show that the
multivariate PMF representations improve predictive performance of professional
bouldering competitions by capturing both the overall strength of climbers and
their specialized skill sets. We provide our code open-source at
https://github.com/baronet2/boulder2vec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eigen Attention: Attention in Low-Rank Space for KV Cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a groundbreaking advancement in the
domain of natural language processing due to their impressive reasoning
abilities. Recently, there has been considerable interest in increasing the
context lengths for these models to enhance their applicability to complex
tasks. However, at long context lengths and large batch sizes, the key-value
(KV) cache, which stores the attention keys and values, emerges as the new
bottleneck in memory usage during inference. To address this, we propose Eigen
Attention, which performs the attention operation in a low-rank space, thereby
reducing the KV cache memory overhead. Our proposed approach is orthogonal to
existing KV cache compression techniques and can be used synergistically with
them. Through extensive experiments over OPT, MPT, and Llama model families, we
demonstrate that Eigen Attention results in up to 40% reduction in KV cache
sizes and up to 60% reduction in attention operation latency with minimal drop
in performance. Code is available at
https://github.com/UtkarshSaxena1/EigenAttn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffBatt: A <span class="highlight-title">Diffusion</span> Model for Battery Degradation Prediction and
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, André Hebenbrock, Raphael Ginster, Steffen Blömeke, Stefan Wittek, Christoph Herrmann, Thomas S. Spengler, Thomas Turek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a critical challenge in the pursuit of green
technologies and sustainable energy solutions. Despite significant research
efforts, predicting battery capacity loss accurately remains a formidable task
due to its complex nature, influenced by both aging and cycling behaviors. To
address this challenge, we introduce a novel general-purpose model for battery
degradation prediction and synthesis, DiffBatt. Leveraging an innovative
combination of conditional and unconditional diffusion models with
classifier-free guidance and transformer architecture, DiffBatt achieves high
expressivity and scalability. DiffBatt operates as a probabilistic model to
capture uncertainty in aging behaviors and a generative model to simulate
battery degradation. The performance of the model excels in prediction tasks
while also enabling the generation of synthetic degradation curves,
facilitating enhanced model training by data augmentation. In the remaining
useful life prediction task, DiffBatt provides accurate results with a mean
RMSE of 196 cycles across all datasets, outperforming all other models and
demonstrating superior generalizability. This work represents an important step
towards developing foundational models for battery degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Conversion with Switching Costs: Robust and Learning-Augmented
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce and study online conversion with switching costs, a family of
online problems that capture emerging problems at the intersection of energy
and sustainability. In this problem, an online player attempts to purchase
(alternatively, sell) fractional shares of an asset during a fixed time horizon
with length $T$. At each time step, a cost function (alternatively, price
function) is revealed, and the player must irrevocably decide an amount of
asset to convert. The player also incurs a switching cost whenever their
decision changes in consecutive time steps, i.e., when they increase or
decrease their purchasing amount. We introduce competitive (robust)
threshold-based algorithms for both the minimization and maximization variants
of this problem, and show they are optimal among deterministic online
algorithms. We then propose learning-augmented algorithms that take advantage
of untrusted black-box advice (such as predictions from a machine learning
model) to achieve significantly better average-case performance without
sacrificing worst-case competitive guarantees. Finally, we empirically evaluate
our proposed algorithms using a carbon-aware EV charging case study, showing
that our algorithms substantially improve on baseline methods for this problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared as a conference paper at SIGMETRICS / Performance '24. 47
  pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Shen, Yici Yan, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-X knockoff has garnered significant attention among various feature
selection methods due to its guarantees for controlling the false discovery
rate (FDR). Since its introduction in parametric design, knockoff techniques
have evolved to handle arbitrary data distributions using deep learning-based
generative models. However, we have observed limitations in the current
implementations of the deep Model-X knockoff framework. Notably, the "swap
property" that knockoffs require often faces challenges at the sample level,
resulting in diminished selection power. To address these issues, we develop
"Deep Dependency Regularized Knockoff (DeepDRK)," a distribution-free deep
learning method that effectively balances FDR and power. In DeepDRK, we
introduce a novel formulation of the knockoff model as a learning problem under
multi-source adversarial attacks. By employing an innovative perturbation
technique, we achieve lower FDR and higher power. Our model outperforms
existing benchmarks across synthetic, semi-synthetic, and real-world datasets,
particularly when sample sizes are small and data distributions are
non-Gaussian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 15 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gymnasium: A Standard Interface for Reinforcement Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U. Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Hannah Tan, Omar G. Younis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) is a continuously growing field that has the
potential to revolutionize many areas of artificial intelligence. However,
despite its promise, RL research is often hindered by the lack of
standardization in environment and algorithm implementations. This makes it
difficult for researchers to compare and build upon each other's work, slowing
down progress in the field. Gymnasium is an open-source library that provides a
standard API for RL environments, aiming to tackle this issue. Gymnasium's main
feature is a set of abstractions that allow for wide interoperability between
environments and training algorithms, making it easier for researchers to
develop and test RL algorithms. In addition, Gymnasium provides a collection of
easy-to-use environments, tools for easily customizing environments, and tools
to ensure the reproducibility and robustness of RL research. Through this
unified framework, Gymnasium significantly streamlines the process of
developing and testing RL algorithms, enabling researchers to focus more on
innovation and less on implementation details. By providing a standardized
platform for RL research, Gymnasium helps to drive forward the field of
reinforcement learning and unlock its full potential. Gymnasium is available
online at https://github.com/Farama-Foundation/Gymnasium
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Refinement Protocols for Distributed Distribution Estimation
  under $\ell^p$-Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deheng Yuan, Tao Guo, Zhongyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the communication-constrained estimation of discrete distributions
under $\ell^p$ losses, where each distributed terminal holds multiple
independent samples and uses limited number of bits to describe the samples. We
obtain the minimax optimal rates of the problem in most parameter regimes. An
elbow effect of the optimal rates at $p=2$ is clearly identified. To show the
optimal rates, we first design estimation protocols to achieve them. The key
ingredient of these protocols is to introduce adaptive refinement mechanisms,
which first generate rough estimate by partial information and then establish
refined estimate in subsequent steps guided by the rough estimate. The
protocols leverage successive refinement, sample compression, thresholding and
random hashing methods to achieve the optimal rates in different parameter
regimes. The optimality of the protocols is shown by deriving compatible
minimax lower bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Response Theory via <span class="highlight-title">Generative</span> Score Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovico Theo Giorgini, Katherine Deck, Tobias Bischoff, Andre Souza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an approach for analyzing the responses of dynamical systems to
external perturbations that combines score-based generative modeling with the
Generalized Fluctuation-Dissipation Theorem (GFDT). The methodology enables
accurate estimation of system responses, including those with non-Gaussian
statistics. We numerically validate our approach using time-series data from
three different stochastic partial differential equations of increasing
complexity: an Ornstein-Uhlenbeck process with spatially correlated noise, a
modified stochastic Allen-Cahn equation, and the 2D Navier-Stokes equations. We
demonstrate the improved accuracy of the methodology over conventional methods
and discuss its potential as a versatile tool for predicting the statistical
behavior of complex dynamical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In press. Includes supplementary material in the file
  supp_material.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aggregating distribution <span class="highlight-title">forecast</span>s from deep ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Schulz, Lutz Köhler, Sebastian Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of accurately quantifying forecast uncertainty has motivated
much recent research on probabilistic forecasting. In particular, a variety of
deep learning approaches has been proposed, with forecast distributions
obtained as output of neural networks. These neural network-based methods are
often used in the form of an ensemble, e.g., based on multiple model runs from
different random initializations or more sophisticated ensembling strategies
such as dropout, resulting in a collection of forecast distributions that need
to be aggregated into a final probabilistic prediction. With the aim of
consolidating findings from the machine learning literature on ensemble methods
and the statistical literature on forecast combination, we address the question
of how to aggregate distribution forecasts based on such `deep ensembles'.
Using theoretical arguments and a comprehensive analysis on twelve benchmark
data sets, we systematically compare probability- and quantile-based
aggregation methods for three neural network-based approaches with different
forecast distribution types as output. Our results show that combining forecast
distributions from deep ensembles can substantially improve the predictive
performance. We propose a general quantile aggregation framework for deep
ensembles that allows for corrections of systematic deficiencies and performs
well in a variety of settings, often superior compared to a linear combination
of the forecast densities. Finally, we investigate the effects of the ensemble
size and derive recommendations of aggregating distribution forecasts from deep
ensembles in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Time-Aware Graph Neural Networks to Predict <span class="highlight-title">Temporal</span> Centralities
  in Dynamic Graphs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franziska Heeg, Ingo Scholtes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Node centralities play a pivotal role in network science, social network
analysis, and recommender systems. In temporal data, static path-based
centralities like closeness or betweenness can give misleading results about
the true importance of nodes in a temporal graph. To address this issue,
temporal generalizations of betweenness and closeness have been defined that
are based on the shortest time-respecting paths between pairs of nodes.
However, a major issue of those generalizations is that the calculation of such
paths is computationally expensive. Addressing this issue, we study the
application of De Bruijn Graph Neural Networks (DBGNN), a time-aware graph
neural network architecture, to predict temporal path-based centralities in
time series data. We experimentally evaluate our approach in 13 temporal graphs
from biological and social systems and show that it considerably improves the
prediction of betweenness and closeness centrality compared to (i) a static
Graph Convolutional Neural Network, (ii) an efficient sampling-based
approximation technique for temporal betweenness, and (iii) two
state-of-the-art time-aware graph learning techniques for dynamic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Robot Navigation Policies with Task-Specific Uncertainty
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Leonardo Bobadilla, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots performing navigation tasks in complex environments face significant
challenges due to uncertainty in state estimation. Effectively managing this
uncertainty is crucial, but the optimal approach varies depending on the
specific details of the task: different tasks require varying levels of
precision in different regions of the environment. For instance, a robot
navigating a crowded space might need precise localization near obstacles but
can operate effectively with less precise state estimates in open areas. This
varying need for certainty in different parts of the environment, depending on
the task, calls for policies that can adapt their uncertainty management
strategies based on task-specific requirements. In this paper, we present a
framework for integrating task-specific uncertainty requirements directly into
navigation policies. We introduce Task-Specific Uncertainty Map (TSUM), which
represents acceptable levels of state estimation uncertainty across different
regions of the operating environment for a given task. Using TSUM, we propose
Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE),
a policy conditioning framework that incorporates these uncertainty
requirements into the robot's decision-making process. We find that
conditioning policies on TSUMs provides an effective way to express
task-specific uncertainty requirements and enables the robot to reason about
the context-dependent value of certainty. We show how integrating GUIDE into
reinforcement learning frameworks allows the agent to learn navigation policies
without the need for explicit reward engineering to balance task completion and
uncertainty management. We evaluate GUIDE on a variety of real-world navigation
tasks and find that it demonstrates significant improvements in task completion
rates compared to baselines. Evaluation videos can be found at
https://guided-agents.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Bi-Focal Perspectives and Granular Feature Integration for
  Accurate Reliable Early Alzheimer's Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pandiyaraju V, Shravan Venkatraman, Abeshek A, Aravintakshan S A, Pavan Kumar S, Kannan A
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD) is the most common form of neurodegeneration, which
impacts millions of people each year. Diagnosing and classifying AD accurately
with neuroimaging data is an ongoing challenge in the field of medicine.
Traditional Convolutional Neural Networks (CNNs) are good at capturing
low-level information from images, but their capability to extract high-level
minuscule particles is suboptimal, which is a significant challenge in
detecting AD from MRI scans. To overcome this, we propose a novel Granular
Feature Integration method to combine information extraction at different
scales combined with an efficient information flow. We also propose a Bi-Focal
Perspective mechanism to highlight focus on subtle neurofibrillary tangles and
amyloid plaques in MRI scans. Our model yielded an F1-Score of 99.31%, a
precision of 99.24%, and a recall of 99.51%, which shows a major improvement in
comparison to existing state-of-the-art (SOTA) CNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEPS: Boosting Generalization in Parametric PDE Neural Solvers through
  Adaptive Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Kassaï Koupaï, Jorge Mifsut Benet, Yuan Yin, Jean-Noël Vittaut, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving parametric partial differential equations (PDEs) presents significant
challenges for data-driven methods due to the sensitivity of spatio-temporal
dynamics to variations in PDE parameters. Machine learning approaches often
struggle to capture this variability. To address this, data-driven approaches
learn parametric PDEs by sampling a very large variety of trajectories with
varying PDE parameters. We first show that incorporating conditioning
mechanisms for learning parametric PDEs is essential and that among them,
$\textit{adaptive conditioning}$, allows stronger generalization. As existing
adaptive conditioning methods do not scale well with respect to the number of
parameters to adapt in the neural solver, we propose GEPS, a simple adaptation
mechanism to boost GEneralization in Pde Solvers via a first-order optimization
and low-rank rapid adaptation of a small set of context parameters. We
demonstrate the versatility of our approach for both fully data-driven and for
physics-aware neural solvers. Validation performed on a whole range of
spatio-temporal forecasting problems demonstrates excellent performance for
generalizing to unseen conditions including initial conditions, PDE
coefficients, forcing terms and solution domain. $\textit{Project page}$:
https://geps-project.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast training and sampling of Restricted Boltzmann Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Béreux, Aurélien Decelle, Cyril Furtlehner, Lorenzo Rosset, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Restricted Boltzmann Machines (RBMs) are effective tools for modeling complex
systems and deriving insights from data. However, training these models with
highly structured data presents significant challenges due to the slow mixing
characteristics of Markov Chain Monte Carlo processes. In this study, we build
upon recent theoretical advancements in RBM training, to significantly reduce
the computational cost of training (in very clustered datasets), evaluating and
sampling in RBMs in general. The learning process is analogous to thermodynamic
continuous phase transitions observed in ferromagnetic models, where new modes
in the probability measure emerge in a continuous manner. Such continuous
transitions are associated with the critical slowdown effect, which adversely
affects the accuracy of gradient estimates, particularly during the initial
stages of training with clustered data. To mitigate this issue, we propose a
pre-training phase that encodes the principal components into a low-rank RBM
through a convex optimization process. This approach enables efficient static
Monte Carlo sampling and accurate computation of the partition function. We
exploit the continuous and smooth nature of the parameter annealing trajectory
to achieve reliable and computationally efficient log-likelihood estimations,
enabling online assessment during the training, and propose a novel sampling
strategy named parallel trajectory tempering (PTT) which outperforms previously
optimized MCMC methods. Our results show that this training strategy enables
RBMs to effectively address highly structured datasets that conventional
methods struggle with. We also provide evidence that our log-likelihood
estimation is more accurate than traditional, more computationally intensive
approaches in controlled scenarios. The PTT algorithm significantly accelerates
MCMC processes compared to existing and conventional methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Initial Guessing Bias: How Untrained Networks Favor Some Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00809v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00809v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Francazi, Aurelien Lucchi, Marco Baity-Jesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and controlling biasing effects in neural networks is crucial
for ensuring accurate and fair model performance. In the context of
classification problems, we provide a theoretical analysis demonstrating that
the structure of a deep neural network (DNN) can condition the model to assign
all predictions to the same class, even before the beginning of training, and
in the absence of explicit biases. We prove that, besides dataset properties,
the presence of this phenomenon, which we call \textit{Initial Guessing Bias}
(IGB), is influenced by model choices including dataset preprocessing methods,
and architectural decisions, such as activation functions, max-pooling layers,
and network depth. Our analysis of IGB provides information for architecture
selection and model initialization. We also highlight theoretical consequences,
such as the breakdown of node-permutation symmetry, the violation of
self-averaging and the non-trivial effects that depth has on the phenomenon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed notation typos in Figure3 and Figure4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group-blind optimal transport to group parity and its constrained
  variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhou, Jakub Marecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness holds a pivotal role in the realm of machine learning, particularly
when it comes to addressing groups categorised by protected attributes, e.g.,
gender, race. Prevailing algorithms in fair learning predominantly hinge on
accessibility or estimations of these protected attributes, at least in the
training process. We design a single group-blind projection map that aligns the
feature distributions of both groups in the source data, achieving
(demographic) group parity, without requiring values of the protected attribute
for individual samples in the computation of the map, as well as its use.
Instead, our approach utilises the feature distributions of the privileged and
unprivileged groups in a boarder population and the essential assumption that
the source data are unbiased representation of the population. We present
numerical results on synthetic data and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking the Role of Remote Sensors in Comfort, Energy and Demand
  Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozan Baris Mulayim, Edson Severnini, Mario Bergés
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In single-zone multi-node systems (SZMRSs), temperature controls rely on a
single probe near the thermostat, resulting in temperature discrepancies that
cause thermal discomfort and energy waste. Augmenting smart thermostats (STs)
with per-room sensors has gained acceptance by major ST manufacturers. This
paper leverages additional sensory information to empirically characterize the
services provided by buildings, including thermal comfort, energy efficiency,
and demand response (DR). Utilizing room-level time-series data from 1,000
houses, metadata from 110,000 houses across the United States, and data from
two real-world testbeds, we examine the limitations of SZMNSs and explore the
potential of remote sensors. We discovered that comfortable DR durations
(CDRDs) for rooms are typically 70% longer or 40% shorter than for the room
with the thermostat. When averaging, rooms at the control temperature's bounds
are typically deviated around -3{\deg}F to 2.5{\deg}F from the average.
Moreover, in 95% of houses, we identified rooms experiencing notably higher
solar gains compared to the rest of the rooms, while 85% and 70% of houses
demonstrated lower heat input and poor insulation, respectively. Lastly, it
became evident that the consumption of cooling energy escalates with the
increase in the number of sensors, whereas heating usage experiences
fluctuations ranging from -19% to +25%. This study serves as a benchmark for
assessing the thermal comfort and DR services in the existing housing stock,
while also highlighting the energy efficiency impacts of sensing technologies.
Our approach sets the stage for more granular, precise control strategies of
SZMNSs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Figures, 8 Tables, 25 Pages. Published in Data-Centric Engineering
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Health Index Monitoring with Feature Generation and
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaëtan Frusque, Ismail Nejjar, Majid Nabavi, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Health Index (HI) is crucial for evaluating system health and is
important for tasks like anomaly detection and Remaining Useful Life (RUL)
prediction of safety-critical systems. Real-time, meticulous monitoring of
system conditions is essential, especially in manufacturing high-quality and
safety-critical components such as spray coatings. However, acquiring accurate
health status information (HI labels) in real scenarios can be difficult or
costly because it requires continuous, precise measurements that fully capture
the system's health. As a result, using datasets from systems run-to-failure,
which provide limited HI labels only at the healthy and end-of-life phases,
becomes a practical approach. We employ Deep Semi-supervised Anomaly Detection
(DeepSAD) embeddings to tackle the challenge of extracting features associated
with the system's health state. Additionally, we introduce a diversity loss to
further enrich the DeepSAD embeddings. We also propose applying an alternating
projection algorithm with isotonic constraints to transform the embedding into
a normalized HI with an increasing trend. Validation on the PHME2010 milling
dataset, a recognized benchmark with ground truth HIs, confirms the efficacy of
our proposed HI estimations. Our methodology is further applied to monitor the
wear states of thermal spray coatings using high-frequency voltage. These
contributions facilitate more accessible and reliable HI estimation,
particularly in scenarios where obtaining ground truth HI labels is impossible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-time Integration of Nonlinear Wave Equations with Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanhang Lei, Zhen Lei, Lei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural operators have shown promise in solving many types of Partial
Differential Equations (PDEs). They are significantly faster compared to
traditional numerical solvers once they have been trained with a certain amount
of observed data. However, their numerical performance in solving
time-dependent PDEs, particularly in long-time prediction of dynamic systems,
still needs improvement. In this paper, we focus on solving the long-time
integration of nonlinear wave equations via neural operators by replacing the
initial condition with the prediction in a recurrent manner. Given limited
observed temporal trajectory data, we utilize some intrinsic features of these
nonlinear wave equations, such as conservation laws and well-posedness, to
improve the algorithm design and reduce accumulated error. Our numerical
experiments examine these improvements in the Korteweg-de Vries (KdV) equation,
the sine-Gordon equation, and the Klein-Gordon wave equation on the irregular
domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered
  by Semantic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17705v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17705v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Cheng, Yao Sun, Dusit Niyato, Lan Zhang, Lei Zhang, Muhammad Ali Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant advances in AI-generated content (AIGC) and the
proliferation of mobile devices, providing high-quality AIGC services via
wireless networks is becoming the future direction. However, the primary
challenges of AIGC services provisioning in wireless networks lie in unstable
channels, limited bandwidth resources, and unevenly distributed computational
resources. To this end, this paper proposes a semantic communication
(SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where
only semantic information of the content rather than all the binary bits should
be generated and transmitted by using SemCom. Specifically, SemAIGC integrates
diffusion models within the semantic encoder and decoder to design a
workload-adjustable transceiver thereby allowing adjustment of computational
resource utilization in edge and local. In addition, a Resource-aware wOrklOad
Trade-off (ROOT) scheme is devised to intelligently make workload adaptation
decisions for the transceiver, thus efficiently generating, transmitting, and
fine-tuning content as per dynamic wireless channel conditions and service
requirements. Simulations verify the superiority of our proposed SemAIGC
framework in terms of latency and content quality compared to conventional
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Your Step (by Step): Chain-of-<span class="highlight-title">Thought</span> can Reduce Performance on
  Tasks where Thinking Makes Humans Worse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21333v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21333v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting has become a widely used strategy for
working with large language and multimodal models. While CoT has been shown to
improve performance across many tasks, determining the settings in which it is
effective remains an ongoing effort. In particular, it is still an open
question in what settings CoT systematically reduces model performance. In this
paper, we seek to identify the characteristics of tasks where CoT reduces
performance by drawing inspiration from cognitive psychology, looking at cases
where (i) verbal thinking or deliberation hurts performance in humans, and (ii)
the constraints governing human performance generalize to language models.
Three such cases are implicit statistical learning, visual recognition, and
classifying with patterns containing exceptions. In extensive experiments
across all three settings, we find that a diverse collection of
state-of-the-art models exhibit significant drop-offs in performance (e.g., up
to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using
inference-time reasoning compared to zero-shot counterparts. We also identify
three tasks that satisfy condition (i) but not (ii), and find that while verbal
thinking reduces human performance in these tasks, CoT retains or increases
model performance. Overall, our results show that while there is not an exact
parallel between the cognitive processes of models and those of humans,
considering cases where thinking has negative consequences for human
performance can help us identify settings where it negatively impacts models.
By connecting the literature on human deliberation with evaluations of CoT, we
offer a new tool that can be used in understanding the impact of prompt choices
and inference-time reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Regulation Neurons in <span class="highlight-title">Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread use, the mechanisms by which large language models
(LLMs) represent and regulate uncertainty in next-token predictions remain
largely unexplored. This study investigates two critical components believed to
influence this uncertainty: the recently discovered entropy neurons and a new
set of components that we term token frequency neurons. Entropy neurons are
characterized by an unusually high weight norm and influence the final layer
normalization (LayerNorm) scale to effectively scale down the logits. Our work
shows that entropy neurons operate by writing onto an unembedding null space,
allowing them to impact the residual stream norm with minimal direct effect on
the logits themselves. We observe the presence of entropy neurons across a
range of models, up to 7 billion parameters. On the other hand, token frequency
neurons, which we discover and describe here for the first time, boost or
suppress each token's logit proportionally to its log frequency, thereby
shifting the output distribution towards or away from the unigram distribution.
Finally, we present a detailed case study where entropy neurons actively manage
confidence in the setting of induction, i.e. detecting and continuing repeated
subsequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deinterleaving of Discrete Renewal Process Mixtures with Application to
  Electronic Support Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Pinsolle, Olivier Goudet, Cyrille Enderli, Sylvain Lamprier, Jin-Kao Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new deinterleaving method for mixtures of
discrete renewal Markov chains. This method relies on the maximization of a
penalized likelihood score. It exploits all available information about both
the sequence of the different symbols and their arrival times. A theoretical
analysis is carried out to prove that minimizing this score allows to recover
the true partition of symbols in the large sample limit, under mild conditions
on the component processes. This theoretical analysis is then validated by
experiments on synthetic data. Finally, the method is applied to deinterleave
pulse trains received from different emitters in a RESM (Radar Electronic
Support Measurements) context and we show that the proposed method competes
favorably with state-of-the-art methods on simulated warfare datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Connect Speech Foundation Models and <span class="highlight-title">Large Language Model</span>s? What
  Matters and What Does Not 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Almost Surely Asymptotically Constant Graph Neural Networks <span class="chip">NeurIPS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03880v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03880v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Adam-Day, Michael Benedikt, İsmail İlkan Ceylan, Ben Finkelshtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new angle on the expressive power of graph neural networks
(GNNs) by studying how the predictions of real-valued GNN classifiers, such as
those classifying graphs probabilistically, evolve as we apply them on larger
graphs drawn from some random graph model. We show that the output converges to
a constant function, which upper-bounds what these classifiers can uniformly
express. This strong convergence phenomenon applies to a very wide class of
GNNs, including state of the art models, with aggregates including mean and the
attention-based mechanism of graph transformers. Our results apply to a broad
class of random graph models, including sparse and dense variants of the
Erd\H{o}s-R\'enyi model, the stochastic block model, and the Barab\'asi-Albert
model. We empirically validate these findings, observing that the convergence
phenomenon appears not only on random graphs but also on some real-world
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS '24 camera-ready version; 10 body pages, 29 appendix pages,
  11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Brave Assumption-Based Argumentation Frameworks via ASP <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele De Angelis, Maurizio Proietti, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assumption-based Argumentation (ABA) is advocated as a unifying formalism for
various forms of non-monotonic reasoning, including logic programming. It
allows capturing defeasible knowledge, subject to argumentative debate. While,
in much existing work, ABA frameworks are given up-front, in this paper we
focus on the problem of automating their learning from background knowledge and
positive/negative examples. Unlike prior work, we newly frame the problem in
terms of brave reasoning under stable extensions for ABA. We present a novel
algorithm based on transformation rules (such as Rote Learning, Folding,
Assumption Introduction and Fact Subsumption) and an implementation thereof
that makes use of Answer Set Programming. Finally, we compare our technique to
state-of-the-art ILP systems that learn defeasible knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper published in: Proceedings 27th European
  Conference on Artificial Intelligence, Frontiers in Artificial Intelligence
  and Applications, Volume 392: ECAI 2024, pp. 3445 - 3452. DOI:
  10.3233/FAIA240896</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When AI Eats Itself: On the Caveats of AI Autophagy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Xing, Fadong Shi, Jiahao Huang, Yinzhe Wu, Yang Nan, Sheng Zhang, Yingying Fang, Mike Roberts, Carola-Bibiane Schönlieb, Javier Del Ser, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (AI) technologies and large models are
producing realistic outputs across various domains, such as images, text,
speech, and music. Creating these advanced generative models requires
significant resources, particularly large and high-quality datasets. To
minimise training expenses, many algorithm developers use data created by the
models themselves as a cost-effective training solution. However, not all
synthetic data effectively improve model performance, necessitating a strategic
balance in the use of real versus synthetic data to optimise outcomes.
Currently, the previously well-controlled integration of real and synthetic
data is becoming uncontrollable. The widespread and unregulated dissemination
of synthetic data online leads to the contamination of datasets traditionally
compiled through web scraping, now mixed with unlabeled synthetic data. This
trend, known as the AI autophagy phenomenon, suggests a future where generative
AI systems may increasingly consume their own outputs without discernment,
raising concerns about model performance, reliability, and ethical
implications. What will happen if generative AI continuously consumes itself
without discernment? What measures can we take to mitigate the potential
adverse effects? To address these research questions, this study examines the
existing literature, delving into the consequences of AI autophagy, analyzing
the associated risks, and exploring strategies to mitigate its impact. Our aim
is to provide a comprehensive perspective on this phenomenon advocating for a
balanced approach that promotes the sustainable development of generative AI
technologies in the era of large models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAN: Detecting Backdoors Activated by Adversarial Neuron Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Shujian Yu, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks on deep learning represent a recent threat that has gained
significant attention in the research community. Backdoor defenses are mainly
based on backdoor inversion, which has been shown to be generic,
model-agnostic, and applicable to practical threat scenarios. State-of-the-art
backdoor inversion recovers a mask in the feature space to locate prominent
backdoor features, where benign and backdoor features can be disentangled.
However, it suffers from high computational overhead, and we also find that it
overly relies on prominent backdoor features that are highly distinguishable
from benign features. To tackle these shortcomings, this paper improves
backdoor feature inversion for backdoor detection by incorporating extra neuron
activation information. In particular, we adversarially increase the loss of
backdoored models with respect to weights to activate the backdoor effect,
based on which we can easily differentiate backdoored and clean models.
Experimental results demonstrate our defense, BAN, is 1.37$\times$ (on
CIFAR-10) and 5.11$\times$ (on ImageNet200) more efficient with an average
9.99\% higher detect success rate than the state-of-the-art defense BTI-DBF.
Our code and trained models are publicly available
at~\url{https://github.com/xiaoyunxxy/ban}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deterministic Uncertainty Propagation for Improved Model-Based Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Akgül, Manuel Haußmann, Melih Kandemir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to model-based offline reinforcement learning often
incorporate uncertainty-based reward penalization to address the distributional
shift problem. These approaches, commonly known as pessimistic value iteration,
use Monte Carlo sampling to estimate the Bellman target to perform temporal
difference based policy evaluation. We find out that the randomness caused by
this sampling step significantly delays convergence. We present a theoretical
result demonstrating the strong dependency of suboptimality on the number of
Monte Carlo samples taken per Bellman target calculation. Our main contribution
is a deterministic approximation to the Bellman target that uses progressive
moment matching, a method developed originally for deterministic variational
inference. The resulting algorithm, which we call Moment Matching Offline
Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next
state through a nonlinear Q-network in a deterministic fashion by approximating
the distributions of hidden layer activations by a normal distribution. We show
that it is possible to provide tighter guarantees for the suboptimality of
MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO
to converge faster than these approaches in a large set of benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably <span class="highlight-title">Transformer</span>s Harness Multi-Concept Word Semantics for Efficient
  In-Context Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavior<span class="highlight-title">GPT</span>: Smart <span class="highlight-title">Agent</span> Simulation for Autonomous Driving with
  Next-Patch Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating realistic interactions among traffic agents is crucial for
efficiently validating the safety of autonomous driving systems. Existing
leading simulators primarily use an encoder-decoder structure to encode the
historical trajectories for future simulation. However, such a paradigm
complicates the model architecture, and the manual separation of history and
future trajectories leads to low data utilization. To address these challenges,
we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a
decoder-only, autoregressive architecture designed to simulate the sequential
motion of multiple agents. Crucially, our approach discards the traditional
separation between "history" and "future," treating each time step as the
"current" one, resulting in a simpler, more parameter- and data-efficient
design that scales seamlessly with data and computation. Additionally, we
introduce the Next-Patch Prediction Paradigm (NP3), which enables models to
reason at the patch level of trajectories and capture long-range
spatial-temporal interactions. BehaviorGPT ranks first across several metrics
on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in
multi-agent and agent-map interactions. We outperformed state-of-the-art models
with a realism score of 0.741 and improved the minADE metric to 1.540, with an
approximately 91.6% reduction in model parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plasticity Loss in Deep Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Klein, Lukas Miklautz, Kevin Sidak, Claudia Plant, Sebastian Tschiatschek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Akin to neuroplasticity in human brains, the plasticity of deep neural
networks enables their quick adaption to new data. This makes plasticity
particularly crucial for deep Reinforcement Learning (RL) agents: Once
plasticity is lost, an agent's performance will inevitably plateau because it
cannot improve its policy to account for changes in the data distribution,
which are a necessary consequence of its learning process. Thus, developing
well-performing and sample-efficient agents hinges on their ability to remain
plastic during training. Furthermore, the loss of plasticity can be connected
to many other issues plaguing deep RL, such as training instabilities, scaling
failures, overestimation bias, and insufficient exploration. With this survey,
we aim to provide an overview of the emerging research on plasticity loss for
academics and practitioners of deep reinforcement learning. First, we propose a
unified definition of plasticity loss based on recent works, relate it to
definitions from the literature, and discuss metrics for measuring plasticity
loss. Then, we categorize and discuss numerous possible causes of plasticity
loss before reviewing currently employed mitigation strategies. Our taxonomy is
the first systematic overview of the current state of the field. Lastly, we
discuss prevalent issues within the literature, such as a necessity for broader
evaluation, and provide recommendations for future research, like gaining a
better understanding of an agent's neural activity and behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Without Harm: An Influence-Guided Active Sampling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Pang, Jialu Wang, Zhaowei Zhu, Yuanshun Yao, Chen Qian, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of fairness in machine learning (ML), ensuring that the models do
not exhibit biases toward protected demographic groups, typically results in a
compromise scenario. This compromise can be explained by a Pareto frontier
where given certain resources (e.g., data), reducing the fairness violations
often comes at the cost of lowering the model accuracy. In this work, we aim to
train models that mitigate group fairness disparity without causing harm to
model accuracy. Intuitively, acquiring more data is a natural and promising
approach to achieve this goal by reaching a better Pareto frontier of the
fairness-accuracy tradeoff. The current data acquisition methods, such as fair
active learning approaches, typically require annotating sensitive attributes.
However, these sensitive attribute annotations should be protected due to
privacy and safety concerns. In this paper, we propose a tractable active data
sampling algorithm that does not rely on training group annotations, instead
only requiring group annotations on a small validation set. Specifically, the
algorithm first scores each new example by its influence on fairness and
accuracy evaluated on the validation dataset, and then selects a certain number
of examples for training. We theoretically analyze how acquiring more data can
improve fairness without causing harm, and validate the possibility of our
sampling approach in the context of risk disparity. We also provide the upper
bound of generalization error and risk disparity as well as the corresponding
connections. Extensive experiments on real-world data demonstrate the
effectiveness of our proposed algorithm. Our code is available at
https://github.com/UCSC-REAL/FairnessWithoutHarm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Principled Graph <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10119v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10119v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Müller, Daniel Kusuma, Blai Bonet, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning architectures based on the k-dimensional Weisfeiler-Leman
(k-WL) hierarchy offer a theoretically well-understood expressive power.
However, such architectures often fail to deliver solid predictive performance
on real-world tasks, limiting their practical impact. In contrast, global
attention-based models such as graph transformers demonstrate strong
performance in practice, but comparing their expressive power with the k-WL
hierarchy remains challenging, particularly since these architectures rely on
positional or structural encodings for their expressivity and predictive
performance. To address this, we show that the recently proposed Edge
Transformer, a global attention model operating on node pairs instead of nodes,
has at least 3-WL expressive power. Empirically, we demonstrate that the Edge
Transformer surpasses other theoretically aligned architectures regarding
predictive performance while not relying on positional or structural encodings.
Our code is available at https://github.com/luis-mueller/towards-principled-gts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> models for Gaussian distributions: Exact solutions and
  Wasserstein errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14250v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14250v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile Pierret, Bruno Galerne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion or score-based models recently showed high performance in image
generation. They rely on a forward and a backward stochastic differential
equations (SDE). The sampling of a data distribution is achieved by solving
numerically the backward SDE or its associated flow ODE. Studying the
convergence of these models necessitates to control four different types of
error: the initialization error, the truncation error, the discretization and
the score approximation. In this paper, we study theoretically the behavior of
diffusion models and their numerical implementation when the data distribution
is Gaussian. In this restricted framework where the score function is a linear
operator, we derive the analytical solutions of the backward SDE and the
probability flow ODE. We prove that these solutions and their discretizations
are all Gaussian processes, which allows us to compute exact Wasserstein errors
induced by each error type for any sampling scheme. Monitoring convergence
directly in the data space instead of relying on Inception features, our
experiments show that the recommended numerical schemes from the diffusion
models literature are also the best sampling schemes for Gaussian
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascade of phase transitions in the training of Energy-based models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bachtis, Giulio Biroli, Aurélien Decelle, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the feature encoding process in a prototypical
energy-based generative model, the Restricted Boltzmann Machine (RBM). We start
with an analytical investigation using simplified architectures and data
structures, and end with numerical analysis of real trainings on real datasets.
Our study tracks the evolution of the model's weight matrix through its
singular value decomposition, revealing a series of phase transitions
associated to a progressive learning of the principal modes of the empirical
probability distribution. The model first learns the center of mass of the
modes and then progressively resolve all modes through a cascade of phase
transitions. We first describe this process analytically in a controlled setup
that allows us to study analytically the training dynamics. We then validate
our theoretical results by training the Bernoulli-Bernoulli RBM on real data
sets. By using data sets of increasing dimension, we show that learning indeed
leads to sharp phase transitions in the high-dimensional limit. Moreover, we
propose and test a mean-field finite-size scaling hypothesis. This shows that
the first phase transition is in the same universality class of the one we
studied analytically, and which is reminiscent of the mean-field
paramagnetic-to-ferromagnetic phase transition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, accepted to Neurips2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learnability of high-dimensional targets by two-parameter models and
  gradient flow <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Yarotsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the theoretical possibility of learning $d$-dimensional targets
with $W$-parameter models by gradient flow (GF) when $W<d$. Our main result
shows that if the targets are described by a particular $d$-dimensional
probability distribution, then there exist models with as few as two parameters
that can learn the targets with arbitrarily high success probability. On the
other hand, we show that for $W<d$ there is necessarily a large subset of
GF-non-learnable targets. In particular, the set of learnable targets is not
dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the
$W$-dimensional sphere contains non-learnable targets. Finally, we observe that
the model in our main theorem on almost guaranteed two-parameter learning is
constructed using a hierarchical procedure and as a result is not expressible
by a single elementary function. We show that this limitation is essential in
the sense that most models written in terms of elementary functions cannot
achieve the learnability demonstrated in this theorem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready NeurIPS 2024 version; some extra comments and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked
  Preferences <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique. We release the preference pairs used in
alignment at:
https://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2024 as long (findings) conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphXAIN: Narratives to Explain Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Cedro, David Martens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are a powerful technique for machine learning on
graph-structured data, yet they pose interpretability challenges, especially
for non-expert users. Existing GNN explanation methods often yield technical
outputs such as subgraphs and feature importance scores, which are not easily
understood. Building on recent insights from social science and other
Explainable AI (XAI) methods, we propose GraphXAIN, a natural language
narrative that explains individual predictions made by GNNs. We present a
model-agnostic and explainer-agnostic XAI approach that complements graph
explainers by generating GraphXAINs, using Large Language Models (LLMs) and
integrating graph data, individual predictions from GNNs, explanatory
subgraphs, and feature importances. We define XAI Narratives and XAI
Descriptions, highlighting their distinctions and emphasizing the importance of
narrative principles in effective explanations. By incorporating natural
language narratives, our approach supports graph practitioners and non-expert
users, aligning with social science research on explainability and enhancing
user understanding and trust in complex GNN models. We demonstrate GraphXAIN's
capabilities on a real-world graph dataset, illustrating how its generated
narratives can aid understanding compared to traditional graph explainer
outputs or other descriptive explanation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust and Efficient Fine-tuning of <span class="highlight-title">LLM</span>s with Bayesian
  Reparameterization of Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Sengupta, Vaibhav Seth, Arinjay Pathak, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 10 figures, 10 tables, Code:
  https://github.com/LCS2-IIITD/MonteCLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is K-fold cross validation the best model selection method for Machine
  Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan M Gorriz, R. Martin Clemente, F Segovia, J Ramirez, A Ortiz, J. Suckling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a technique that can compactly represent complex patterns, machine
learning has significant potential for predictive inference. K-fold
cross-validation (CV) is the most common approach to ascertaining the
likelihood that a machine learning outcome is generated by chance, and it
frequently outperforms conventional hypothesis testing. This improvement uses
measures directly obtained from machine learning classifications, such as
accuracy, that do not have a parametric description. To approach a frequentist
analysis within machine learning pipelines, a permutation test or simple
statistics from data partitions (i.e., folds) can be added to estimate
confidence intervals. Unfortunately, neither parametric nor non-parametric
tests solve the inherent problems of partitioning small sample-size datasets
and learning from heterogeneous data sources. The fact that machine learning
strongly depends on the learning parameters and the distribution of data across
folds recapitulates familiar difficulties around excess false positives and
replication. A novel statistical test based on K-fold CV and the Upper Bound of
the actual risk (K-fold CUBV) is proposed, where uncertain predictions of
machine learning with CV are bounded by the worst case through the evaluation
of concentration inequalities. Probably Approximately Correct-Bayesian upper
bounds for linear classifiers in combination with K-fold CV are derived and
used to estimate the actual risk. The performance with simulated and
neuroimaging datasets suggests that K-fold CUBV is a robust criterion for
detecting effects and validating accuracy values obtained from machine learning
and classical CV schemes, while avoiding excess false positives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust <span class="highlight-title">Prompt</span> Optimization for Defending <span class="highlight-title">Language Model</span>s Against
  Jailbreaking Attacks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17263v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17263v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zhou, Bo Li, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in AI alignment, large language models (LLMs) remain
vulnerable to adversarial attacks or jailbreaking, in which adversaries can
modify prompts to induce unwanted behavior. While some defenses have been
proposed, they have not been adapted to newly proposed attacks and more
challenging threat models. To address this, we propose an optimization-based
objective for defending LLMs against jailbreaking attacks and an algorithm,
Robust Prompt Optimization (RPO) to create robust system-level defenses. Our
approach directly incorporates the adversary into the defensive objective and
optimizes a lightweight and transferable suffix, enabling RPO to adapt to
worst-case adaptive attacks. Our theoretical and experimental results show
improved robustness to both jailbreaks seen during optimization and unknown
jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2
to 0% on JailbreakBench, setting the state-of-the-art. Code can be found at
https://github.com/lapisrocks/rpo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight; code available at
  https://github.com/lapisrocks/rpo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private <span class="highlight-title">Continual</span> Learning using <span class="highlight-title">Pre-Train</span>ed Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Tobaben, Marcus Klasson, Rui Li, Arno Solin, Antti Honkela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the intersection of continual learning (CL) and
differential privacy (DP). Crucially, continual learning models must retain
knowledge across tasks, but this conflicts with the differential privacy
requirement of restricting individual samples to be memorised in the model. We
propose using pre-trained models to address the trade-offs between privacy and
performance in a continual learning setting. More specifically, we present
necessary assumptions to enable privacy-preservation and propose combining
pre-trained models with parameter-free classifiers and parameter-efficient
adapters that are learned under differential privacy. Our experiments
demonstrate their effectiveness and provide insights into balancing the
competing demands of continual learning and privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, Accepted at Scalable Continual Learning for
  Lifelong Foundation Models Workshop at 38th Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Generation for Controllable <span class="highlight-title">Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjae Lee, Kyungmin Kim, Taesoo Kim, Sangdon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthiness of generative language models (GLMs) is crucial in their
deployment to critical decision making systems. Hence, certified risk control
methods such as selective prediction and conformal prediction have been applied
to mitigating the hallucination problem in various supervised downstream tasks.
However, the lack of appropriate correctness metric hinders applying such
principled methods to language generation tasks. In this paper, we circumvent
this problem by leveraging the concept of textual entailment to evaluate the
correctness of the generated sequence, and propose two selective generation
algorithms which control the false discovery rate with respect to the textual
entailment relation (FDR-E) with a theoretical guarantee:
$\texttt{SGen}^{\texttt{Sup}}$ and $\texttt{SGen}^{\texttt{Semi}}$.
$\texttt{SGen}^{\texttt{Sup}}$, a direct modification of the selective
prediction, is a supervised learning algorithm which exploits
entailment-labeled data, annotated by humans. Since human annotation is costly,
we further propose a semi-supervised version, $\texttt{SGen}^{\texttt{Semi}}$,
which fully utilizes the unlabeled data by pseudo-labeling, leveraging an
entailment set function learned via conformal prediction. Furthermore,
$\texttt{SGen}^{\texttt{Semi}}$ enables to use more general class of selection
functions, neuro-selection functions, and provides users with an optimal
selection function class given multiple candidates. Finally, we demonstrate the
efficacy of the $\texttt{SGen}$ family in achieving a desired FDR-E level with
comparable selection efficiency to those from baselines on both open and closed
source GLMs. Code and datasets are provided at
https://github.com/ml-postech/selective-generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-image <span class="highlight-title">Diffusion</span> Models in <span class="highlight-title">Generative</span> AI: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07909v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07909v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey reviews the progress of diffusion models in generating images
from text, ~\textit{i.e.} text-to-image diffusion models. As a self-contained
work, this survey starts with a brief introduction of how diffusion models work
for image synthesis, followed by the background for text-conditioned image
synthesis. Based on that, we present an organized review of pioneering methods
and their improvements on text-to-image generation. We further summarize
applications beyond image generation, such as text-guided generation for
various modalities like videos, and text-guided image editing. Beyond the
progress made so far, we discuss existing challenges and promising future
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First survey on the recent progress of text-to-image generation based
  on the diffusion model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian RG Flow in Neural Network Field Theories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica N. Howard, Marc S. Klinger, Anindita Maiti, Alexander G. Stapleton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Neural Network Field Theory correspondence (NNFT) is a mapping from
neural network (NN) architectures into the space of statistical field theories
(SFTs). The Bayesian renormalization group (BRG) is an information-theoretic
coarse graining scheme that generalizes the principles of the exact
renormalization group (ERG) to arbitrarily parameterized probability
distributions, including those of NNs. In BRG, coarse graining is performed in
parameter space with respect to an information-theoretic distinguishability
scale set by the Fisher information metric. In this paper, we unify NNFT and
BRG to form a powerful new framework for exploring the space of NNs and SFTs,
which we coin BRG-NNFT. With BRG-NNFT, NN training dynamics can be interpreted
as inducing a flow in the space of SFTs from the information-theoretic `IR'
$\rightarrow$ `UV'. Conversely, applying an information-shell coarse graining
to the trained network's parameters induces a flow in the space of SFTs from
the information-theoretic `UV' $\rightarrow$ `IR'. When the
information-theoretic cutoff scale coincides with a standard momentum scale,
BRG is equivalent to ERG. We demonstrate the BRG-NNFT correspondence on two
analytically tractable examples. First, we construct BRG flows for trained,
infinite-width NNs, of arbitrary depth, with generic activation functions. As a
special case, we then restrict to architectures with a single infinitely-wide
layer, scalar outputs, and generalized cos-net activations. In this case, we
show that BRG coarse-graining corresponds exactly to the momentum-shell ERG
flow of a free scalar SFT. Our analytic results are corroborated by a numerical
experiment in which an ensemble of asymptotically wide NNs are trained and
subsequently renormalized using an information-shell BRG scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 9 figures, 2 tables; updated references and fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04682v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04682v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of these text-to-video (T2V) generative models often produce
single-scene video clips that depict an entity performing a particular action
(e.g., 'a red panda climbing a tree'). However, it is pertinent to generate
multi-scene videos since they are ubiquitous in the real-world (e.g., 'a red
panda climbing a tree' followed by 'the red panda sleeps on the top of the
tree'). To generate multi-scene videos from the pretrained T2V model, we
introduce a simple and effective Time-Aligned Captions (TALC) framework.
Specifically, we enhance the text-conditioning mechanism in the T2V
architecture to recognize the temporal alignment between the video scenes and
scene descriptions. For instance, we condition the visual features of the
earlier and later scenes of the generated video with the representations of the
first scene description (e.g., 'a red panda climbing a tree') and second scene
description (e.g., 'the red panda sleeps on the top of the tree'),
respectively. As a result, we show that the T2V model can generate multi-scene
videos that adhere to the multi-scene text descriptions and be visually
consistent (e.g., entity and background). Further, we finetune the pretrained
T2V model with multi-scene video-text data using the TALC framework. We show
that the TALC-finetuned model outperforms the baseline by achieving a relative
gain of 29% in the overall score, which averages visual consistency and text
adherence using human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super-resolution in disordered media using neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21556v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21556v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Christie, Matan Leibovich, Miguel Moscoso, Alexei Novikov, George Papanicolaou, Chrysoula Tsogka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a methodology that exploits large and diverse data sets to
accurately estimate the ambient medium's Green's functions in strongly
scattering media. Given these estimates, obtained with and without the use of
neural networks, excellent imaging results are achieved, with a resolution that
is better than that of a homogeneous medium. This phenomenon, also known as
super-resolution, occurs because the ambient scattering medium effectively
enhances the physical imaging aperture. This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ordered Momentum for Asynchronous SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed learning is essential for training large-scale deep models.
Asynchronous SGD (ASGD) and its variants are commonly used distributed learning
methods, particularly in scenarios where the computing capabilities of workers
in the cluster are heterogeneous. Momentum has been acknowledged for its
benefits in both optimization and generalization in deep model training.
However, existing works have found that naively incorporating momentum into
ASGD can impede the convergence. In this paper, we propose a novel method
called ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated into
ASGD by organizing the gradients in order based on their iteration indexes. We
theoretically prove the convergence of OrMo with both constant and
delay-adaptive learning rates for non-convex problems. To the best of our
knowledge, this is the first work to establish the convergence analysis of ASGD
with momentum without dependence on the maximum delay. Empirical results
demonstrate that OrMo can achieve better convergence performance compared with
ASGD and other asynchronous methods with momentum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JKO for Landau: a variational particle method for homogeneous Landau
  equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Huang, Li Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the gradient flow viewpoint of the Landau equation and
corresponding dynamic formulation of the Landau metric in [arXiv:2007.08591],
we develop a novel implicit particle method for the Landau equation in the
framework of the JKO scheme. We first reformulate the Landau metric in a
computationally friendly form, and then translate it into the Lagrangian
viewpoint using the flow map. A key observation is that, while the flow map
evolves according to a rather complicated integral equation, the unknown
component is merely a score function of the corresponding density plus an
additional term in the null space of the collision kernel. This insight guides
us in designing and training the neural network for the flow map. Additionally,
the objective function is in a double summation form, making it highly suitable
for stochastic methods. Consequently, we design a tailored version of
stochastic gradient descent that maintains particle interactions and
significantly reduces the computational complexity. Compared to other
deterministic particle methods, the proposed method enjoys exact entropy
dissipation and unconditional stability, therefore making it suitable for
large-scale plasma simulations over extended time periods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Partial Differential Equations with Deep Parallel Neural
  Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinglong Ma, Peizhi Zhao, Sen Wang, Tao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Solving partial differential equations has shifted the focus
of traditional neural network studies from finite-dimensional Euclidean spaces
to generalized functional spaces in research. A novel methodology is to learn
an operator as a means of approximating the mapping between outputs. Currently,
researchers have proposed a variety of operator architectures. Nevertheless,
the majority of these architectures adopt an iterative update architecture,
whereby a single operator is learned from the same function space. In practical
physical science problems, the numerical solutions of partial differential
equations are complex, and a serial single operator is unable to accurately
approximate the intricate mapping between input and output. So, We propose a
deep parallel operator model (DPNO) for efficiently and accurately solving
partial differential equations. DPNO employs convolutional neural networks to
extract local features and map data into distinct latent spaces. Designing a
parallel block of double Fourier neural operators to solve the iterative error
problem. DPNO approximates complex mappings between inputs and outputs by
learning multiple operators in different potential spaces in parallel blocks.
DPNO achieved the best performance on five of them, with an average improvement
of 10.5\%, and ranked second on one dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency of Neural Causal Partial Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Tan, Jose Blanchet, Vasilis Syrgkanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Neural Causal Models (NCMs) showcased how identification
and partial identification of causal effects can be automatically carried out
via training of neural generative models that respect the constraints encoded
in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However,
formal consistency of these methods has only been proven for the case of
discrete variables or only for linear causal models. In this work, we prove the
consistency of partial identification via NCMs in a general setting with both
continuous and categorical variables. Further, our results highlight the impact
of the design of the underlying neural network architecture in terms of depth
and connectivity as well as the importance of applying Lipschitz regularization
in the training phase. In particular, we provide a counterexample showing that
without Lipschitz regularization this method may not be asymptotically
consistent. Our results are enabled by new results on the approximability of
Structural Causal Models (SCMs) via neural generative models, together with an
analysis of the sample complexity of the resulting architectures and how that
translates into an error in the constrained optimization problem that defines
the partial identification bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 8 figures, accepted by Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty
  and Smoothness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Wen, Xudong Yu, Rui Yang, Haoyuan Chen, Chenjia Bai, Zhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To obtain a near-optimal policy with fewer interactions in Reinforcement
Learning (RL), a promising approach involves the combination of offline RL,
which enhances sample efficiency by leveraging offline datasets, and online RL,
which explores informative transitions by interacting with the environment.
Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained
agent within limited online interactions. However, due to the significant
distribution shift between online experiences and offline data, most offline RL
algorithms suffer from performance drops and fail to achieve stable policy
improvement in O2O adaptation. To address this problem, we propose the Robust
Offline-to-Online (RO2O) algorithm, designed to enhance offline policies
through uncertainty and smoothness, and to mitigate the performance drop in
online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty
penalty and adversarial samples for policy and value smoothness, which enable
RO2O to maintain a consistent learning procedure in online adaptation without
requiring special changes to the learning objective. Theoretical analyses in
linear MDPs demonstrate that the uncertainty and smoothness lead to a tighter
optimality bound in O2O against distribution shift. Experimental results
illustrate the superiority of RO2O in facilitating stable offline-to-online
learning and achieving significant improvement with limited online
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by Journal of Artificial Intelligence
  Research (JAIR). arXiv admin note: text overlap with arXiv:2306.06871 by
  other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially-Private Multi-Tier Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11592v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11592v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Chen, Frank Po-Chen Lin, Dong-Jun Han, Christopher G. Brinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While federated learning (FL) eliminates the transmission of raw data over a
network, it is still vulnerable to privacy breaches from the communicated model
parameters. In this work, we propose Multi-Tier Federated Learning with
Multi-Tier Differential Privacy (M^2FDP), a DP-enhanced FL methodology for
jointly optimizing privacy and performance in hierarchical networks. One of the
key concepts of M^2FDP is to extend the concept of HDP towards Multi-Tier
Differential Privacy (MDP), while also adapting DP noise injection at different
layers of an established FL hierarchy -- edge devices, edge servers, and cloud
servers -- according to the trust models within particular subnetworks. We
conduct a comprehensive analysis of the convergence behavior of M^2FDP,
revealing conditions on parameter tuning under which the training process
converges sublinearly to a finite stationarity gap that depends on the network
hierarchy, trust model, and target privacy level.
  Subsequent numerical evaluations demonstrate that M^2FDP obtains substantial
improvements in these metrics over baselines for different privacy budgets, and
validate the impact of different system configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Top-down Information for Self-modulating Slot Attention <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Kim, Seoyeon Kim, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric learning (OCL) aims to learn representations of individual
objects within visual scenes without manual supervision, facilitating efficient
and effective visual reasoning. Traditional OCL methods primarily employ
bottom-up approaches that aggregate homogeneous visual features to represent
objects. However, in complex visual environments, these methods often fall
short due to the heterogeneous nature of visual features within an object. To
address this, we propose a novel OCL framework incorporating a top-down
pathway. This pathway first bootstraps the semantics of individual objects and
then modulates the model to prioritize features relevant to these semantics. By
dynamically modulating the model based on its own output, our top-down pathway
enhances the representational quality of objects. Our framework achieves
state-of-the-art performance across multiple synthetic and real-world
object-discovery benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Latent State Learning: Exploiting Regional Network Similarities for
  QoS Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Wang, Xiaohong Zhang, Kechi Zhang, Ze Shi Li, Meng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual objects, whether users or services, within a specific region often
exhibit similar network states due to their shared origin from the same city or
autonomous system (AS). Despite this regional network similarity, many existing
techniques overlook its potential, resulting in subpar performance arising from
challenges such as data sparsity and label imbalance. In this paper, we
introduce the regional-based dual latent state learning network(R2SL), a novel
deep learning framework designed to overcome the pitfalls of traditional
individual object-based prediction techniques in Quality of Service (QoS)
prediction. Unlike its predecessors, R2SL captures the nuances of regional
network behavior by deriving two distinct regional network latent states: the
city-network latent state and the AS-network latent state. These states are
constructed utilizing aggregated data from common regions rather than
individual object data. Furthermore, R2SL adopts an enhanced Huber loss
function that adjusts its linear loss component, providing a remedy for
prevalent label imbalance issues. To cap off the prediction process, a
multi-scale perception network is leveraged to interpret the integrated feature
map, a fusion of regional network latent features and other pertinent
information, ultimately accomplishing the QoS prediction. Through rigorous
testing on real-world QoS datasets, R2SL demonstrates superior performance
compared to prevailing state-of-the-art methods. Our R2SL approach ushers in an
innovative avenue for precise QoS predictions by fully harnessing the regional
network similarities inherent in objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Meta-Learning Meets Online and <span class="highlight-title">Continual</span> Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyeon Son, Soochan Lee, Gunhee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, deep neural networks have demonstrated significant
success using the training scheme that involves mini-batch stochastic gradient
descent on extensive datasets. Expanding upon this accomplishment, there has
been a surge in research exploring the application of neural networks in other
learning scenarios. One notable framework that has garnered significant
attention is meta-learning. Often described as "learning to learn,"
meta-learning is a data-driven approach to optimize the learning algorithm.
Other branches of interest are continual learning and online learning, both of
which involve incrementally updating a model with streaming data. While these
frameworks were initially developed independently, recent works have started
investigating their combinations, proposing novel problem settings and learning
algorithms. However, due to the elevated complexity and lack of unified
terminology, discerning differences between the learning frameworks can be
challenging even for experienced researchers. To facilitate a clear
understanding, this paper provides a comprehensive survey that organizes
various problem settings using consistent terminology and formal descriptions.
By offering an overview of these learning paradigms, our work aims to foster
further advancements in this promising area of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Noise Resilient for QoS Prediction with Probabilistic Deep
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02580v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02580v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Sheng Huang, Meng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate Quality of Service (QoS) prediction is essential for enhancing user
satisfaction in web recommendation systems, yet existing prediction models
often overlook feature noise, focusing predominantly on label noise. In this
paper, we present the Probabilistic Deep Supervision Network (PDS-Net), a
robust framework designed to effectively identify and mitigate feature noise,
thereby improving QoS prediction accuracy. PDS-Net operates with a dual-branch
architecture: the main branch utilizes a decoder network to learn a
Gaussian-based prior distribution from known features, while the second branch
derives a posterior distribution based on true labels. A key innovation of
PDS-Net is its condition-based noise recognition loss function, which enables
precise identification of noisy features in objects (users or services). Once
noisy features are identified, PDS-Net refines the feature's prior
distribution, aligning it with the posterior distribution, and propagates this
adjusted distribution to intermediate layers, effectively reducing noise
interference. Extensive experiments conducted on two real-world QoS datasets
demonstrate that PDS-Net consistently outperforms existing models, achieving an
average improvement of 8.91% in MAE on Dataset D1 and 8.32% on Dataset D2
compared to the ate-of-the-art. These results highlight PDS-Net's ability to
accurately capture complex user-service relationships and handle feature noise,
underscoring its robustness and versatility across diverse QoS prediction
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Tuning For Toxicity Mitigation Generalizes Across Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detoxifying multilingual Large Language Models (LLMs) has become crucial due
to their increasing global use. In this work, we explore zero-shot
cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike
previous studies that show limited cross-lingual generalization for other
safety tasks, we demonstrate that Direct Preference Optimization (DPO) training
with only English data can significantly reduce toxicity in multilingual
open-ended generations. For example, the probability of mGPT-1.3B generating
toxic continuations drops from 46.8% to 3.9% across 17 different languages
after training. Our results also extend to other multilingual LLMs, such as
BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal
intervention and activation analysis, we identified the dual multilinguality
property of MLP layers in LLMs, which explains the cross-lingual generalization
of DPO. Finally, we show that bilingual sentence retrieval can predict the
cross-lingual transferability of DPO preference tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-11-16T05:28:41.962092848Z">
            2024-11-16 05:28:41 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
